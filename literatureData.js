const literatureData = [
{ 
  "id": 1, 
  "year": 2025, 
  "authors": "Lima, Maria R.; O'Connell, Amy; Zhou, Feiyang; Nagahara, Alethea; Hulyalkar, Avni; Deshpande, Anura; Thomason, Jesse; Vaidyanathan, Ravi; Matarić, Maja", 
  "title": "Promoting Cognitive Health in Elder Care with Large Language Model-Powered Socially Assistive Robots", 
  "venue": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems", 
  "doi": "10.1145/3706598.3713582", 
  "url": "https://dl.acm.org/doi/10.1145/3706598.3713582", 
  "abstract": "As the global population ages, there is increasing need for accessible technologies that promote cognitive health and detect early signs of cognitive decline. This research demonstrates the potential for in-residence monitoring and assessment of cognitive health using large language model (LLM)-powered socially assistive robots (SARs). We conducted a 5-week within-subjects study involving 22 older adults in retirement homes to investigate the feasibility of LLM-powered SARs for promoting and assessing cognitive health. We designed tasks that involved verbal dialogue based on clinically validated cognitive tools. Our findings reveal improved task performance after three robot-administered sessions, with significantly more detailed picture descriptions, fewer word repetitions in semantic fluency, and reduced need for hints. We found that older adults were more socially engaged in robot-administered tasks compared to those administered by a human, and they accepted and were willing to engage with SARs in this context, which had not been tested before.", 
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Emotional Grounding", 
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Anticipatory Assistance", 
  "alignment": "Repair in Ethical and Normative Alignment", 
  "modality": "Text,Voice,Visuals,Motion,Hybrid", 
  "morphology": "Functional", 
  "autonomy": "Semi-Autonomy", 
  "studyMethod": "Interviews,Questionnaires", 
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Usability", 
  "application": "Healthcare and Wellbeing" 
},

{
  "id": 2,
  "year": 2025,
  "authors": "Zhang, Alex Wuqi; Kovacs, Clark; de Pablo, Liberto; Zhang, Justin; Bai, Maggie; Jeong, Sooyeon; Sebo, Sarah",
  "title": "Exploring Robot Personality Traits and Their Influence on User Affect and Experience",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721606",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721606",
  "abstract": "As human-robot interactions become more social, a robot's personality plays an increasingly vital role in shaping user experience and its overall effectiveness. In this study, we examine the impact of three distinct robot personalities on user experiences during well-being exercises: a Baseline Personality that aligns with user expectations, a High Extraversion Personality, and a High Neuroticism Personality. These personalities were manifested through the robot's dialogue, which were generated using a large language model (LLM) guided by key behavioral characteristics from the Big 5 personality traits. In a between-subjects user study (N = 66), where each participant interacted with one distinct robot personality, we found that both the High Extraversion and High Neuroticism Robot Personalities significantly enhanced participants' emotional states (arousal, control, and valence). The High Extraversion Robot Personality was also rated as the most enjoyable to interact with. Additionally, evidence suggested that participants' personality traits moderated the effectiveness of specific robot personalities in eliciting positive outcomes from well-being exercises. Our findings highlight the potential benefits of designing robot personalities that deviate from users' expectations, thereby enriching human-robot interactions.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Voice,Visuals",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Questionnaires",
  "evaluationMetrics": "User's Perceptual and Relational Experience",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 3,
  "year": 2025,
  "authors": "Spitale, Micol; Axelsson, Minja; Gunes, Hatice",
  "title": "VITA: A Multi-Modal LLM-Based System for Longitudinal, Autonomous and Adaptive Robotic Mental Well-Being Coaching",
  "venue": "ACM Transactions on Human-Robot Interaction",
  "doi": "10.1145/3712265",
  "url": "https://dl.acm.org/doi/10.1145/3712265",
  "abstract": "Recently, several works have explored if and how robotic coaches can promote and maintain mental well-being in different settings. However, findings from these studies revealed that these robotic coaches are not ready to be used and deployed in real-world settings due to several limitations that span from technological challenges to coaching success. To overcome these challenges, this article presents VITA, a novel multi-modal LLM-based system that allows robotic coaches to autonomously adapt to the coachee's multi-modal behaviours (facial valence and speech duration) and deliver coaching exercises in order to promote mental well-being in adults. We identified five objectives that correspond to the challenges in the recent literature, and we show how the VITA system addresses these via experimental validations that include one in-lab pilot study (N = 4) that enabled us to test different robotic coach configurations (pre-scripted, generic and adaptive models) and inform its design for using it in the real world, and one real-world study (N = 17) conducted in a workplace over 4 weeks. Our results show that: (i) coachees perceived the VITA adaptive and generic configurations more positively than the pre-scripted one, and they felt understood and heard by the adaptive robotic coach, (ii) the VITA adaptive robotic coach kept learning successfully by personalising to each coachee over time and did not detect any interaction ruptures during the coaching and (iii) coachees had significant mental well-being improvements via the VITA-based robotic coach practice.",
  "sense": "Modular Perception and Textual Abstraction,Emotional Grounding",
  "interaction": "",
  "alignment": "Sustained Personalization, Repair in Ethical and Normative Alignment",
  "modality": "Voice,Visuals",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Field Deployments,Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Usability,Safety,Cognitive Load and Workload",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 4,
  "year": 2025,
  "authors": "Hsu, Long-Jing; Swaminathan, Manasi; Khoo, Weslie; Amon, Kyrie Jig; Sato, Hiroki; Dobbala, Sathvika; Tsui, Kate; Crandall, David; Sabanovic, Selma",
  "title": "Bittersweet Snapshots of Life: Designing to Address Complex Emotions in a Reminiscence Interaction between Older Adults and a Robot",
  "venue": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems",
  "doi": "10.1145/3706598.3714256",
  "url": "https://dl.acm.org/doi/10.1145/3706598.3714256",
  "abstract": "Human-Computer Interaction and Human-Robot Interaction researchers have developed various reminiscence technologies for older adults, but the focus of such work has mostly been on making the technology usable and improving older adults' memory recall. Our study of a robot facilitating reminiscence through conversations about personal photographs with 20 older adults uncovered a less discussed aspect of such interactions: reminiscence can evoke both bitter and sweet emotions. Without adequate emotional sensitivity, the robot sometimes responded inappropriately, requiring researchers to intervene in the interaction to address misunderstandings. To understand how to better address these challenges, we conducted a follow-up co-design workshop with 7 older adults to explore how the robot could better support managing bittersweet emotions. Through reflexive thematic analysis of the two studies, this paper identifies factors that trigger bittersweet emotions during reminiscence with a robot and provides strategies for technology to manage these emotions during such interactions. This research highlights the importance of addressing emotional experiences in the design of reminiscence technology. It also raises ethical concerns about the emotional vulnerability of deploying one-on-one AI technologies for older adults.",
  "sense": "Emotional Grounding,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Creative Storytelling and Social Engagement,Social Initiation",
  "alignment": "Emotional Repair in Social Interaction",
  "modality": "Voice,Motion",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Interviews,Co-design workshops",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Safety",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 5,
  "year": 2025,
  "authors": "Tao, Yiran; Yang, Jehan; Ding, Dan; Erickson, Zackory",
  "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721521",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721521",
  "abstract": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learning-based methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time. The project website with supplementary materials is at https://lams-assistance.github.io/.",
  "sense": "Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Sustained Personalization,Episodic Memory Integration",
  "modality": "Visuals,Motion,Tangible and Haptic Interaction",
  "morphology": "Functional",
  "autonomy": "Teleoperation",
  "studyMethod": "Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Usability,Cognitive Load and Workload",
  "application": "Other"
},
{
  "id": 6,
  "year": 2025,
  "authors": "Banna, Tahsin Tariq; Rahman, Sejuti; Tareq, Mohammad",
  "title": "Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions",
  "venue": "Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems",
  "doi": "10.5555/3709347.3743537",
  "url": "https://dl.acm.org/doi/10.5555/3709347.3743537",
  "abstract": "As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots' verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots' verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg's personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht's (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p<.001). Post-hoc analyses using Šidák's multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: Δ Δmean 4.42, p = 0.02; Condition 3 vs. 1: Δ Δmean 8.23, p < 0.01; Condition 3 vs. 2: Δ Δmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service.",
  "sense": "Emotional Grounding,Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Creative Storytelling and Social Engagement,Social Initiation",
  "alignment": "Sustained Personalization,Emotional Repair in Social Interaction",
  "modality": "Voice",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Anthropomorphism",
  "application": "Other"
},
{
  "id": 7,
  "year": 2025,
  "authors": "Lo, Jia-Hsun; Huang, Han-Pang; Chen, Yen-Ching; Chen, Jen-Hau",
  "title": "Memory Robot Design: A New Perspective from Human Brain Model and Large Language Model",
  "venue": "IEEE Access: Practical Innovations, Open Solutions",
  "doi": "10.1109/ACCESS.2025.3538889",
  "url": "https://ieeexplore.ieee.org/document/10872903",
  "abstract": "With the spread of generative AI, the study proposed a memory-based cognitive robot architecture by using a Large Language Model (LLM), inspired by the working memory of the human brain model. A card-pairing task is designed to perform visual working memory with 60 participants with the measurement of electroencephalography (EEG). The proposed human brain model is a muti-featured EEG model, represented by the concept of brain energy, connectivity, and complexity. Band power ratios show that the anti-correlation of alpha and gamma waveforms can be observed in the occipital lobe. Brain connectivity is represented through magnitude squared coherence and phase locking value, and brain complexity is calculated by the Katz fractal dimension. Three pathways—attention, short-term memory, and distraction resistance are revealed. The changes in Katz fractal dimension are discovered in the frontal and occipital lobes. A novel memory architecture of robot cognition designed from human brain model includes three types of memory—short-term memory, working memory, and long-term memory, which is driven by GPT-4o. Two memory tests, a recalling test, and a working memory test, are conducted to validate the memory ability of the robot. The precision / recall / F1-score of working memory performance is 1.0 / 0.35 / 0.519, and the average accuracy of recalling test is 0.7. It is demonstrated that the different types of human memory functions can be implemented in the cognitive robot architecture driven by LLM. The research not only provides insight into the working memory of the human brain model but also realizes the robot architecture with the application of generative AI.",
  "sense": "Integrated Visual-Language Reasoning,Task Intent Formulation",
  "interaction": "",
  "alignment": "Sustained Personalization,Episodic Memory Integration",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,Safety",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 8,
  "year": 2024,
  "authors": "Wester, Joel; Moghe, Bhakti; Winkle, Katie; van Berkel, Niels",
  "title": "Facing LLMs: Robot Communication Styles in Mediating Health Information between Parents and Young Adults",
  "venue": "Proceedings of the ACM on Human-Computer Interaction",
  "doi": "10.1145/3687036",
  "url": "https://dl.acm.org/doi/10.1145/3687036",
  "abstract": "Young adults may feel embarrassed when disclosing sensitive information to their parents, while parents might similarly avoid sharing sensitive aspects of their lives with their children. How to design interactive interventions that are sensitive to the needs of both younger and older family members in mediating sensitive information remains an open question. In this paper, we explore the integration of large language models (LLMs) with social robots. Specifically, we use GPT-4 to adapt different Robot Communication Styles (RCS) for a social robot mediator designed to elicit self-disclosure and mediate health information between parents and young adults living apart. We design and compare four literature-informed RCS: three LLM-adapted (Humorous, Self-deprecating, and Persuasive) and one manually created (Human-scripted), and assess participant perceptions of Likeability, Usefulness, Helpfulness, Relatedness, and Interpersonal Closeness. Through an online experiment with 183 participants, we assess the RCS across two groups: adults with children (Parents) and young adults without children (Young Adults). Our results indicate that both Parents and Young Adults favoured the Human-scripted and Self-deprecating RCS as compared to the other two RCS. The Self-deprecating RCS furthermore led to increased relatedness as compared to the Humorous RCS. Our qualitative findings reveal challenges people have in disclosing health information to family members, and who normally assumes the role of family facilitator—two areas in which social robots can play a key role. The findings offer insights for integrating LLMs with social robots in health-mediation and other contexts involving the sharing of sensitive information.",
  "sense": "Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Voice",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Interviews,Questionnaires",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 9,
  "year": 2025,
  "authors": "Pineda, Kaitlynn Taylor; Brown, Ethan; Huang, Chien-Ming",
  "title": "\"See You Later, Alligator\": Impacts of Robot Small Talk on Task, Rapport, and Interaction Dynamics in Human-Robot Collaboration",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721589",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721589",
  "abstract": "Small talk can foster rapport building in human-human teamwork; yet how non-anthropomorphic robots, such as collaborative manipulators commonly used in industry, may capitalize on these social communications remains unclear. This work investigates how robot-initiated small talk influences task performance, rapport, and interaction dynamics in human-robot collaboration. We developed an autonomous robot system that assists a human in an assembly task while initiating and engaging in small talk. A user study (N = 58) was conducted in which participants worked with either a functional robot, which engaged in only task-oriented speech, or a social robot, which also initiated small talk. Our study found that participants in the social condition reported significantly higher levels of rapport with the robot. Moreover, all participants in the social condition responded to the robot's small talk attempts; 59% initiated questions to the robot, and 73% engaged in lingering conversations after requesting the final task item. Although active working times were similar across conditions, participants in the social condition recorded longer task durations than those in the functional condition. We discuss the design and implications of robot small talk in shaping human-robot collaboration.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation",
  "alignment": "Episodic Memory Integration",
  "modality": "Voice,Motion,Hybrid,Proximity",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience",
  "application": "Industrial Manufacturing"
},
{
  "id": 10,
  "year": 2025,
  "authors": "Reimann, Merle M.; Hindriks, Koen V.; Kunneman, Florian A.; Oertel, Catharine; Skantze, Gabriel; Leite, Iolanda",
  "title": "What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721576",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721576",
  "abstract": "When encountering a robot in the wild, it is not inherently clear to human users what the robot's capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech-based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user's rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who had three speech-based interactions with a social robot in a restaurant setting. Our results suggest that users preferred the robot communicating its capabilities proactively and adjusted their behavior in those interactions, using a more conversational interaction style while also enjoying the interaction more.",
  "sense": "Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Social Initiation",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Voice,Motion",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Usability",
  "application": "Public Spaces Service"
},
{
  "id": 11,
  "year": 2025,
  "authors": "Kamelabad, Alireza M.; Inoue, Elin; Skantze, Gabriel",
  "title": "Comparing Monolingual and Bilingual Social Robots as Conversational Practice Companions in Language Learning",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721590",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721590",
  "abstract": "This study explores the impact of monolingual and bilingual robots in Robot-Assisted Language Learning (RALL) for non-native Swedish learners. In a within-group design, 47 participants interacted with a social robot under two conditions: a monolingual robot that communicated exclusively in Swedish and a bilingual robot capable of switching between Swedish and English. Each participant engaged in multiple role-play scenarios designed to match their language proficiency levels, and their experiences were assessed through surveys and behavioral data. The results show that the bilingual robot was generally favored by participants, leading to a more relaxed, enjoyable experience. The perceived learning was improved at the end of the experiment regardless of the condition. These findings suggest that incorporating bilingual support in language-learning robots may enhance user engagement and effectiveness, particularly for lower-proficiency learners.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Task Intent Formulation,Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity,Anticipatory Assistance",
  "alignment": "Episodic Memory Integration",
  "modality": "Voice,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Safety",
  "application": "Teaching and Education"
},
{
  "id": 12,
  "year": 2025,
  "authors": "Ali, Safinah; Abodayeh, Ayat; Dhuliawala, Zahra; Breazeal, Cynthia; Park, Hae Won",
  "title": "Towards Inclusive Co-creative Child-robot Interaction: Can Social Robots Support Neurodivergent Children's Creativity?",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721531",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721531",
  "abstract": "This research designs and applies inclusive child-robot interactions for collaborative creativity, where elementary school children and a social robot collaboratively create and publish picture stories. The robot offers creativity scaffolding during parts of the creative process of storytelling through social interactions such as feedback, question asking, divergent thinking, and positive reinforcement. The collaborative tasks and robot interactions are personalized for neurodivergent children's unique needs. Through a five-session user study with 32 children (ages 5-9) over 8 months, the study investigates the impact of the social robot on children's creativity in storytelling over time, their creative interactions with the robot, and their perceptions of the robot as a creative collaborator. Results show that inclusive design practices eliminated creative barriers for children with neurodevelopmental disorders, enhanced verbal creativity, influenced creative processes, and led to the emergence of diverse creator styles. The authors propose Inclusive Co-creative Child-robot Interaction (ICCRI) guidelines for fostering creativity and accommodating diverse creator styles in open-ended creative tasks.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Task Intent Formulation",
  "interaction": "Embodied Social Expressiveness,Creative Storytelling and Social Engagement",
  "alignment": "Emotional Repair in Social Interaction",
  "modality": "Voice,Motion,Hybrid",
  "morphology": "Desktop Companions",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Anthropomorphism",
  "application": "Teaching and Education"
},
{
  "id": 13,
  "year": 2025,
  "authors": "Zhang, Renchi; van der Linden, Jesse; Dodou, Dimitra; Seyffert, Harleigh; Eisma, Yke Bauke; de Winter, Joost",
  "title": "Walk along: An experiment on controlling the mobile robot “spot” with voice and gestures",
  "venue": "ACM Transactions on Human-Robot Interaction",
  "doi": "10.1145/3729540",
  "url": "https://doi.org/10.1145/3729540",
  "abstract": "Robots are becoming more capable and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two touchless methods for directing mobile robots: voice control and gesture control, to investigate the efficiency of these methods and the preference of users. We tested these methods in two conditions: one in which participants remained stationary and one in which they walked freely alongside the robot. We hypothesized that walking alongside the robot would result in higher intuitiveness ratings and improved task performance, based on the idea that walking promotes spatial alignment and reduces the effort required for mental rotation. In a 2×2 within-subject design, 218 participants guided the quadruped robot Spot along a circuitous route with multiple 90° turns using rotate left, rotate right, and walk forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants' preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, whereas gesture control while standing caused confusion for left/right commands. Nevertheless, 29% of participants preferred gesture control, citing increased task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants often followed behind Spot, particularly in the gesture control condition, when they were allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could make gesture control more effective.",
  "sense": "Task Intent Formulation",
  "interaction": "",
  "alignment": "",
  "modality": "Voice,Visuals,Hybrid",
  "morphology": "Zoomorphic",
  "autonomy": "Teleoperation",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,Usability,Cognitive Load and Workload",
  "application": "AR/VR-enabled Interactions"
},
{
  "id": 14,
  "year": 2025,
  "authors": "Ito, Shunichiro; Kochigami, Kanae; Kanda, Takayuki",
  "title": "A Robot Dynamically Asking Questions in University Classes",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721591",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721591",
  "abstract": "We developed a robot that dynamically asks questions after listening to a lecture in university group classes to enhance students' learning. Based on interview results with ten professors, we designed our robot to let professors choose when a robot asks questions, ask various kinds of questions by letting professors choose question types, and ask questions dynamically without sharing them with professors in advance. The prepared question types include such as clarification questions, which are fact-based questions about a lecture, and thought-provoking questions, which activate students' critical thinking. We expanded them into eight sub-categories of questions that the most common types that are likely to emerge in classrooms. Leveraging large language models (LLMs), based on lecture transcripts, we developed a robot system that generates questions in the eight sub-categories and yields 0.93 classification accuracy toward human coding results. We also conducted a case study with our developed robot in four university lectures. In these lectures that involved our robot, the professors often asked reasoning questions and criticism questions, and the students showed engagement with the robot-professor dialogues. Interviews with 20 students revealed that the robot's questions contributed to the classes by helping the students reflect on the lectures and gain new perspectives. The professors also recognized some benefits of the robot, perceiving its presence as a question facilitator, a mood maker, a communication tool as well as a motivator to prepare more diligently for their own lectures.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Task-Oriented Planning and Execution,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Interviews,Technical Evaluation,Case Studies",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Perceived Intelligence",
  "application": "Teaching and Education"
},
{
  "id": 15,
  "year": 2024,
  "authors": "Karli, Ulas Berk; Chen, Juo-Tung; Antony, Victor Nikhil; Huang, Chien-Ming",
  "title": "Alchemist: LLM-Aided End-User Development of Robot Applications",
  "venue": "Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3610977.3634969",
  "url": "https://dl.acm.org/doi/10.1145/3610977.3634969",
  "abstract": "Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming—moving from the conventional process of specifying programming logic to an iterative, collaborative process where users specify desired program outcomes while LLMs produce detailed specifications. We introduce Alchemist, a novel integrated development system leveraging LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, reducing the required expertise. We provide a detailed examination of the system design and an exploratory study with true end-users to assess capabilities, usability, and limitations. Lessons learned highlight the potential of LLMs to democratize end-user development of robot applications.",
  "sense": "Static and Semi-Static Context Injection,Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity,Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,Usability",
  "application": "Industrial Manufacturing"
},
{
  "id": 16,
  "year": 2025,
  "authors": "Wang, Chenyang; Tozadore, Daniel; Bruno, Barbara; Dillenbourg, Pierre",
  "title": "The Child-Robot Relational Norm Intervention to Promote Correct Handwriting Posture for Children",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721534",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721534",
  "abstract": "Persuasive social robots have the ability to influence human behaviour through social interaction, which makes them a valuable technological solution for all applications aiming to support a person's behaviour change. The recently proposed Child-Robot Relational Norm Intervention (CRNI) model introduces a new approach for persuasive social robotics, leveraging children's reluctance to disturb robots to promote behaviour change. Unlike traditional methods that rely on direct feedback or reminders, CRNI encourages children to self-monitor and self-correct improper behaviour, by making the robot express mild distress whenever the child exhibits the incorrect behaviour. This paper proposes the first implementation of the CRNI approach in a real HRI context and evaluates its effectiveness in improving children's handwriting posture. The evaluation includes two user studies: (i) a multi-session study with five children investigating the long-term impact of the approach, (ii) a controlled study with 29 children comparing CRNI to direct robot reminders. The results indicate that the CRNI model leads to more sustained posture correction compared to direct interventions. More broadly, our findings suggest that relational norm-based approaches can offer an effective yet less intrusive method for fostering positive behaviours in children.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness",
  "alignment": "",
  "modality": "Text,Voice,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments,Questionnaires,Co-design workshops",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Usability,Cognitive Load and Workload",
  "application": "Teaching and Education"
},
{
  "id": 17,
  "year": 2025,
  "authors": "Ikeda, Bryce; Gramopadhye, Maitrey; Nekervis, LillyAnn; Szafir, Daniel",
  "title": "MARCER: Multimodal Augmented Reality for Composing and Executing Robot Tasks",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721555",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721555",
  "abstract": "In this work, we combine the strengths of humans and robots by developing MARCER, a novel interactive and multimodal end-user robot programming system. MARCER utilizes a Large Language Model to translate users' natural language task descriptions and environmental context into Action Plans for robot execution, based on a trigger-action programming paradigm that facilitates authoring reactive robot behaviors. MARCER also affords interaction via augmented reality to help users parameterize and validate robot programs and provide real-time, visual previews and feedback directly in the context of the robot's operating environment. We present the design, implementation, and evaluation of MARCER to explore the usability of such systems and demonstrate how trigger-action programming, Large Language Models, and augmented reality hold deep-seated synergies that, when combined, empower users to program general-purpose robots to perform everyday tasks.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Usability",
  "application": "Domestic and Everyday Use"
},
{
  "id": 18,
  "year": 2025,
  "authors": "Xu, Michael F.; Mutlu, Bilge",
  "title": "Exploring the Use of Robots for Diary Studies",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721513",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721513",
  "abstract": "As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.",
  "sense": "Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness",
  "alignment": "Emotional Repair in Social Interaction",
  "modality": "Voice,Visuals,Motion",
  "morphology": "Desktop companions",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Interviews,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Usability",
  "application": "Domestic and Everyday Use"
},
{
  "id": 19,
  "year": 2025,
  "authors": "Leusmann, Jan; Belardinelli, Anna; Haliburton, Luke; Hasler, Stephan; Schmidt, Albrecht; Mayer, Sven; Gienger, Michael; Wang, Chao",
  "title": "Investigating LLM-Driven Curiosity in Human-Robot Interaction",
  "venue": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems",
  "doi": "10.1145/3706598.3713923",
  "url": "https://dl.acm.org/doi/10.1145/3706598.3713923",
  "abstract": "Integrating curious behavior traits into robots is essential for them to learn and adapt to new tasks over their lifetime and to enhance human-robot interaction. However, the effects of robots expressing curiosity on user perception, user interaction, and user experience in collaborative tasks are unclear. In this work, we present a Multimodal Large Language Model-based system that equips a robot with non-verbal and verbal curiosity traits. We conducted a user study (N = 20) to investigate how these traits modulate the robot's behavior and the users' impressions of sociability and quality of interaction. Participants prepared cocktails or pizzas with a robot, which was either curious or non-curious. Our results show that we could create user-centric curiosity, which users perceived as more human-like, inquisitive, and autonomous while resulting in a longer interaction time. We contribute a set of design recommendations allowing system designers to take advantage of curiosity in collaborative tasks.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Task-Oriented Planning and Execution,Social Initiation,Anticipatory Assistance",
  "alignment": "Sustained Personalization,Behavioral Repair in Task Execution",
  "modality": "Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence,Usability,Cognitive Load and Workload",
  "application": "Domestic and Everyday Use"
},
{
  "id": 20,
  "year": 2025,
  "authors": "Zhang, Alex Wuqi; Queiroz, Rafael; Sebo, Sarah",
  "title": "Balancing User Control and Perceived Robot Social Agency through the Design of End-User Robot Programming Interfaces",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721598",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721598",
  "abstract": "Perceived social agency-the perception of a robot as an autonomous and intelligent social other-is important for fostering meaningful and engaging human-robot interactions. While end-user programming (EUP) enables users to customize robot behavior, enhancing usability and acceptance, it can also potentially undermine the robot's perceived social agency. This study explores the trade-offs between user control over robot behavior and preserving the robot's perceived social agency, and how these factors jointly impact user experience. We conducted a between-subjects study (N = 57) where participants customized the robot's behavior using either a High-Granularity Interface with detailed block-based programming, a Low-Granularity Interface with broader input-form customizations, or no EUP at all. Results show that while both EUP interfaces improved alignment with user preferences, the Low-Granularity Interface better preserved the robot's perceived social agency and led to a more engaging interaction. These findings highlight the need to balance user control with perceived social agency, suggesting that moderate customization without excessive granularity may enhance the overall satisfaction and acceptance of robot products.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Social Initiation",
  "alignment": "",
  "modality": "Text,Voice,Tangible and Haptic Interaction",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Perceived Intelligence,Usability",
  "application": "Domestic and Everyday Use"
},
{
  "id": 21,
  "year": 2025,
  "authors": "Antony, Victor Nikhil; Stiber, Maia; Huang, Chien-Ming",
  "title": "Xpress: A System For Dynamic, Context-Aware Robot Facial Expressions using Language Models",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721605",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721605",
  "abstract": "Facial expressions are vital in human communication and significantly influence outcomes in human-robot interaction (HRI), such as likeability, trust, and companionship. However, current methods for generating robotic facial expressions are often labor-intensive, lack adaptability across contexts and platforms, and have limited expressive ranges-leading to repetitive behaviors that reduce interaction quality, particularly in long-term scenarios. We introduce Xpress, a system that leverages language models (LMs) to dynamically generate context-aware facial expressions for robots through a three-phase process: encoding temporal flow, conditioning expressions on context, and generating facial expression code. We demonstrated Xpress as a proof-of-concept through two user studies (n=15x2) and a case study with children and parents (n=13), in storytelling and conversational scenarios to assess the system's context-awareness, expressiveness, and dynamism. Results demonstrate Xpress's ability to dynamically produce expressive and contextually appropriate facial expressions, highlighting its versatility and potential in HRI applications.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Creative Storytelling and Social Engagement",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation,Case Studies",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Anthropomorphism",
  "application": "Social and Conversational Systems"
},
{
  "id": 22,
  "year": 2024,
  "authors": "Stampf, Annika; Colley, Mark; Girst, Bettina; Rukzio, Enrico",
  "title": "Exploring Passenger-Automated Vehicle Negotiation Utilizing Large Language Models for Natural Interaction",
  "venue": "Proceedings of the 16th International Conference on Automotive User Interfaces and Interactive Vehicular Applications",
  "doi": "10.1145/3640792.3675725",
  "url": "https://dl.acm.org/doi/10.1145/3640792.3675725",
  "abstract": "As vehicle automation advances to SAE Levels 3 to 5, transitioning driving control from human to system, ensuring automated vehicles (AVs) align with user preferences becomes a challenge. Natural interaction emerges as a common goal, offering ways to convey user interests in a user-friendly manner. However, technical, legal, or design constraints may prevent fulfilling these preferences, leading to potential conflicts. Through an online survey (N=50), potential driver-passenger conflicts and their handling strategies were explored. Subsequently, in a Virtual Reality study (N=14), we applied identified strategies (ranging from distracting to motivating and adhering to social norms) to user-AV interactions using a state-of-the-art language model (GPT-4 Turbo) primed with the strategies to simulate realistic dialogues. Additionally, adaptive communication was compared to non-adaptive communication. Our findings reveal a preference for adaptive communication. Yet, despite using advanced modeling, accurately predicting user interactions remained challenging, with users often trying to outsmart the AI.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Voice,Visuals",
  "morphology": "AR/VR",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,WoZ",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Usability,Safety",
  "application": "AR/VR-enabled interactions"
},
{
  "id": 23,
  "year": 2025,
  "authors": "Pan, Ziqi; Zhang, Xiucheng; Li, Zisu; Peng, Zhenhui; Fan, Mingming; Ma, Xiaojuan",
  "title": "ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts",
  "venue": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems",
  "doi": "10.1145/3706598.3713791",
  "url": "https://dl.acm.org/doi/10.1145/3706598.3713791",
  "abstract": "Intelligent agents coexisting with humans often need to interact with human-shared objects in environments. Thus, agents should plan their interactions based on objects' affordances and the current situation to achieve acceptable outcomes. How to support intelligent agents' planning of affordance-based interactions compatible with human perception and values in real-world contexts remains under-explored. We conducted a formative study identifying the physical, intrapersonal, and interpersonal contexts that count to household human-agent interaction. We then proposed ACKnowledge, a computational framework integrating a dynamic knowledge graph, a large language model, and a vision language model for affordance-based interaction planning in dynamic human environments. In evaluations, ACKnowledge generated acceptable planning results with an understandable process. In real-world simulation tasks, ACKnowledge achieved a high execution success rate and overall acceptability, significantly enhancing usage-rights respectfulness and social appropriateness over baselines. The case study's feedback demonstrated ACKnowledge's negotiation and personalization capabilities toward an understandable planning process.",
  "sense": "Integrated Visual-Language Reasoning,Task Intent Formulation,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Visuals,Motion",
  "morphology": "Desktop companions",
  "autonomy": "Full Autonomy",
  "studyMethod": "Interviews,Questionnaires,Technical Evaluation,Wizard-of-Oz (WoZ),Case Studies,Simulations",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Usability",
  "application": "Domestic and everyday use"
},
{
  "id": 24,
  "year": 2024,
  "authors": "Arjmand, Mehdi; Nouraei, Farnaz; Steenstra, Ian; Bickmore, Timothy",
  "title": "Empathic Grounding: Explorations using Multimodal Interaction and Large Language Models with Conversational Agents",
  "venue": "Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents",
  "doi": "10.1145/3652988.3673949",
  "url": "https://dl.acm.org/doi/10.1145/3652988.3673949",
  "abstract": "We introduce the concept of “empathic grounding” in conversational agents as an extension of Clark's conceptualization of grounding in conversation in which the grounding criterion includes listener empathy for the speaker's affective state. Empathic grounding is generally required whenever the speaker's emotions are foregrounded and can make the grounding process more efficient and reliable by communicating both propositional and affective understanding. Both speaker expressions of affect and listener empathic grounding can be multimodal, including facial expressions and other nonverbal displays. Thus, models of empathic grounding for embodied agents should be multimodal to facilitate natural and efficient communication. We describe a multimodal model that takes as input user speech and facial expression to generate multimodal grounding moves for a listening agent using a large language model. We also describe a testbed to evaluate approaches to empathic grounding, in which a humanoid robot interviews a user about a past episode of pain and then has the user rate their perception of the robot's empathy. We compare our proposed model to one that only generates non-affective grounding cues in a between-subjects experiment. Findings demonstrate that empathic grounding increases user perceptions of empathy, understanding, emotional intelligence, and trust. Our work highlights the role of emotion awareness and multimodality in generating appropriate grounding moves for conversational agents.",
  "sense": "Integrated Visual-Language Reasoning,Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Wizard-of-Oz (WoZ)",
  "evaluationMetrics": "User's Perceptual and Relational Experience",
  "application": "Domestic and Everyday Use"
},
{
  "id": 25,
  "year": 2024,
  "authors": "Pereira, Andre; Marcinek, Lubos; Miniota, Jura; Thunberg, Sofia; Lagerstedt, Erik; Gustafson, Joakim; Skantze, Gabriel; Irfan, Bahar",
  "title": "Multimodal User Enjoyment Detection in Human-Robot Conversation: The Power of Large Language Models",
  "venue": "Proceedings of the 26th International Conference on Multimodal Interaction",
  "doi": "10.1145/3678957.3685729",
  "url": "https://dl.acm.org/doi/10.1145/3678957.3685729",
  "abstract": "Enjoyment is a crucial yet complex indicator of positive user experience in Human-Robot Interaction (HRI). While manual enjoyment annotation is feasible, developing reliable automatic detection methods remains a challenge. This paper investigates a multimodal approach to automatic enjoyment annotation for HRI conversations, leveraging large language models (LLMs), visual, audio, and temporal cues. Our findings demonstrate that both text-only and multimodal LLMs with carefully designed prompts can achieve performance comparable to human annotators in detecting user enjoyment. Furthermore, results reveal a stronger alignment between LLM-based annotations and user self-reports of enjoyment compared to human annotators. While multimodal supervised learning techniques did not improve all of our performance metrics, they could successfully replicate human annotators and highlighted the importance of visual and audio cues in detecting subtle shifts in enjoyment. This research demonstrates the potential of LLMs for real-time enjoyment detection, paving the way for adaptive companion robots that can dynamically enhance user experiences.",
  "sense": "Modular Perception and Textual Abstraction,Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience",
  "application": "Social and Conversational Systems"
},
{
  "id": 26,
  "year": 2025,
  "authors": "Hu, Yaxin; Zhu, Anjun; Toma, Catalina L.; Mutlu, Bilge",
  "title": "Designing Telepresence Robots to Support Place Attachment",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721522",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721522",
  "abstract": "People feel attached to places that are meaningful to them, which psychological research calls ''place attachment.'' Place attachment is associated with self-identity, self-continuity, and psychological well-being. Even small cues, including videos, images, sounds, and scents, can facilitate feelings of connection and belonging to a place. Telepresence robots that allow people to see, hear, and interact with a remote place have the potential to establish and maintain a connection with places and support place attachment. In this paper, we explore the design space of robotic telepresence to promote place attachment, including how users might be guided in a remote place and whether they experience the environment individually or with others. We prototyped a telepresence robot that allows one or more remote users to visit a place and be guided by a local human guide or a conversational agent. Participants were 38 university alumni who visited their alma mater via the telepresence robot. Our findings uncovered four distinct user personas in the remote experience and highlighted the need for social participation to enhance place attachment. We generated design implications for future telepresence robot design to support people's connections with places of personal significance.",
  "sense": "Task Intent Formulation",
  "interaction": "Social Initiation",
  "alignment": "",
  "modality": "Text,Voice,Motion",
  "morphology": "Functional",
  "autonomy": "Teleoperation",
  "studyMethod": "Field Deployments,Interviews,Technical Evaluation,Wizard-of-Oz (WoZ)",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Perceived Intelligence,Usability",
  "application": "Public Spaces Service"
},
{
  "id": 27,
  "year": 2025,
  "authors": "Goubard, Cedric; Demiris, Yiannis",
  "title": "Cognitive Modelling of Visual Attention Captures Trust Dynamics in Human-Robot Collaboration",
  "venue": "ACM Transactions on Human-Robot Interaction",
  "doi": "10.1145/3732795",
  "url": "https://dl.acm.org/doi/10.1145/3732795",
  "abstract": "Understanding how humans perceive and interact with robots is crucial for collaborative scenarios. Trust, a pivotal factor in such interactions, is inherently volatile and subjective, posing significant challenges for robots. However, trust has also been shown to influence specific human bio-signals and behaviours, suggesting that it could be inferred from those indicators. One such indicator is visual attention, the cognitive process of focusing on distinct environmental elements, often manifested through eye gaze. Despite recent research connecting eye gaze and trust in Human-Robot Collaboration scenarios, this relationship remains largely unexplored. This paper presents a novel signal, the Attention Arbitration Ratio (AAR), which is shown to be a promising real-time predictor of subjective and objective trust measures. We obtain this signal using a visual attention modelling framework that explicitly emulates the Bottom-Up and Top-Down processes, two key cognitive components. We demonstrate the connection between the AAR and trust using Bayesian data analysis, and we analyse the sensitivity of that connection with different visual attention models. For evaluation purposes, we collected gaze data and trust questionnaires from 49 interactions where 29 participants engaged in a collaborative assistive cooking task with a robot, for a total duration of 24h53 of data collection.",
  "sense": "Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Visuals,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "LLM-Specific Performance",
  "application": "Domestic and Everyday Use"
},
{
  "id": 28,
  "year": 2024,
  "authors": "Axelsson, Minja; Spitale, Micol; Gunes, Hatice",
  "title": "\"Oh, Sorry, I Think I Interrupted You\": Designing Repair Strategies for Robotic Longitudinal Well-being Coaching",
  "venue": "Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3610977.3634948",
  "url": "https://dl.acm.org/doi/10.1145/3610977.3634948",
  "abstract": "Robotic well-being coaches have been shown to successfully promote people's mental well-being. To provide successful coaching, a robotic coach should have the capability to repair the mistakes it makes. Past investigations of robot mistakes are limited to game or task-based, one-off and in-lab studies. This paper presents a 4-phase design process to design repair strategies for robotic longitudinal well-being coaching with the involvement of real-world stakeholders: 1) designing repair strategies with a professional well-being coach; 2) a longitudinal study with the involvement of experienced users (i.e., who had already interacted with a robotic coach) to investigate the repair strategies defined in (1); 3) a design workshop with users from the study in (2) to gather their perspectives on the robotic coach's repair strategies; 4) discussing the results obtained in (2) and (3) with the mental well-being professional to reflect on how to design repair strategies for robotic coaching. Our results show that users have different expectations for a robotic coach than a human coach, which influences how repair strategies should be designed. We show that different repair strategies (e.g., apologizing, explaining, or repairing empathically) are appropriate in different scenarios, and that preferences for repair strategies change during longitudinal interactions with the robotic coach.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Emotional Repair in Social Interaction",
  "modality": "Voice,Motion",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Interviews,Questionnaires,Co-design Workshops,BodyStorming",
  "evaluationMetrics": "User's Perceptual and Relational Experience",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 29,
  "year": 2025,
  "authors": "Hsu, Long-Jing; Bays, Janice; Swaminathan, Manasi; Khoo, Weslie; Sato, Hiroki; Amon, Kyrie Jig; Dobbala, Sathvika; Thant, Min Min; Foster, Alex; Tsui, Kate; Stafford, Philip B.; Crandall, David; Sabanovic, Selma",
  "title": "Research as Care: A Reflection on Incorporating the Ethics of Care in Design Research with People Living with Dementia",
  "venue": "Proceedings of the 2025 ACM Designing Interactive Systems Conference",
  "doi": "10.1145/3715336.3735678",
  "url": "https://dl.acm.org/doi/10.1145/3715336.3735678",
  "abstract": "When computing researchers design technologies for vulnerable populations and engage with them over extended periods, researchers may incorporate \"care\"—deliberate actions to build and maintain relationships with participants—to improve engagement and deepen their understanding of situated perspectives. However, when researchers choose to take actions involving care, these efforts are rarely made explicit. Reflecting on our three-year project of designing and testing a social robot with 31 participants living with dementia, we realized the benefit of intentional reflection on the ethics and practice of care during the research process. We offer \"research as care\" guidelines into computing design research: 1) viewing participants as individuals, 2) being intentional in the ongoing and dynamic engagement, 3) acknowledging the reciprocity inherent in care, 4) reporting care practices transparently, 5) tailoring care to the specific context, and 6) making an informed choice to incorporate care. By incorporating research as care, computing design researchers can provide a more productive experience for participants and enhance their designs' overall quality and validity.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Creative Storytelling and Social Engagement,Social Initiation",
  "alignment": "Sustained Personalization,Episodic Memory Integration,Emotional Repair in Social Interaction",
  "modality": "Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments,Interviews,Wizard-of-Oz (WoZ),Co-design Workshops",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 30,
  "year": 2023,
  "authors": "Cho, Hyungjun; Lee, Jiyeon; Ku, Bonhee; Jeong, Yunwoo; Yadgarova, Shakhnozakhon; Nam, Tek-Jin",
  "title": "ARECA: A Design Speculation on Everyday Products Having Minds",
  "venue": "Proceedings of the 2023 ACM Designing Interactive Systems Conference",
  "doi": "10.1145/3563657.3596002",
  "url": "https://dl.acm.org/doi/10.1145/3563657.3596002",
  "abstract": "An increasing number of everyday products are being designed to possess qualities such as intelligence, consciousness, and emotion. However, there is a need for more understanding on how to design for these properties of mind. To address this, we present the design of Areca, an air purifier that keeps a diary. This paper outlines our design process, focusing on how the diary generation process gives Areca properties of mind and how its appearance and interaction design support this concept. Through exhibiting Areca in a design exhibition, we gathered people's initial reactions and perceptions to further evaluate the effectiveness of our design intentions. Finally, based on these experiences, we engage in discussions on the design of products having minds.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Persona Adaptation and Conversational Fluidity,Creative Storytelling and Social Engagement",
  "alignment": "",
  "modality": "Visuals,Tangible and Haptic Interaction",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments",
  "evaluationMetrics": "Perceived Intelligence,Anthropomorphism,Usability",
  "application": "Domestic and Everyday Use"
},
{
  "id": 31,
  "year": 2023,
  "authors": "Axelsson, Agnes; Skantze, Gabriel",
  "title": "Do you follow? A fully automated system for adaptive robot presenters",
  "venue": "Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3568162.3576958",
  "url": "https://dl.acm.org/doi/10.1145/3568162.3576958",
  "abstract": "An interesting application for social robots is to act as a presenter, for example as a museum guide. In this paper, we present a fully automated system architecture for building adaptive presentations for embodied agents. The presentation is generated from a knowledge graph, which is also used to track the grounding state of information, based on multimodal feedback from the user. We introduce a novel way to use large-scale language models (GPT-3 in our case) to lexicalise arbitrary knowledge graph triples, greatly simplifying the design of this aspect of the system. We also present an evaluation where 43 participants interacted with the system. The results show that users prefer the adaptive system and consider it more human-like and flexible than a static version of the same system, but only partial results are seen in their learning of the facts presented by the robot.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Task-Oriented Planning and Execution,Creative Storytelling and Social Engagement",
  "alignment": "Sustained Personalization,Behavioral Repair in Task Execution",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Anthropomorphism",
  "application": "Teaching and Education"
},
{
  "id": 32,
  "year": 2025,
  "authors": "Ferrini, Lorenzo; Andriella, Antonio; Ros, Raquel; Lemaignan, Séverin",
  "title": "From Percepts to Semantics: A Multi-modal Saliency Map to Support Social Robots' Attention",
  "venue": "ACM Transactions on Human-Robot Interaction",
  "doi": "10.1145/3737891",
  "url": "https://dl.acm.org/doi/10.1145/3737891",
  "abstract": "In social robots, visual attention expresses awareness of the scenario components and dynamics. As in humans, their attention should be driven by a combination of different attention mechanisms. In this paper, we introduce multi-modal saliency maps, i.e. spatial representations of saliency that dynamically integrate multiple attention sources depending on the context. We provide the mathematical formulation of the model and an open-source software implementation. Finally, we present an initial exploration of its potential in social interaction scenarios with humans, and evaluate its implementation.",
  "sense": "Modular Perception and Textual Abstraction,Human Model Alignment",
  "interaction": "",
  "alignment": "",
  "modality": "Visuals,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance",
  "application": "Social and Conversational Systems"
},
{
  "id": 33,
  "year": 2025,
  "authors": "Mahadevan, Karthik; Lewis, Blaine; Li, Jiannan; Mutlu, Bilge; Tang, Anthony; Grossman, Tovi",
  "title": "ImageInThat: Manipulating Images to Convey User Instructions to Robots",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721582",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721582",
  "abstract": "Foundation models are rapidly improving the capability of robots in performing everyday tasks autonomously such as meal preparation, yet robots will still need to be instructed by humans due to model performance, the difficulty of capturing user preferences, and the need for user agency. Robots can be instructed using various methods---natural language conveys immediate instructions but can be abstract or ambiguous, whereas end-user programming supports longer-horizon tasks but interfaces face difficulties in capturing user intent. In this work, we propose using direct manipulation of images as an alternative paradigm to instruct robots, and introduce a specific instantiation called ImageInThat which allows users to perform direct manipulation on images in a timeline-style interface to generate robot instructions. Through a user study, we demonstrate the efficacy of ImageInThat to instruct robots in kitchen manipulation tasks, comparing it to a text-based natural language instruction method. The results show that participants were faster with ImageInThat and preferred to use it over the text-based method. Supplementary material including code can be found at: https://image-in-that.github.io/.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Visuals,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Case Studies,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Usability,Cognitive Load and Workload",
  "application": "Domestic and Everyday Use"
},
{
  "id": 34,
  "year": 2025,
  "authors": "Kontogiorgos, Dimosthenis; Shah, Julie",
  "title": "Questioning the Robot: Using Human Non-verbal Cues to Estimate the Need for Explanations",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721577",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721577",
  "abstract": "As black-box AI systems become increasingly complex, understanding when and how to provide explanations to users is crucial. Multimodal signals, such as facial expressions, offer novel insights into how frequently explanations should be given. This paper explores whether users' facial features can help estimate the need for explanations in a collaborative robot task. We applied three state-of-the-art eXplainable AI (XAI) methods, addressing how, why, and what-if questions, explaining the robot's failure detection model. Each explanation type conveyed information differently: how-explanations described how the model functions, why-explanations provided personalised insights into input-feature-related cues, and what-if-explanations explored alternative scenarios. In a mixed-design study (N = 33), participants performed a robot-assisted pick-and-place task, receiving different explanation types. Our results show that users responded differently to these explanations, with why-explanations being the most preferred and prompting closer alignment in facial expressions with the robot. Contrary to expectations, what-if explanations led to the least alignment and required greater vocal effort. These findings demonstrate how non-verbal cues can guide the frequency and type of explanations (personalised or general) and further highlight the importance of model transparency in human-robot collaboration.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Task-Oriented Planning and Execution",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Proximity",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience",
  "application": "Industrial manufacturing"
},
{
  "id": 35,
  "year": 2025,
  "authors": "Bellucci, Andrea; Jacucci, Giulio; Trung, Kien Duong; Das, Pritom Kumar; Smirnov, Sergei Viktorovich; Ahmed, Imtiaj; Lugrin, Jean-Luc",
  "title": "Immersive tailoring of embodied agents using large language models",
  "venue": "2025 IEEE conference virtual reality and 3D user interfaces (VR)",
  "doi": "DOI: 10.1109/VR59515.2025.00063",
  "url": "https://ieeexplore.ieee.org/document/10937440",
  "abstract": "LLM-based embodied agents are recently emerging in VR, supporting various scenarios such as pedagogical assistants, virtual companions, and NPCs for games. These agents hold potential to enhance user interactions but require thoughtful design to cater diverse user needs and contexts. We present an architecture that leverages different LLM modules to enable conversational interactions with an embodied agent in multi-user VR. Our system's primary goal is to facilitate immersive tailoring through conversational input, allowing users to dynamically adjust an agent's behavior and properties (e.g., role, personality, and appearance), directly within the virtual space, rather than during development or via separate interfaces. We evaluate the system's performance, measuring latency during tailoring tasks, and share insights from a six-week study involving five users exploring various scenarios. While the approach shows promise, challenges remain, including reducing latency in the speech-to-text-to-speech pipeline and addressing the black-box limitations of LLMs, highlighting areas for future research.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "AR/VR",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance",
  "application": "AR/VR-enabled Interactions"
},
{
  "id": 36,
  "year": 2024,
  "authors": "Verma, Mudit; Bhambri, Siddhant; Kambhampati, Subbarao",
  "title": "Theory of Mind Abilities of Large Language Models in Human-Robot Interaction: An Illusion?",
  "venue": "Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3610978.3640767",
  "url": "https://dl.acm.org/doi/10.1145/3610978.3640767",
  "abstract": "Large Language Models (LLMs) have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of LLMs especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs an LLM to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. The high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack. We report our results on GPT-4 and GPT-3.5-turbo.",
  "sense": "Static and Semi-Static Context Injection,Integrated Visual-Language Reasoning,Task Intent Formulation,Human Model Alignment",
  "interaction": "",
  "alignment": "",
  "modality": "Text,Visuals,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Case Studies",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience",
  "application": "Social and Conversational Systems"
},
{
  "id": 37,
  "year": 2024,
  "authors": "Mahadevan, Karthik; Chien, Jonathan; Brown, Noah; Xu, Zhuo; Parada, Carolina; Xia, Fei; Zeng, Andy; Takayama, Leila; Sadigh, Dorsa",
  "title": "Generative Expressive Robot Behaviors using Large Language Models",
  "venue": "Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3610977.3634999",
  "url": "https://dl.acm.org/doi/10.1145/3610977.3634999",
  "abstract": "People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying \"excuse me\" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.",
  "sense": "Task Intent Formulation",
  "interaction": "Embodied Social Expressiveness",
  "alignment": "Behavioral Repair in Task Execution,Emotional Repair in Social Interaction,Repair in Ethical and Normative Alignment",
  "modality": "Text,Voice,Visuals,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Questionnaires,Technical Evaluation,simulations",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience",
  "application": "Social and Conversational Systems"
},
{
  "id": 38,
  "year": 2025,
  "authors": "Skantze, Gabriel; Irfan, Bahar",
  "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction",
  "venue": "Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.5555/3721488.3721593",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721593",
  "abstract": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Social Initiation",
  "alignment": "",
  "modality": "Voice,Visuals",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing",
  "application": "Social and Conversational Systems"
},
{
  "id": 39,
  "year": 2024,
  "authors": "Mannava, Vivek; Mitrevski, Alex; Plöger, Paul G.",
  "title": "Exploring the suitability of conversational AI for child-robot interaction",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731435",
  "url": "https://ieeexplore.ieee.org/document/10731435",
  "abstract": "Current approaches in education, while aiming to be universally effective, often struggle to fully adapt to the unique needs and communication styles of individual children; this disparity can limit the children's engagement and hinder their learning progress. Similarly, parents or guardians, despite their good intentions, may also be unable to provide consistent and personalized support to each child. In this work, we investigate the use of conversational systems for socially assistive robots (SARs) as a potential solution to this problem, as such systems have the potential to allow children to interact and learn at their own pace, in a way that aligns with their communication preferences. To ensure that the robot's language is suitable for children, we present a system that leverages a combination of natural language processing (NLP) techniques, including dialog management, child-friendly language generation, and context-aware response adaptation; to achieve this, our system combines Rasa for dialog management, GPT-3.5 for language generation, and textstat for language complexity evaluation. We evaluate the suitability of the generated language for a young audience through two user studies with adult participants, one in which the conversational system was embodied in a robot and involved direct interaction between a human and a robot, and another where participants evaluated conversational transcripts from the first study. Our results suggest that the system has the potential to maintain engaging and safe conversations, and adapt its language to individual needs.",
  "sense": "Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation",
  "alignment": "Sustained Personalization,Episodic Memory Integration,Emotional Repair in Social Interaction,Repair in Ethical and Normative Alignment",
  "modality": "Text,Voice,Visuals",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Safety",
  "application": "Teaching and Education"
},
{
  "id": 40,
  "year": 2024,
  "authors": "Sievers, Thomas; Russwinkel, Nele",
  "title": "Introducing a note of levity to human-robot interaction with dialogs containing irony, sarcasm and jocularity",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731234",
  "url": "https://ieeexplore.ieee.org/document/10731234",
  "abstract": "Dialogs between humans and social robots are often fact-oriented and not very mood-dependent, at least on the part of the robot. This raises the question of what influence the tonality of the utterances has on the perceived personality of the robot and how this influences the user's further expectations. One element that is not necessarily expected from robots in conversation is humor, especially the use of irony and sarcasm. We examined the effect of a conversation with cheerfully ironic or sullenly sarcastic remarks by the robot in contrast to utterances in a more seriously neutral tone. To measure these effects in terms of perceived warmth, competence and possible discomfort in conversation we used the Robotic Social Attributes Scale (RoSAS) on 20 participants with an age ranging from 26 to 58. In addition, we asked the participants for their assessment of the suitability of the robot's dialog style for different areas of application and which dialog style they personally liked best. The dialog parts of the robot were generated with ChatGPT using suitable prompts. Our results showed that the perceived warmth gained highest rates when when the robot hat a cheerfully ironic tone. Discomfort was rated significantly higher when a sullenly sarcastic tone was used by the robot. Perceived competence seemed to be slightly negatively influenced in the sarcastic condition. The results proved that tonality appears to be a very relevant design element for a successful interaction between humans and robots, but must be used carefully to achieve the desired goal.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Episodic Memory Integration",
  "modality": "Visuals,Motion,Proximity",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Usability",
  "application": "Social and Conversational Systems"
},
{
  "id": 41,
  "year": 2024,
  "authors": "Sievers, Thomas; Russwinkel, Nele",
  "title": "Interacting with a sentimental robot - making human emotions tangible for a social robot via ChatGPT",
  "venue": "2024 IEEE international conference on advanced robotics and its social impacts (ARSO)",
  "doi": "10.1109/ARSO60199.2024.10557749",
  "url": "https://ieeexplore.ieee.org/document/10557749",
  "abstract": "Recognizing and correctly assessing the emotions of a human conversation partner is - if successful - a milestone in social interaction between humans and social robots. The robot should recognize human emotions and take them into account in its reactions. It is also important that humans and robots assess the content of the dialog from an emotional perspective in the same way. But how can the emotional state of a person in a dialog with a robot be accessed? We examined this question more closely in our study. A Large Language Model (LLM) from OpenAI (ChatGPT) was used for conversations with a Pepper robot. We had the course of the dialog assessed once by the human interlocutor and once by the GPT model itself using sentiment analysis. In addition, the predominant emotion was named by both conversation partners after the dialog. A comparison of these evaluations provided an assessment of whether the human and the social robot arrived at the same results. We were also investigating whether the transmission of emotion recognition data had a noticeable influence on the tonality of the conversation. To do this, we used the robot's emotion recognition capabilities to send cues to the GPT model about the current emotional state of the human at each turn of the conversation, so that the LLM could take this into account in the generation of the robot's utterances. It was found that the predominant emotion of the human and the general mood of a conversation were interpreted by humans and the GPT model in largely the same way, whereby an existing emotion recognition made the robot's assessment of the general mood deviate noticeably.",
  "sense": "Emotional Grounding",
  "interaction": "Social Initiation",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Tangible and Haptic Interaction",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism",
  "application": "Social and Conversational Systems"
},
{
  "id": 42,
  "year": 2023,
  "authors": "Cui, Yuchen; Karamcheti, Siddharth; Palleti, Raj; Shivakumar, Nidhya; Liang, Percy; Sadigh, Dorsa",
  "title": "No, to the Right: Online Language Corrections for Robotic Manipulation via Shared Autonomy",
  "venue": "Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction",
  "doi": "10.1145/3568162.3578623",
  "url": "https://dl.acm.org/doi/10.1145/3568162.3578623",
  "abstract": "Systems for language-guided human-robot interaction must satisfy two key desiderata for broad adoption: adaptivity and learning efficiency. Unfortunately, existing instruction-following agents cannot adapt, lacking the ability to incorporate online natural language supervision, and even if they could, require hundreds of demonstrations to learn even simple policies. In this work, we address these problems by presenting Language-Informed Latent Actions with Corrections (LILAC), a framework for incorporating and adapting to natural language corrections \"to the right\", or \"no, towards the book\" - online, during execution. We explore rich manipulation domains within a shared autonomy paradigm. Instead of discrete turn-taking between a human and robot, LILAC splits agency between the human and robot: language is an input to a learned model that produces a meaningful, low-dimensional control space that the human can use to guide the robot. Each real-time correction refines the human's control space, enabling precise, extended behaviors - with the added benefit of requiring only a handful of demonstrations to learn. We evaluate our approach via a user study where users work with a Franka Emika Panda manipulator to complete complex manipulation tasks. Compared to existing learned baselines covering both open-loop instruction following and single-turn shared autonomy, we show that our corrections-aware approach obtains higher task completion rates, and is subjectively preferred by users because of its reliability, precision, and ease of use.",
  "sense": "Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Text,Visuals,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,Usability",
  "application": "Other"
},
{
  "id": 43,
  "year": 2023,
  "authors": "Ye, Yang; You, Hengxu; Du, Jing",
  "title": "Improved trust in human-robot collaboration with ChatGPT",
  "venue": "IEEE access : practical innovations, open solutions",
  "doi": "DOI: 10.1109/ACCESS.2023.3282111",
  "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141597",
  "abstract": "Human-robot collaboration is becoming increasingly important as robots become more involved in various aspects of human life in the era of Artificial Intelligence. However, the issue of human operators' trust in robots remains a significant concern, primarily due to the lack of adequate semantic understanding and communication between humans and robots. The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach. This paper explores the impact of ChatGPT on trust in a human-robot collaboration assembly task. This study designs a robot control system called RoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human operators fetch, and place tools, while human operators can communicate with and control the robot arm using natural language. A human-subject experiment showed that incorporating ChatGPT in robots significantly increased trust in human-robot collaboration, which can be attributed to the robot's ability to communicate more effectively with humans. Furthermore, ChatGPT's ability to understand the nuances of human language and respond appropriately helps to build a more natural and intuitive human-robot interaction. The findings of this study have significant implications for the development of trustworthy human-robot collaboration systems.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Task-Oriented Planning and Execution",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Cognitive Load and Workload",
  "application": "Industrial Manufacturing"
},
{
  "id": 44,
  "year": 2024,
  "authors": "Su, Zhidong; Sheng, Weihua",
  "title": "ChatAdp: ChatGPT-powered adaptation system for human-robot interaction",
  "venue": "2024 IEEE international conference on robotics and automation (ICRA)",
  "doi": "10.1109/ICRA57147.2024.10611520",
  "url": "https://ieeexplore.ieee.org/document/10611520",
  "abstract": "Different people have different preferences when it comes to human-robot interaction. Therefore, it is desirable for the robot to adapt its actions to fit users' preferences. Human feedback is essential to facilitating robot adaptation. However, when the task is complex or the robot action space is large, it requires a large amount of user feedback. ChatGPT is a powerful generative AI tool based on large language models (LLMs), which possesses a significant corpus of information obtained from human society, and exhibits robust proficiency in the comprehension and acquisition of natural language. Therefore, in this paper, we proposed a ChatGPT-powered adaptation system (ChatAdp) for human-robot interaction which requires less user feedback to achieve a good adaptation result. In the proposed ChatAdp, we use ChatGPT as a user simulator to provide feedback. We evaluated ChatAdp in a case study for context-aware conversation adaptation. The results are very promising. Our proposed method can achieve a mean success rate of 92% on the user's natural language-described preferences after receiving 33 rounds of feedback from a user on average, which is only 2% of the number of states covered by the user preferences and outperforms the two baseline methods.",
  "sense": "Human Model Alignment",
  "interaction": "Social Initiation,Anticipatory Assistance",
  "alignment": "Sustained Personalization",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Case Studies",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Safety",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 45,
  "year": 2023,
  "authors": "Zhang, Bowen; Soh, Harold",
  "title": "Large language models as zero-shot human models for human-robot interaction",
  "venue": "2023 IEEE/RSJ international conference on intelligent robots and systems (IROS)",
  "doi": "10.1109/IROS55552.2023.10341488",
  "url": "https://ieeexplore.ieee.org/document/10341488",
  "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios focused on the important element of trust. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n=65) where preliminary results show that planning with an LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.",
  "sense": "Static and Semi-Static Context Injection,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Proximity",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Case Studies",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Safety",
  "application": "Social and Conversational Systems"
},
{
  "id": 46,
  "year": 2024,
  "authors": "Latif, Ehsan; Parasuraman, Ramviyas; Zhai, Xiaoming",
  "title": "PhysicsAssistant: An LLM-powered interactive learning robot for physics lab investigations",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731312",
  "url": "https://ieeexplore.ieee.org/document/10731312",
  "abstract": "Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Episodic Memory Integration,Behavioral Repair in Task Execution",
  "modality": "Text,Voice,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Case Studies",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience",
  "application": "Teaching and Education"
},
{
  "id": 47,
  "year": 2024,
  "authors": "Yu, Hongqi; Tang, Fei; Zhang, Lei; Gomez, Randy; Nichols, Eric; Li, Guangliang",
  "title": "Improving perceived emotional intelligence of embodied chatbot haru via multi-modal interaction",
  "venue": "2024 IEEE international conference on robotics and biomimetics (ROBIO)",
  "doi": "10.1109/ROBIO64047.2024.10907584",
  "url": "https://ieeexplore.ieee.org/document/10907584",
  "abstract": "To realize natural and friendly interaction, this paper proposed an emotion-aware framework to enable embodied chatbot Haru to mimic human users' emotions via multi-modal interaction during conversation. The proposed framework consists of ASR for speech recognition, ChatGPT for dialogue generation and Text-to-Speech for text to speech conversion. In addition, the pitch, rate and emotions of human speech detected with deep learning models are used to adjust the pitch and rate of Haru's speech. In addition, Haru can mimic the emotional state of human users by expressing emotive routine behaviors consisting of base rotation, eye animation, mouth movement etc., based on the detected emotion of human speech. Results of a user study with 20 subjects show that our embodied chatbot Haru can successfully mimic the intonation and emotion of human speech via vocal and visual interactions. Moreover, participants reported to have a better conversing experience with our embodied chatbot Haru compared to a neutral one with only lip synchronization.",
  "sense": "Emotional Grounding",
  "interaction": "Embodied Social Expressiveness",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion",
  "morphology": "Desktop Companions",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism",
  "application": "Social and Conversational Systems"
},
{
  "id": 48,
  "year": 2024,
  "authors": "Pinto, Maria J.; Belpaeme, Tony",
  "title": "Predictive turn-taking: Leveraging language models to anticipate turn transitions in human-robot dialogue",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731379",
  "url": "https://ieeexplore.ieee.org/document/10731379/",
  "abstract": "Natural and engaging spoken dialogue systems require seamless turn-taking coordination to avoid awkward interruptions and unnatural pauses. Traditional systems often rely on simplistic silence thresholds, relinquishing the turn after a predetermined period of silence, which invariably leads to a suboptimal interaction experience. This work explores the potential of Large Language Models (LLMs) for improved turn-taking prediction. Building upon research that uses linguistic cues, we investigate how LLMs, with their rich contextual knowledge and semantic encoding of language, can be used for this task. We hypothesize that by analysing dialogue context, syntactic structure, and pragmatic cues within the user's utterance, LLMs can offer more accurate turn-completion predictions. This research evaluates the capabilities of recent LLMs such as Gemini, OpenAI's API, Anthropic's Claude2, and Meta AI's Llama 2 to predict turn-ending points solely based on textual information, and demonstrates how the conversation between elderly users and companion robots can be enhanced by LLM-powered end-of-turn prediction.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Embodied Social Expressiveness,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Voice,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,LLM-Specific Performance",
  "application": "Social and Conversational Systems"
},
{
  "id": 49,
  "year": 2025,
  "authors": "Bastin, Brieuc; Hasegawa, Shoichi; Solis, Jorge; Ronsse, Renaud; Macq, Benoit; El Hafi, Lotfi; Ricardez, Gustavo Alfonso Garcia; Taniguchi, Tadahiro",
  "title": "GPTAlly: a safety-oriented system for human-robot collaboration based on foundation models",
  "venue": "2025 IEEE/SICE international symposium on system integration (SII)",
  "doi": "10.1109/SII59315.2025.10870936",
  "url": "https://ieeexplore.ieee.org/document/10870936",
  "abstract": "As robots increasingly integrate into the workplace, Human-Robot Collaboration (HRC) has become increasingly important. However, most HRC solutions are based on pre-programmed tasks and use fixed safety parameters, which keeps humans out of the loop. To overcome this, HRC solutions that can easily adapt to human preferences during the operation as well as their safety precautions considering the familiarity with robots are necessary. In this paper, we introduce GPTAlly, a novel safety-oriented system for HRC that leverages the emerging capabilities of Large Language Models (LLMs). GPTAlly uses LLMs to 1) infer users' subjective safety perceptions to modify the parameters of a Safety Index algorithm; 2) decide on subsequent actions when the robot stops to prevent unwanted collisions; and 3) re-shape the robot arm trajectories based on user instructions. We subjectively evaluate the robot's behavior by comparing the safety perception of GPT-4 to the participants. We also evaluate the accuracy of natural language-based robot programming of decision-making requests. The results show that GPTAlly infers safety perception similarly to humans, and achieves an average of 80% of accuracy in decision-making, with few instances under 50%. Code available at: https://axtiop.github.io/GPTAlly",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Emotional Grounding,Task Intent Formulation,Human Model Alignment",
  "interaction": "Anticipatory Assistance",
  "alignment": "Sustained Personalization",
  "modality": "Text,Voice,Hybrid",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Safety",
  "application": "Industrial Manufacturing"
},
{
  "id": 50,
  "year": 2024,
  "authors": "Yano, Yuga; Mizutani, Akinobu; Fukuda, Yukiya; Kanaoka, Daiju; Ono, Tomohiro; Tamukoh, Hakaru",
  "title": "Unified understanding of environment, task, and human for human-robot interaction in real-world environments",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731235",
  "url": "https://ieeexplore.ieee.org/document/10731235",
  "abstract": "To facilitate human--robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.",
  "sense": "Static and Semi-Static Context Injection,Human Model Alignment",
  "interaction": "Social Initiation,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Voice,Visuals",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience",
  "application": "Public Spaces Service"
},
{
  "id": 51,
  "year": 2025,
  "authors": "Zhang, Tianyi; Au Yeung, Colin; Aurelia, Emily; Onishi, Yuki; Chulpongsatorn, Neil; Li, Jiannan; Tang, Anthony",
  "title": "Prompting an embodied AI agent: How embodiment and multimodal signaling affects prompting behaviour",
  "venue": "Proceedings of the 2025 CHI conference on human factors in computing systems",
  "doi": "10.1145/3706598.3713110",
  "url": "https://doi.org/10.1145/3706598.3713110",
  "abstract": "Current voice agents wait for a user to complete their verbal instruction before responding; yet, this is misaligned with how humans engage in everyday conversational interaction, where interlocutors use multimodal signaling (e.g. nodding, grunting, or looking at referred to objects) to ensure conversational grounding. We designed an embodied VR agent that exhibits multimodal signaling behaviors in response to situated prompts, by turning its head, or by visually highlighting objects being discussed or referred to. We explore how people prompt this agent to design and manipulate the objects in a VR scene. Through a Wizard of Oz study, we found that participants interacting with an agent that indicated its understanding of spatial and action references were able to prevent errors 30% of the time, and were more satisfied and confident in the agent's abilities. These findings underscore the importance of designing multimodal signaling communication techniques for future embodied agents.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Embodied Social Expressiveness",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Voice,Visuals,Motion,Hybrid,Proximity",
  "morphology": "VR/AR",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Wizard-of-Oz (WoZ),Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience",
  "application": "AR/VR-enabled Interactions"
},
{
  "id": 52,
  "year": 2025,
  "authors": "Shirado, Hirokazu; Shimizu, Kye; Christakis, Nicholas A; Kasahara, Shunichi",
  "title": "Realism drives interpersonal reciprocity but yields to AI-assisted egocentrism in a coordination experiment",
  "venue": "Proceedings of the 2025 CHI conference on human factors in computing systems",
  "doi": "10.1145/3706598.3713371",
  "url": "https://doi.org/10.1145/3706598.3713371",
  "abstract": "Virtual reality technologies that enhance realism and artificial intelligence (AI) systems that assist human behavior are increasingly interwoven in social applications. However, how these technologies might jointly influence interpersonal coordination remains unclear. We conducted an experiment with 240 participants in 120 pairs who interacted through remote-controlled robot cars in a physical space or virtual cars in a digital space, with or without autosteering assistance, using the chicken game, an established model of interpersonal coordination. We find that both realism and AI assistance help improve user performance but through opposing mechanisms. Real-world contexts enhanced communication, fostering reciprocal actions and collective benefits. In contrast, autosteering assistance diminished the need for interpersonal coordination, shifting participants' focus towards self-interest. Notably, when combined, the egocentric effects of autosteering assistance outweighed the prosocial effects of realism. The design of HCI systems that involve social coordination will, we believe, need to take such effects into account.",
  "sense": "",
  "interaction": "Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Visuals,Motion,Proximity",
  "morphology": "VR/AR",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Safety",
  "application": "AR/VR-enabled Interactions"
},
{
  "id": 53,
  "year": 2024,
  "authors": "Elfleet, Morad; Chollet, Mathieu",
  "title": "Investigating the impact of multimodal feedback on user-perceived latency and immersion with LLM-powered embodied conversational agents in virtual reality",
  "venue": "Proceedings of the 24th ACM international conference on intelligent virtual agents",
  "doi": "10.1145/3652988.3673965",
  "url": "https://doi.org/10.1145/3652988.3673965",
  "abstract": "Our research investigates the impact of latency on presence and immersion in virtual reality (VR) environments, focusing on interactions with LLM-powered Embodied Conversational Agents (ECAs). We explore the effectiveness of multimodal feedback strategies—including filled pauses, nonverbal turn-taking behaviours, and visual feedback—in mitigating perceived latency. Eighteen participants were subjected to both a baseline condition, without feedback interventions, and a feedback-enhanced condition. Our findings indicate that the feedback condition significantly improved the sense of presence and immersion. We also found that perceived response time and users' impressions of the agents improved, thereby increasing willingness for future interactions. Additionally, chatbot experience positively correlated with agent likeability, whereas VR experience showed no significant correlation. These results highlight the effectiveness of feedback modalities in enhancing spatial presence and overall immersion, despite latency issues in VR interactions with LLM-powered agents.",
  "sense": "Task Intent Formulation",
  "interaction": "Embodied Social Expressiveness",
  "alignment": "",
  "modality": "Voice,Visuals,Motion,Hybrid",
  "morphology": "VR/AR",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Safety",
  "application": "AR/VR-enabled Interactions"
},
{
  "id": 54,
  "year": 2024,
  "authors": "Perella-Holfeld, Francisco; Sallam, Samar; Petrie, Julia; Gomez, Randy; Irani, Pourang; Sakamoto, Yumiko",
  "title": "Parent and educator concerns on the pedagogical use of AI-equipped social robots",
  "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
  "doi": "doi.org/10.1145/3678556",
  "url": "https://doi.org/10.1145/3678556",
  "abstract": "Social robots equipped with conversational artificial intelligence are becoming increasingly common in educational settings. However, the long-term consequences of such uses remain relatively unknown due to their novelty. To ensure children's safe use of social robots, and proper adoption of the technology, it is crucial to scrutinize potential concerns regarding their usage. This exploration will provide insights to inform the design and development of this technology. Thus, this study investigated parents' and educators' perceptions of social robot use by children in the home and school settings. Our main objectives are to; 1) explore whether the types and/or levels of concern are tied to the role that individuals take (i.e., parents vs. educators); 2) explore if the levels of concern vary based on the gender and age of the potential child user; and 3) compile a catalogue of parents' and educators' concerns, both from the literature and those that are overlooked, surrounding children's use of SRs for learning. To address those inquiries, a cross-national online survey study was conducted with parents and educator participants (N = 396). Overall, participants indicated high levels of concern but recognized the potential in responsibly applying such technology for educational purposes.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "",
  "alignment": "",
  "modality": "Voice,Visuals",
  "morphology": "Desktop Companions",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Questionnaires,Wizard-of-Oz (WoZ)",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Usability,Safety",
  "application": "Teaching and Education"
},
{
  "id": 55,
  "year": 2022,
  "authors": "Elgarf, Maha; Zojaji, Sahba; Skantze, Gabriel; Peters, Christopher",
  "title": "CreativeBot: a Creative Storyteller robot to stimulate creativity in children",
  "venue": "Proceedings of the 2022 international conference on multimodal interaction",
  "doi": "10.1145/3536221.3556578",
  "url": "https://doi.org/10.1145/3536221.3556578",
  "abstract": "We present the design and evaluation of a storytelling activity between children and an autonomous robot aiming at nurturing children's creativity. We assessed whether a robot displaying creative behavior will positively impact children's creativity skills in a storytelling context. We developed two models for the robot to engage in the storytelling activity: creative model, where the robot generates creative story ideas, and the non-creative model, where the robot generates non-creative story ideas. We also investigated whether the type of the storytelling interaction will have an impact on children's creativity skills. We used two types of interaction: 1) Collaborative, where the child and the robot collaborate together by taking turns to tell a story. 2) Non-collaborative: where the robot first tells a story to the child and then asks the child to tell it another story. We conducted a between-subjects study with 103 children in four different conditions: Creative collaborative, Non-creative collaborative, Creative non-collaborative and Non-Creative non-collaborative. The children's stories were evaluated according to the four standard creativity variables: fluency, flexibility, elaboration and originality. Results emphasized that children who interacted with a creative robot showed higher creativity during the interaction than children who interacted with a non-creative robot. Nevertheless, no significant effect of the type of the interaction was found on children's creativity skills. Our findings are significant to the Child-Robot interaction (cHRI) community since they enrich the scientific understanding of the development of child-robot encounters for educational applications.",
  "sense": "",
  "interaction": "Creative Storytelling and Social Engagement",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Voice,Visuals,Hybrid,Tangible and Haptic Interaction",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence",
  "application": "Teaching and Education"
},
{
  "id": 56,
  "year": 2024,
  "authors": "Kim, Callie Y.; Lee, Christine P.; Mutlu, Bilge",
  "title": "Understanding large-language model (LLM)-powered human-robot interaction",
  "venue": "Proceedings of the 2024 ACM/IEEE international conference on human-robot interaction",
  "doi": "10.1145/3610977.3634966",
  "url": "https://doi.org/10.1145/3610977.3634966",
  "abstract": "Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.",
  "sense": "Modular Perception and Textual Abstraction,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Creative Storytelling and Social Engagement",
  "alignment": "",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Safety",
  "application": "Other"
},
{
  "id": 57,
  "year": 2025,
  "authors": "Cho, Hyungjun; Nam, Tek-Jin",
  "title": "Living alongside areca: Exploring human experiences with things expressing thoughts and emotions",
  "venue": "Proceedings of the 2025 CHI conference on human factors in computing systems",
  "doi": "10.1145/3706598.3713228",
  "url": "https://doi.org/10.1145/3706598.3713228",
  "abstract": "Technological advancements such as LLMs have enabled everyday things to use language, fostering increased anthropomorphism during interactions. This study employs material speculation to investigate how people experience things that express their thoughts, emotions, and intentions. We utilized Areca, an air purifier capable of keeping a diary, and placed it in the everyday spaces of eight participants over three weeks. Weekly interviews were conducted to capture participants' evolving interactions with Areca, concluding with a session collaboratively speculating on the future of everyday things. Our findings indicate that things expressing thoughts, emotions, and intentions can be perceived as possessing agency beyond mere functionality. While some participants exhibited emotional engagement with Areca over time, responses varied, including moments of detachment. We conclude with design implications for HCI designers, offering insights into how emerging technologies may shape human-thing relationships in complex ways.",
  "sense": "Emotional Grounding",
  "interaction": "Social Initiation",
  "alignment": "",
  "modality": "Text,Motion,Tangible and Haptic Interaction",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Interviews",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience",
  "application": "Domestic and Everyday Use"
},
{
  "id": 58,
  "year": 2025,
  "authors": "Wang, Mengyang; Yu, Keye; Zhang, Yukai; Fan, Mingming",
  "title": "Challenges in adopting companion robots: An exploratory study of robotic companionship conducted with chinese retirees",
  "venue": "Proceedings of the ACM on Human-Computer Interaction",
  "doi": "10.1145/3710943",
  "url": "https://doi.org/10.1145/3710943",
  "abstract": "Companion robots hold immense potential in providing emotional support to older adults in the rapidly aging world. However, questions have been raised regarding whether healthy older adults benefit from having a robotic companion, how they perceive the value of companion robots, and what their relationship with companion robots would be like. To understand healthy older adults' perceptions, attitudes, and relationships toward companion robots, we conducted multiple focus groups with eighteen retirees. Our findings reveal the social context encountered by older adults in China and the mismatch between the current value proposition of companion robots and healthy older adults' needs. We further identify factors that may influence the adoption of robotic companionship, which include individuals' self-disclosure tendencies, quality of companionship, differentiated value, and seamless collaboration with aging-in-community infrastructure and services.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "Sustained Personalization",
  "modality": "Voice,Hybrid,Tangible and Haptic Interaction",
  "morphology": "Desktop Companions",
  "autonomy": "Full Autonomy",
  "studyMethod": "Interviews,Questionnaires",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Safety",
  "application": "Domestic and Everyday Use"
},
{
  "id": 59,
  "year": 2024,
  "authors": "Padmanabha, Akhil; Yuan, Jessie; Gupta, Janavi; Karachiwalla, Zulekha; Majidi, Carmel; Admoni, Henny; Erickson, Zackory",
  "title": "VoicePilot: Harnessing llms as speech interfaces for physically assistive robots",
  "venue": "Proceedings of the 37th annual ACM symposium on user interface software and technology",
  "doi": "10.1145/3654777.3676401",
  "url": "https://doi.org/10.1145/3654777.3676401",
  "abstract": "Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos, code, and supporting files are located on our project website1",
  "sense": "Static and Semi-Static Context Injection,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Voice,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,Usability",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 60,
  "year": 2024,
  "authors": "Blanco, Antonio; Pérez, Gerardo; Condón, Alicia; Rodríguez, Trinidad; Núñez, Pedro",
  "title": "AI-enhanced social robots for older adults care: Evaluating the efficacy of ChatGPT-powered storytelling in the EBO platform",
  "venue": "2024 33rd IEEE international conference on robot and human interactive communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731292",
  "url": "https://ieeexplore.ieee.org/document/10731292",
  "abstract": "The population of older adults is growing worldwide, and with it, there is a need for appropriate and effective cognitive therapies. Social robots have shown potential as therapeutic tools for older adults, providing companionship, entertainment, and therapeutic support. In particular, storytelling activities promote cognitive stimulation, socio-emotional skills, and simple entertainment. However, conventional storytelling methodologies concerning engagement, personalization, and interactivity exhibit a potential for greater diversity. These traditional approaches often require extensive preparation time, which can limit their feasibility and adaptability in diverse settings. Social robots can help overcome these limitations by providing a more interactive and engaging storytelling experience. In this paper, we present an approach to enhance the storytelling capabilities of the EBO robot, a social robot designed for interaction with older adult people. As a novelty, in addition to integrating the proposed system in the CORTEX cognitive architecture, we offer to include the therapist in the loop and integrate Artificial Intelligence techniques, including a Large Language Model, ChatGPT, to enable the robot to generate natural-sounding and engaging narratives, adjust its interactions based on the cognitive impairments, and align with the interests and preferences of the user. We demonstrate the effectiveness of our approach through a case study involving older adult participants and therapists. Our preliminary results show that the EBO robot, integrating AI techniques, can provide an interactive, customizable, and socially aware storytelling experience that promotes cognitive stimulation, socio-emotional skills, and simple entertainment that is engaging and enjoyable for older adults. Our approach could contribute to developing effective and engaging therapies for older adults using social robots as therapeutic tools.",
  "sense": "Modular Perception and Textual Abstraction,Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity,Task-Oriented Planning and Execution,Creative Storytelling and Social Engagement",
  "alignment": "Episodic Memory Integration",
  "modality": "Voice,Visuals,Motion,Hybrid",
  "morphology": "Desktop Companions",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 61,
  "year": 2024,
  "authors": "Salem, Ahmed; Sumi, Kaoru",
  "title": "A comparative human-robot interaction study between face-display and an advanced social robot",
  "venue": "2024 IEEE 48th annual computers, software, and applications conference (COMPSAC)",
  "doi": "10.1109/COMPSAC61105.2024.00090",
  "url": "http://ieeexplore.ieee.org/document/10633431",
  "abstract": "Social robots are taking major roles in our lives through many sectors, ranging from eHealth to industry. However, social robots that can interact with people are expensive. In this study, we evaluate what makes an inexpensive retro-projected Face-Display/robot different from the advanced social robot “Furhat”. How will a retro-projected robot be perceived compared to an advanced social robot? We built a retro-projected robot and compared it with Furhat. Seventy-two participants interacted with both robots in a between-subjects study design. We compare the two robots using different HRI questionnaires. Furthermore, we investigate which robot face is preferred to interact with, a human face (due to social agency) or an anime face (due to Japanese anime culture). In a retro-projected robot, a human face is preferable as it is rated higher in terms of anthropomorphization, trust, competence, and empathy. An anime face was rated higher in warmth, attractiveness, pleasantness, and comfort to see. In Furhat, few differences were noticed between the human and anime faces. A human face was perceived to be more emphatically responsive, and an anime face was rated highly in trustworthiness, warmth, competence, and empathic understanding. Furhat excelled significantly in the aspects of anthropomorphization, likeability, trustworthiness, warmth, competence, empathic responsiveness, attractiveness, and pleasantness to see. On the contrary, the retro-projected robot excelled in inducing social discomfort. Interestingly, both robots were perceived similarly in aspects of perceived intelli-gence and safety, empathic understanding, and comfort to see. An anime face on either robot is significantly more pleasant and is always perceived to be young in age. We deduce that retro-projected robots are inferior to Furhat in most HRI aspects and similar in a few of them. Thus, retro-projected robots can serve effectively in fields where the three aforementioned indifferent aspects are the main requirements in an application. Moreover, whenever aesthetics and visual pleasantness are needed, an anime face is preferred over a human one.",
  "sense": "Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "",
  "modality": "Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Safety",
  "application": "Social and Conversational Systems"
},
{
  "id": 62,
  "year": 2023,
  "authors": "Li, Jiannan; Sousa, Maurı́cio; Mahadevan, Karthik; Wang, Bryan; Aoyagui, Paula Akemi; Yu, Nicole; Yang, Angela; Balakrishnan, Ravin; Tang, Anthony; Grossman, Tovi",
  "title": "Stargazer: An interactive camera robot for capturing how-to videos based on subtle instructor cues",
  "venue": "Proceedings of the 2023 CHI conference on human factors in computing systems",
  "doi": "10.1145/3544548.3580896",
  "url": "https://doi.org/10.1145/3544548.3580896",
  "abstract": "Live and pre-recorded video tutorials are an effective means for teaching physical skills such as cooking or prototyping electronics. A dedicated cameraperson following an instructor's activities can improve production quality. However, instructors who do not have access to a cameraperson's help often have to work within the constraints of static cameras. We present Stargazer, a novel approach for assisting with tutorial content creation with a camera robot that autonomously tracks regions of interest based on instructor actions to capture dynamic shots. Instructors can adjust the camera behaviors of Stargazer with subtle cues, including gestures and speech, allowing them to fluidly integrate camera control commands into instructional activities. Our user study with six instructors, each teaching a distinct skill, showed that participants could create dynamic tutorial videos with a diverse range of subjects, camera framing, and camera angle combinations using Stargazer.",
  "sense": "Task Intent Formulation",
  "interaction": "",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Voice,Visuals,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation,Think-aloud Protocols",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Usability,Cognitive Load and Workload",
  "application": "Teaching and Education"
},
{
  "id": 63,
  "year": 2024,
  "authors": "Wang, Yufei; Zeng, Wenting; Liu, Changzhen; Ye, Zhuohan; Sun, Jiawei; Ji, Junxiang; Jiang, Zhihan; Yan, Xianyi; Wu, Yongyi; Wang, Yigao; Yang, Dingqi; Wang, Leye; Zhang, Daqing; Wang, Cheng; Chen, Longbiao",
  "title": "CrowdBot: An open-environment robot management system for on-campus services",
  "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
  "doi": "10.1145/3659601",
  "url": "https://doi.org/10.1145/3659601",
  "abstract": "In contemporary campus environments, the provision of timely and efficient services is increasingly challenging due to limitations in accessibility and the complexity and openness of the environment. Existing service robots, while operational, often struggle with adaptability and dynamic task management, leading to inefficiencies. To overcome these limitations, we introduce CrowdBot, a robot management system that enhances service in campus environments. Our system leverages a hierarchical reinforcement learning-based cloud-edge hybrid scheduling framework (REDIS), for efficient online streaming task assignment and dynamic action scheduling. To verify the REDIS framework, we have developed a digital twin simulation platform, which integrates large language models and hot-swapping technology. This facilitates seamless human-robot interaction, efficient task allocation, and cost-effective execution through the reuse of robot equipment. Our comprehensive simulations corroborate the system's remarkable efficacy, demonstrating significant improvements with a 24.46% reduction in task completion times, a 9.37% decrease in travel distances, and up to a 3% savings in power usage. Additionally, the system achieves a 7.95% increase in the number of tasks completed and a 9.49% reduction in response time. Real-world case studies further affirm CrowdBot's capability to adeptly execute tasks and judiciously recycle resources, thereby offering a smart and viable solution for the streamlined management of campus services.",
  "sense": "Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Zoomorphic",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments,Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,Usability",
  "application": "Public Spaces Service"
},
{
  "id": 64,
  "year": 2024,
  "authors": "Wang, Chongyang; Zheng, Siqi; Zhong, Lingxiao; Yu, Chun; Liang, Chen; Wang, Yuntao; Gao, Yuan; Lam, Tin Lun; Shi, Yuanchun",
  "title": "PepperPose: Full-body pose estimation with a companion robot",
  "venue": "Proceedings of the 2024 CHI conference on human factors in computing systems",
  "doi": "10.1145/3613904.3642231",
  "url": "https://doi.org/10.1145/3613904.3642231",
  "abstract": "PepperPose is a companion robot system designed for accurate full-body pose estimation across diverse actions in a user-friendly and location-agnostic manner. The robot actively tracks users and refines its viewpoint to enhance pose accuracy in real-world scenarios. An evaluation with 30 participants performing daily and exercise actions in a home-like space demonstrates promising performance and highlights opportunities for human-robot interaction, along with current limitations and future developments.",
  "sense": "Integrated Visual-Language Reasoning",
  "interaction": "",
  "alignment": "",
  "modality": "Visuals,Motion,Hybrid,Proximity",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Interviews,Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience",
  "application": "Domestic and Everyday Use"
},
{
  "id": 65,
  "year": 2025,
  "authors": "Ge, Yate; Li, Meiying; Huang, Xipeng; Hu, Yuanda; Wang, Qi; Sun, Xiaohua; Guo, Weiwei",
  "title": "GenComUI: Exploring generative visual aids as medium to support task-oriented human-robot communication",
  "venue": "Proceedings of the 2025 CHI conference on human factors in computing systems",
  "doi": "10.1145/3706598.3714238",
  "url": "https://doi.org/10.1145/3706598.3714238",
  "abstract": "This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models (LLMs) that dynamically generates contextual visual aids—such as map annotations, path indicators, and animations—to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Task Intent Formulation,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Voice,Visuals,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,Perceived Intelligence,Anthropomorphism,Usability,Safety,Cognitive Load and Workload",
  "application": "Public Spaces Service"
},
{
  "id": 66,
  "year": 2025,
  "authors": "Ho, Hui-Ru; Kargeti, Nitigya; Liu, Ziqi; Mutlu, Bilge",
  "title": "SET-PAiREd: Designing for parental involvement in learning with an AI-assisted educational robot",
  "venue": "Proceedings of the 2025 CHI conference on human factors in computing systems",
  "doi": "10.1145/3706598.3713330",
  "url": "https://doi.org/10.1145/3706598.3713330",
  "abstract": "AI-assisted learning companion robots are increasingly used in early education. Many parents express concerns about content appropriateness, while they also value how AI and robots could supplement their limited skill, time, and energy to support their children's learning. We designed a card-based kit, SET, to systematically capture scenarios that have different extents of parental involvement. We developed a prototype interface, PAiREd, with a learning companion robot to deliver LLM-generated educational content that can be reviewed and revised by parents. Parents can flexibly adjust their involvement in the activity by determining what they want the robot to help with. We conducted an in-home field study involving 20 families with children aged 3-5. Our work contributes to an empirical understanding of the level of support parents with different expectations may need from AI and robots and a prototype that demonstrates an innovative interaction paradigm for flexibly including parents in supporting their children.",
  "sense": "Static and Semi-Static Context Injection,Integrated Visual-Language Reasoning,Human Model Alignment",
  "interaction": "Embodied Social Expressiveness,Task-Oriented Planning and Execution,Creative Storytelling and Social Engagement",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Tangible and Haptic Interaction",
  "morphology": "Desktop Companions",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Interviews,Questionnaires,Co-design Workshops,Think-aloud Protocols",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Usability,Safety,Cognitive Load and Workload",
  "application": "Teaching and Education"
},
{
  "id": 67,
  "year": 2025,
  "authors": "Lai, Yuzhi; Yuan, Shenghai; Nassar, Youssef; Fan, Mingyu; Gopal, Atmaraaj; Yorita, Arihiro; Kubota, Naoyuki; Rätsch, Matthias",
  "title": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application With Voice and Deictic Posture via Large Language Model",
  "venue": "IEEE Robotics & Automation Magazine",
  "doi": "10.1109/MRA.2025.3543957",
  "url": "https://ieeexplore.ieee.org/document/10910098/",
  "abstract": "Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing human‒robot interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly, due to difficulties with complex syntax or sign language. To address the challenge, this article introduces a multimodal interaction framework that combines voice and deictic posture information to create a more natural HRI system. Visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity, using a Universal Robots UR3e manipulator. Our method demonstrates significantly better HRI performance in terms of accuracy and robustness. To benefit the research community and the general public, we made our code and design open source.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Voice,Motion,Hybrid,Proximity",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Field Deployments,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Usability,Safety",
  "application": "Healthcare and Wellbeing"
},
{
  "id": 68,
  "year": 2024,
  "authors": "Zu, Weiqin; Song, Wenbin; Chen, Ruiqing; Guo, Ze; Sun, Fanglei; Tian, Zheng; Pan, Wei; Wang, Jun",
  "title": "Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework",
  "venue": "2024 IEEE International Conference on Robotics and Automation (ICRA)",
  "doi": "10.1109/ICRA57147.2024.10611462",
  "url": "https://ieeexplore.ieee.org/document/10611462/",
  "abstract": "The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Detailed experiments are conducted in both simulation and the real world demonstrating that LIM2N has solid user needs understanding, alongside an enhanced interactive experience.",
  "sense": "Static and Semi-Static Context Injection,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution,Anticipatory Assistance",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence",
  "application": "Domestic and Everyday Use"
},
{
  "id": 69,
  "year": 2024,
  "authors": "Farooq, Muhammad Umar; Kang, Geon; Seo, Jiwon; Bae, Jungchan; Kang, Seoyeon; Jang, Young Jae",
  "title": "DAIM-HRI: A new Human-Robot Integration Technology for Industries",
  "venue": "2024 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO)",
  "doi": "10.1109/ARSO60199.2024.10557811",
  "url": "https://ieeexplore.ieee.org/document/10557811/",
  "abstract": "Robots have become a crucial part of the automation landscape of Industry 4.0, where human-robot interaction plays a pivotal role in task completion on the shop floor. Despite their significance, the integration of robots in small and medium enterprises (SMEs) is hindered by exorbitant costs and technical requirements. New human-robot integration (HRI) technologies can facilitate robot integration and enable non-technical staff at SMEs to control and collaborate with the robots. This work introduces a DAIM-HRI system that leverages a Large Language Model (LLM) and Robot Operating System (ROS) to enable natural language command-based robot control. It is designed to allow non-technical users to operate the robots in their language without any pre-training. By bridging the gap between humans and robots, this technology provides prospects for enhanced collaboration between shop floor workers and robots ultimately improving the efficiency of the manufacturing process.",
  "sense": "Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Text,Voice,Hybrid,Proximity",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Safety",
  "application": "Industrial Manufacturing"
},
{
  "id": 70,
  "year": 2024,
  "authors": "Grassi, Lucrezia; Hong, Zhouyang; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio",
  "title": "Grounding conversational robots on vision through dense captioning and large language models",
  "venue": "2024 IEEE international conference on robotics and automation (ICRA)",
  "doi": "10.1109/ICRA57147.2024.10611232",
  "url": "https://ieeexplore.ieee.org/document/10611232",
  "abstract": "This work explores a novel approach to empowering robots with visual perception capabilities using textual descriptions. Our approach involves the integration of GPT-4 with dense captioning, enabling robots to perceive and interpret the visual world through detailed text-based descriptions. To assess both user experience and the technical feasibility of this approach, experiments were conducted with human participants interacting with a Pepper robot equipped with visual capabilities. The results affirm the viability of the proposed approach, allowing to perform vision-based conversations effectively, despite processing time limitations.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning",
  "interaction": "Social Initiation",
  "alignment": "",
  "modality": "Text,Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Safety",
  "application": "Public Spaces Service"
},
{
  "id": 71,
  "year": 2025,
  "authors": "Tsushima, Yosuke; Yamamoto, Shu; Ravankar, Ankit A; Luces, Jose Victorio Salazar; Hirata, Yasuhisa",
  "title": "Task planning for a factory robot using large language model",
  "venue": "IEEE Robotics and Automation Letters",
  "doi": "DOI: 10.1109/LRA.2025.3531153",
  "url": "https://ieeexplore.ieee.org/document/10844355",
  "abstract": "In recent years, automation has significantly advanced the automobile manufacturing industry. However, many tasks still involve human intervention, so there is a demand for the development of robots to support workers. Additionally, as the human-centric approach, such as Industry 5.0, is gaining attention, it is expected that support robots like these will become necessary in the future. This study aims to develop a system that can support workers by utilizing robots that anyone can easily use and flexibly respond to various tasks. This system adopts a large language model (LLM) for work planning and generates tasks that robots can execute by making bidirectional and interactive suggestions and modifications through natural language dialogue in response to human demands, aiming to improve further productivity and the working environment in automobile manufacturing factories. The proposed system was tested in a simulated factory environment and then the performance was confirmed in an actual factory setting. And it was confirmed that various tasks can be executed by robot through work planning and dialogue with the LLM.",
  "sense": "Static and Semi-Static Context Injection,Task Intent Formulation,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution,Social Initiation",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Text,Visuals,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Field Deployments,Questionnaires,Technical Evaluation,Case Studies,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Usability,Safety",
  "application": "Industrial Manufacturing"
},
{
  "id": 72,
  "year": 2025,
  "authors": "Shen, Jocelyn; Lee, Audrey; Alghowinem, Sharifa; Adkins, River; Breazeal, Cynthia; Park, Hae Won",
  "title": "Social robots as social proxies for fostering connection and empathy towards humanity",
  "venue": "Proceedings of the 2025 ACM/IEEE international conference on human-robot interaction",
  "doi": "10.5555/3721488.3721608",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721608",
  "abstract": "Despite living in an increasingly connected world, social isolation is a prevalent issue today. While social robots have been explored as tools to enhance social connection through companionship, their potential as asynchronous social platforms for fostering connection towards humanity has received less attention. In this work, we introduce the design of a social support companion that facilitates the exchange of emotionally relevant stories and scaffolds reflection to enhance feelings of connection via five design dimensions. We investigate how social robots can serve as \"social proxies\" facilitating human stories, passing stories from other human narrators to the user. To this end, we conduct a real-world deployment of 40 robot stations in users' homes over the course of two weeks. Through thematic analysis of user interviews, we find that social proxy robots can foster connection towards other people's experiences via mechanisms such as identifying connections across stories or offering diverse perspectives. We present design guidelines from our study insights on the use of social robot systems that serve as social platforms to enhance human empathy and connection.",
  "sense": "Emotional Grounding",
  "interaction": "Creative Storytelling and Social Engagement,Social Initiation",
  "alignment": "Sustained Personalization,Episodic Memory Integration",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Desktop Companions",
  "autonomy": "Full Autonomy",
  "studyMethod": "Field Deployments,Interviews,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Safety",
  "application": "Social and Conversational Systems"
},
{
  "id": 73,
  "year": 2025,
  "authors": "Lo, Jia-Hsun; Huang, Han-Pang; Lo, Jie-Shih",
  "title": "LLM-based robot personality simulation and cognitive system",
  "venue": "Scientific Reports",
  "doi": "10.1038/s41598-025-01528-8",
  "url": "https://www.nature.com/articles/s41598-025-01528-8",
  "abstract": "The inherence of personality in human-robot interaction enhances conversational dynamics and user experience. The deployment of Chat GPT-4 within a cognitive robot framework is designed by using state-space realization to emulate specific personality traits, incorporating elements of emotion, motivation, visual attention, and both short-term and long-term memory. The encoding and retrieval of long-term memory are facilitated through document embedding techniques, while emotions are generated based on predictions of future events. This framework processes textual and visual information, responding or initiating actions in accordance with the configured personality settings and cognitive processes. The constancy and effectiveness of the personality simulation have been compared to human baseline and validated via two personality assessments: the International Personality Item Pool - Neuroticism, Extraversion and Openness (IPIP-NEO) and the Big Five personality test. Our proposed personality model of cognitive robot is designed by using Kelly's role construct repertory, Cattell's 16 personality factors and preferences, which are analyzed by construct validity and compared to human subjects. Theory of mind is observed in personality simulation, which perform better second-order of belief compared to other agent on the improved theory of mind dataset (ToMi dataset). Based on the proposed methods, our designed robot, Mobi, is enable to chat based on its own personality, handle social conflicts and understand user's intent. Such simulations can achieve a high degree of human likeness, characterized by conversations that are flexible and imbued with intention.",
  "sense": "Static and Semi-Static Context Injection,Integrated Visual-Language Reasoning,Emotional Grounding,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Task-Oriented Planning and Execution,Anticipatory Assistance",
  "alignment": "Sustained Personalization,Episodic Memory Integration",
  "modality": "Text,Voice,Visuals,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Questionnaires,Technical Evaluation,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,Perceived Intelligence,Anthropomorphism",
  "application": "Social and Conversational Systems"
},
{
  "id": 74,
  "year": 2025,
  "authors": "Grassi, Lucrezia; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio",
  "title": "Strategies for Controlling the Conversation Dynamics in Multi-Party Human-Robot Interaction",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-025-01298-3",
  "url": "https://doi.org/10.1007/s12369-025-01298-3",
  "abstract": "This article tackles the research question of whether it is possible to control conversation dynamics in a multi-party scenario using easily implementable solutions on off-the-shelf robotic platforms. To this end, we expanded upon our previously developed cloud robotic architecture by incorporating policies aimed at managing conversation dynamics through selective addressing of individuals, with the ultimate goal of balancing or unbalancing users' participation or making subgroups of participants interact. Specifically, we computed the dominance of each speaker as a weighted sum of their speaking time and the number of words spoken within a moving window and used the Louvain algorithm to partition speakers into a set of non-overlapping communities. We then implemented six control policies, which were applied by the robot. Two of them, named BH and BS, aim to reduce dominance error (i.e., the difference in dominance between the most and least dominant speakers—both policies give the floor to the less dominant speaker). Two other policies, UH and US, are designed to increase the dominance error (both give the floor to the most dominant speaker). Finally, CH and CS aim to reduce the community error (i.e., the difference between the actual number of detected subgroups among speakers and the ideal target of a single group to which all speakers belong). Policies BH, UH, and CH (with “H” standing for “hard”) do not allow any exceptions to the policy rules, while BS, US, and CS (with “S” for “soft”) permit exceptions. To test the impact of these policies, we conducted a between-subjects study (N = 300) involving middle school students engaging in dialogue with a humanoid robot acting as a moderator. The study compared five conditions: in four of them, the robot used information gathered during the conversation to decide which speaker to address, applying one of the control policies—BH, BS, CH, or CS. The policies UH and US were excluded, as having a robot consistently give the floor to the most dominant child may raise ethical concerns. In the fifth condition, a baseline neutral policy (N) was applied, in which the robot did not explicitly address any speaker. The results imply that a robot using the proper control policies can influence conversation dynamics to keep both dominance error and community error significantly lower than those of a robot using the baseline policy, leading to more balanced participation and a reduction in the number of subgroups. Indeed, statistically significant differences have been found between the five policies considered in the dominance and community errors. However, no statistically significant differences in user experience—as measured by three scales of the validated SASSI questionnaire—were found when the robot used one of the control policies, as compared to the baseline, suggesting that participants are not negatively impacted by the robot's attempt to control the conversation.",
  "sense": "Task Intent Formulation,Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity",
  "alignment": "",
  "modality": "Voice,Proximity",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Cognitive Load and Workload",
  "application": "Teaching and Education"
},
{
  "id": 75,
  "year": 2025,
  "authors": "Herath, Damith; Busby Grant, Janie; Rodriguez, Adrian; Davis, Jenny L.",
  "title": "First impressions of a humanoid social robot with natural language capabilities",
  "venue": "Scientific Reports",
  "doi": "10.1038/s41598-025-04274-z",
  "url": "https://www.nature.com/articles/s41598-025-04274-z",
  "abstract": "Concurrent developments in robotic design and natural language processing (NLP) have enabled the production of humanoid chatbots that can operate in commercial and community settings. Though still novel, the presence of physically embodied social robots is growing and will soon be commonplace. Our study is set at this point of emergence, investigating people's first impressions of a humanoid chatbot in a public venue. Specifically, we introduced “Pepper” to attendees at an innovation festival. Pepper is a humanoid robot outfitted with ChatGPT. Attendees engaged with Pepper in a bounded interaction and provided feedback about their experience (n = 88). Qualitative analyses reveal participants' mixed emotional resonance, reactions to Pepper's embodied form and movements, expectations about interpersonal connection and rituals of interaction, and attentiveness to issues of diversity and social inclusion. Findings document live responses to a humanoid chatbot, highlight the affective, social, and material forces that shape human-robot interaction, and underscore the value of “in the wild” studies, creating space and scope for user-publics to express their perspectives and concerns. Such insights are acutely relevant as we enter this next era of social robotics.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Field Deployments,Questionnaires,Technical Evaluation",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience,Anthropomorphism",
  "application": "Public Spaces Service"
},
{
  "id": 76,
  "year": 2024,
  "authors": "Dell'Anna, Davide; Jamshidnejad, Anahita",
  "title": "SONAR: An Adaptive Control Architecture for Social Norm Aware Robots",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-024-01172-8",
  "url": "https://doi.org/10.1007/s12369-024-01172-8",
  "abstract": "Recent advances in robotics and artificial intelligence have made it necessary or desired for humans to get involved in interactions with social robots. A key factor for the human acceptance of these robots is their awareness of environmental and social norms. In this paper, we introduce SONAR (for SOcial Norm Aware Robots), a novel robot-agnostic control architecture aimed at enabling social agents to autonomously recognize, act upon, and learn over time social norms during interactions with humans. SONAR integrates several state-of-the-art theories and technologies, including the belief-desire-intention (BDI) model of reasoning and decision making for rational agents, fuzzy logic theory, and large language models, to support adaptive and norm-aware autonomous decision making. We demonstrate the feasibility and applicability of SONAR via real-life experiments involving human-robot interactions (HRI) using a Nao robot for scenarios of casual conversations between the robot and each participant. The results of our experiments show that our SONAR implementation can effectively and efficiently be used in HRI to provide the robot with environmental and social and norm awareness. Compared to a robot with no explicit social and norm awareness, introducing social and norm awareness via SONAR results in interactions that are perceived as more positive and enjoyable by humans, as well as in higher perceived trust in the social robot. Moreover, we investigate, via computer-based simulations, the extent to which SONAR can be used to learn and adapt to the social norms of different societies. The results of these simulations illustrate that SONAR can successfully learn adequate behaviors in a society from a relatively small amount of data. We publicly release the source code of SONAR, along with data and experiments logs.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Task Intent Formulation,Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation",
  "alignment": "Repair in Ethical and Normative Alignment",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Proximity",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Case studies,simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Usability,Cognitive Load and Workload",
  "application": "Social and Conversational Systems"
},
{
  "id": 77,
  "year": 2025,
  "authors": "Nardelli, Alice; Maccagni, Giacomo; Minutoli, Federico; Sgorbissa, Antonio; Recchiuto, Carmine",
  "title": "Towards Intuitive Interaction: Cognitive Architecture for Artificial Personality, Emotional Intelligence, and Cognitive Capabilities",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-025-01260-3",
  "url": "https://doi.org/10.1007/s12369-025-01260-3",
  "abstract": "Robotic personality shows significant potential in enhancing Human-Robot Interaction. However, shortcomings at both theoretical and implementation levels lead to stereotypical and fragmented models of artificial personality. To overcome these limitations, this study aims to further validate the Conscientiousness, Extroversion, and Agreeableness (CEA) taxonomy for synthetic personalities based on the corresponding three traits of the Big Five model. In the proposed implementation, the robotic personality, directly inspired by human psychology, not only influences how actions are executed but also impacts the robot's inner hedonic drives and, consequently, the action selection process. Our objective is to introduce a task- and platform-independent framework that drives agent behavior through the interplay between personality dynamics, emotional responses to others, memory encoding, anticipation of future actions, and associated hedonic experiences. To validate the effectiveness of the framework in generating perceivable personality traits and emotionally and cognitively intelligent behaviors, and to explore the impact of embodiment on social interaction, a dual experiment was conducted. This experiment involved participants engaging in dyadic conversations with either a digital human or a robot.",
  "sense": "Emotional Grounding,Human Model Alignment",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness,Anticipatory Assistance",
  "alignment": "Episodic Memory Integration,Emotional Repair in Social Interaction",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Proximity",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Case Studies,Simulations",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Usability",
  "application": "Social and Conversational Systems"
},
{
  "id": 78,
  "year": 2025,
  "authors": "Bassiouny, Abdelrhman; Elsayed, Ahmed H.; Falomir, Zoe; del Pobil, Angel P.",
  "title": "UJI-Butler: A Symbolic/Non-symbolic Robotic System that Learns Through Multi-modal Interaction",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-025-01234-5",
  "url": "https://doi.org/10.1007/s12369-025-01234-5",
  "abstract": "This paper introduces UJI-Butler, an innovative multi-robot framework that blends symbolic and non-symbolic artificial intelligence methods. Unlike previous systems, UJI-Butler integrates large language models (LLMs) with a knowledge base akin to RAG-based systems, while imposing logical reasoning on LLM-generated results. It facilitates multi-modal interaction with human users through speech, sign language, and physical interaction, fostering a human-in-the-loop learning paradigm. By acquiring new knowledge through verbal communication and mastering manipulation skills via human-lead-through programming, UJI-Butler enhances transparency and trust by incorporating human feedback during operations. Experimental results demonstrate that UJI-Butler's combination of symbolic and non-symbolic AI offers intuitive interaction and accelerates the learning process with experience. It adeptly stores and utilizes knowledge gained from verbal communication, recognizing hand gestures for requests. Additionally, UJI-Butler successfully performs user-taught physical skills and generalizes them to varying object sizes and locations. The explicit nature of acquired knowledge enables seamless transferability to other platforms and modification by human users. The code of the whole project is available on Github, in addition, video demonstrations of the UJI-Butler system are available online in a Youtube Playlist.",
  "sense": "Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Task Intent Formulation",
  "interaction": "Task-Oriented Planning and Execution,Social Initiation,Anticipatory Assistance",
  "alignment": "Sustained Personalization,Episodic Memory Integration,Behavioral Repair in Task Execution",
  "modality": "Voice,Visuals,Motion,Tangible and Haptic Interaction",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Technical Evaluation",
  "evaluationMetrics": "Task Accuracy and Performance,Perceived Intelligence,Usability,Safety",
  "application": "Domestic and Everyday Use"
},
{
  "id": 79,
  "year": 2025,
  "authors": "Sakamoto, Yuki; Uchida, Takahisa; Ishiguro, Hiroshi",
  "title": "Effectiveness of Conversational Robots Capable of Estimating and Modeling User Values",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-025-01258-x",
  "url": "https://doi.org/10.1007/s12369-025-01258-x",
  "abstract": "Personalizing a dialogue system according to the user has been recognized to have various positive effects. Despite the significance of user values, concepts guiding choices and evaluations being recognized in communication, they have not been considered in personalized dialogue systems. Therefore, this study constructs a dialogue system that understands user values through conversation. Furthermore, the impact of understanding values on the interactions between dialogue systems and users is examined. The method is organized with a user model of preferences and values based on the established means-end chain model. We used a large language model (LLM) to estimate values based on the users' preferences and the reasons they prefer them. Furthermore, an infinite relational model (IRM) estimates the relationships between multiple elements within the user model. The experiments show that the proposed method could estimate user values and enhance animacy and perceived intelligence in users' impressions of an android robot, prompting new insights into users' own values. The perception of the robot contributes to improved-quality interactions, and new insights into values facilitate a deeper self-understanding of users. This achievement, demonstrating the effects of using values for interaction, can provide valuable insights into human-robot interaction.",
  "sense": "Human Model Alignment",
  "interaction": "Social Initiation",
  "alignment": "",
  "modality": "Text,Voice,Motion",
  "morphology": "Humanoid",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Technical Evaluation,Wizard-of-Oz (WoZ)",
  "evaluationMetrics": "Task Accuracy and Performance,LLM-Specific Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism",
  "application": "Social and Conversational Systems"
},
{
  "id": 80,
  "year": 2024,
  "authors": "Rosén, Julia; Lindblom, Jessica; Lamb, Maurice; Billing, Erik",
  "title": "Previous Experience Matters: An in-Person Investigation of Expectations in Human-Robot Interaction",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-024-01107-3",
  "url": "https://doi.org/10.1007/s12369-024-01107-3",
  "abstract": "The human-robot interaction (HRI) field goes beyond the mere technical aspects of developing robots, often investigating how humans perceive robots. Human perceptions and behavior are determined, in part, by expectations. Given the impact of expectations on behavior, it is important to understand what expectations individuals bring into HRI settings and how those expectations may affect their interactions with the robot over time. For many people, social robots are not a common part of their experiences, thus any expectations they have of social robots are likely shaped by other sources. As a result, individual expectations coming into HRI settings may be highly variable. Although there has been some recent interest in expectations within the field, there is an overall lack of empirical investigation into its impacts on HRI, especially in-person robot interactions. To this end, a within-subject in-person study (N=31) was performed where participants were instructed to engage in open conversation with the social robot Pepper during two 2.5 min sessions. The robot was equipped with a custom dialogue system based on the GPT-3 large language model, allowing autonomous responses to verbal input. Participants' affective changes towards the robot were assessed using three questionnaires, NARS, RAS, commonly used in HRI studies, and Closeness, based on the IOS scale. In addition to the three standard questionnaires, a custom question was administered to capture participants' views on robot capabilities. All measures were collected three times, before the interaction with the robot, after the first interaction with the robot, and after the second interaction with the robot. Results revealed that participants to large degrees stayed with the expectations they had coming into the study, and in contrast to our hypothesis, none of the measured scales moved towards a common mean. Moreover, previous experience with robots was revealed to be a major factor of how participants experienced the robot in the study. These results could be interpreted as implying that expectations of robots are to large degrees decided before interactions with the robot, and that these expectations do not necessarily change as a result of the interaction. Results reveal a strong connection to how expectations are studied in social psychology and human-human interaction, underpinning its relevance for HRI research.",
  "sense": "Static and Semi-Static Context Injection",
  "interaction": "Persona Adaptation and Conversational Fluidity,Embodied Social Expressiveness",
  "alignment": "Episodic Memory Integration",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,User's Perceptual and Relational Experience,Perceived Intelligence,Cognitive Load and Workload",
  "application": "Domestic and Everyday Use"
},
{
  "id": 81,
  "year": 2025,
  "authors": "Kodur, Krishna; Zand, Manizheh; Tognotti, Matthew; Banerjee, Sean; Banerjee, Natasha Kholgade; Kyrarini, Maria",
  "title": "Exploring the Dynamics of Human-Robot Interaction: Robot Error, Sentiment Analysis, and Politeness",
  "venue": "International Journal of Social Robotics",
  "doi": "10.1007/s12369-025-01282-x",
  "url": "https://doi.org/10.1007/s12369-025-01282-x",
  "abstract": "As robots become more integrated into daily life, understanding natural human communication is essential for developing user-friendly human-robot interaction (HRI) systems. While prior research focuses on robot-initiated behaviors, little is known about how user-initiated politeness, sentiment, and communication patterns influence perceptions of robots in real-world tasks. This study addresses this gap by analyzing interactions between 35 participants and Alex, a mobile manipulator that understands unscripted speech, during a collaborative cooking task. The study explores four hypotheses: 1) The average number of words per command used for instructing the robot is affected by how the person perceives the robot's interaction., (2) The robot errors affect the number of words used as well as how the interaction is perceived, (3) The politeness level of the individual expressed during human-robot interaction is affected by the perceived usefulness of the robot, and (4) The sentiment (negative, neutral, or positive) of the individual expressed during human-robot interaction is affected by the perceived ease of use, safety, and trust in their interactions. Kendall's Tau correlation was used for analysis. Results showed a negative correlation between robot errors and the minimum number of words used (τ = − 0.39, p = 0.0027), and lower politeness when users felt tasks were completed successfully (τ = − 0.36, p = 0.0238). Sentiment analysis revealed that slowing speech led to more neutral sentiment (τ = 0.39, p = 0.0027) and reduced positive sentiment (τ = − 0.36, p = 0.0052). These findings highlight how robot errors and user behaviors influence HRI, emphasizing the need for adaptive robots that respond to variations in politeness and sentiment, ultimately enhancing collaboration and user satisfaction.",
  "sense": "Modular Perception and Textual Abstraction",
  "interaction": "",
  "alignment": "Emotional Repair in Social Interaction",
  "modality": "Text,Voice,Motion",
  "morphology": "Functional",
  "autonomy": "Semi-Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,Usability,Safety",
  "application": "Domestic and Everyday Use"
},
{
  "id": 82,
  "year": 2024,
  "authors": "Stark, Carson; Chun, Bohkyung; Charleston, Casey; Ravi, Varsha; Pabon, Luis; Sunkari, Surya; Mohan, Tarun; Stone, Peter; Hart, Justin",
  "title": "Dobby: A Conversational Service Robot Driven by GPT-4",
  "venue": "2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731375",
  "url": "https://ieeexplore.ieee.org/document/10731375/",
  "abstract": "This work introduces a robotics platform which comprehensively integrates multi-step action execution, natural language understanding, and memory to interactively perform service tasks in accordance with variable needs and intentions of users. The proposed architecture is built around an AI agent, derived from GPT-4, which is embedded in an embodied system. Our approach utilizes semantic matching, plan validation, and state messages to ground the agent in the physical world, enabling a seamless merger between communication and behavior. We demonstrate the advantages of this system with an HRI study comparing mobile robots with and without conversational AI capabilities in a free-form tour-guide scenario. The increased adaptability of the system is measured along five dimensions: flexible task planning, interactive exploration of information, emotional-friendliness, personalization, and increased overall user satisfaction.",
  "sense": "Static and Semi-Static Context Injection,Emotional Grounding,Task Intent Formulation",
  "interaction": "Persona Adaptation and Conversational Fluidity,Task-Oriented Planning and Execution,Social Initiation,Anticipatory Assistance",
  "alignment": "Sustained Personalization,Episodic Memory Integration",
  "modality": "Text,Voice,Visuals,Motion,Hybrid,Proximity",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Interviews,Questionnaires",
  "evaluationMetrics": "Task Accuracy and Performance,User's Perceptual and Relational Experience,Perceived Intelligence,Anthropomorphism,Usability",
  "application": "Public Spaces Service"
},
{
  "id": 83,
  "year": 2025,
  "authors": "Malnatsky, Elena; Ligthart, Mike E.U.",
  "title": "Fitting humor: Age-based personalization for shaping relatable child-robot interactions",
  "venue": "Proceedings of the 2025 ACM/IEEE international conference on human-robot interaction",
  "doi": "doi/10.5555/3721488.3721532",
  "url": "https://dl.acm.org/doi/10.5555/3721488.3721532",
  "abstract": "In this paper, we present a participatory design approach to age-based personalization for child-robot interaction. This is an important step towards social robots being effective across age groups. As a testbed for our approach, we used humor. Personalized humor is a powerful social motivator and is uniquely suited to build relatable and sustained child-robot interactions. Through a series of co-design workshops (n = 102 children), we identified humor concepts that fit the specific sense of humor for each of the four age groups (8-9, 9-10, 10-11, 11-12 y.o.), as well as humor concepts that resonated across these age groups. A user study showed that, overall, children found the interaction more amusing and a better fit for both their own sense of humor and that of their peer group when the robot used age-personalized humor compared to age-agnostic humor. The strength of the effects varied by age group, with the oldest group consistently scoring lower on the outcome measures, indicating that the design was not equally effective for all groups.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Creative Storytelling and Social Engagement",
  "alignment": "",
  "modality": "Text,Voice,Motion,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Questionnaires,Co-design workshops,Bodystorming",
  "evaluationMetrics": "LLM-Specific Performance,User's Perceptual and Relational Experience",
  "application": "Teaching and Education"
},
{
  "id": 84,
  "year": 2024,
  "authors": "Grassi, Lucrezia; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio",
  "title": "Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness",
  "venue": "2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)",
  "doi": "10.1109/RO-MAN60168.2024.10731381",
  "url": "https://ieeexplore.ieee.org/document/10731381/",
  "abstract": "This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system's pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system's performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.",
  "sense": "Static and Semi-Static Context Injection,Modular Perception and Textual Abstraction,Integrated Visual-Language Reasoning,Emotional Grounding",
  "interaction": "Persona Adaptation and Conversational Fluidity,Social Initiation",
  "alignment": "Episodic Memory Integration",
  "modality": "Voice,Visuals,Hybrid",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Field Deployments,Technical Evaluation",
  "evaluationMetrics": "Task Efficiency and Timing,LLM-Specific Performance",
  "application": "Social and Conversational Systems"
},
{
  "id": 85,
  "year": 2023,
  "authors": "Wilcock, Graham; Jokinen, Kristiina",
  "title": "To Err Is Robotic; to Earn Trust, Divine: Comparing ChatGPT and Knowledge Graphs for HRI",
  "venue": "2023 32nd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)",
  "doi": "10.1109/RO-MAN57019.2023.10309510",
  "url": "https://ieeexplore.ieee.org/document/10309510",
  "abstract": "This paper discusses two approaches to conversational AI—large language models (LLMs) and knowledge graphs—and compares the types of errors that occur in human-robot interactions for each approach. Example dialogues are provided, and solutions are proposed for error types including false implications, ontological errors, theory of mind errors, and handling of speech recognition errors. The work emphasizes implications for earning user trust.",
  "sense": "",
  "interaction": "Persona Adaptation and Conversational Fluidity,Task-Oriented Planning and Execution",
  "alignment": "",
  "modality": "Text,Voice,Visuals",
  "morphology": "Humanoid",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Technical Evaluation",
  "evaluationMetrics": "LLM-Specific Performance",
  "application": "Public Spaces Service"
},
{
  "id": 86,
  "year": 2024,
  "authors": "Jin, Yixiang; Li, Dingzhe; A, Yong; Shi, Jun; Hao, Peng; Sun, Fuchun; Zhang, Jianwei; Fang, Bin",
  "title": "RobotGPT: Robot Manipulation Learning From ChatGPT",
  "venue": "IEEE Robotics and Automation Letters",
  "doi": "10.1109/LRA.2024.3357432",
  "url": "https://ieeexplore.ieee.org/document/10412086/",
  "abstract": "We present RobotGPT, an innovative decision framework for robotic manipulation that prioritizes stability and safety. The execution code generated by ChatGPT cannot guarantee the stability and safety of the system. ChatGPT may provide different answers for the same task, leading to unpredictability. This instability prevents the direct integration of ChatGPT into the robot manipulation loop. Although setting the temperature to 0 can generate more consistent outputs, it may cause ChatGPT to lose diversity and creativity. Our objective is to leverage ChatGPT's problem-solving capabilities in robot manipulation and train a reliable agent. The framework includes an effective prompt structure and a robust learning model. Additionally, we introduce a metric for measuring task difficulty to evaluate ChatGPT's performance in robot manipulation. Furthermore, we evaluate RobotGPT in both simulation and real-world environments. Compared to directly using ChatGPT to generate code, our framework significantly improves task success rates, with an average increase from 38.5% to 91.5%. Therefore, training a RobotGPT by utilizing ChatGPT as an expert is a more stable approach compared to directly using ChatGPT as a task planner.",
  "sense": "Static and Semi-Static Context Injection,Task Intent Formulation,Human Model Alignment",
  "interaction": "Task-Oriented Planning and Execution",
  "alignment": "Behavioral Repair in Task Execution",
  "modality": "Text,Visuals,Motion,Hybrid",
  "morphology": "Functional",
  "autonomy": "Full Autonomy",
  "studyMethod": "Laboratory Experiment,Questionnaires,Technical Evaluation,Simulation",
  "evaluationMetrics": "Task Efficiency and Timing,Task Accuracy and Performance,LLM-Specific Performance",
  "application": "Domestic and Everyday Use"
}
];
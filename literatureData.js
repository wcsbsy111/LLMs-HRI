const literatureData = [
    { "id": 1, "year": 2025, "authors": "Lima, Maria R.; O'Connell, Amy; Zhou, Feiyang; Nagahara, Alethea; Hulyalkar, Avni; Deshpande, Anura; Thomason, Jesse; Vaidyanathan, Ravi; Matarić, Maja", "title": "Promoting Cognitive Health in Elder Care with Large Language Model-Powered Socially Assistive Robots", "venue": "CHI", "doi": "10.1145/3706598.3713582", "url": "https://dl.acm.org/doi/10.1145/3706598.3713582", "abstract": "As the global population ages, there is increasing need for accessible technologies that promote cognitive health and detect early signs of cognitive decline. This research demonstrates the potential for in-residence monitoring and assessment of cognitive health using large language model (LLM)-powered socially assistive robots (SARs). We conducted a 5-week within-subjects study involving 22 older adults in retirement homes to investigate the feasibility of LLM-powered SARs for promoting and assessing cognitive health. We designed tasks that involved verbal dialogue based on clinically validated cognitive tools. Our findings reveal improved task performance after three robot-administered sessions, with significantly more detailed picture descriptions, fewer word repetitions in semantic fluency, and reduced need for hints. We found that older adults were more socially engaged in robot-administered tasks compared to those administered by a human, and they accepted and were willing to engage with SARs in this context, which had not been tested before.", "morphology": "Functional", "studyMethod": "Demonstration,Interview,Questionnaire", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 2, "year": 2025, "authors": "Zhang, Alex Wuqi; Kovacs, Clark; de Pablo, Liberto; Zhang, Justin; Bai, Maggie; Jeong, Sooyeon; Sebo, Sarah", "title": "Exploring Robot Personality Traits and Their Influence on User Affect and Experience", "venue": "HRI", "doi": "10.5555/3721488.3721606", "url": "https://dl.acm.org/doi/10.5555/3721488.3721606", "abstract": "As human-robot interactions become more social, a robot's personality plays an increasingly vital role in shaping user experience and its overall effectiveness. In this study, we examine the impact of three distinct robot personalities on user experiences during well-being exercises: a Baseline Personality that aligns with user expectations, a High Extraversion Personality, and a High Neuroticism Personality. These personalities were manifested through the robot's dialogue, which were generated using a large language model (LLM) guided by key behavioral characteristics from the Big 5 personality traits. In a between-subjects user study (N = 66), where each participant interacted with one distinct robot personality, we found that both the High Extraversion and High Neuroticism Robot Personalities significantly enhanced participants' emotional states. Our findings highlight the potential benefits of designing robot personalities that deviate from users' expectations, thereby enriching human-robot interactions.", "morphology": "Humanoid", "studyMethod": "Questionnaire", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 3, "year": 2025, "authors": "Spitale, Micol; Axelsson, Minja; Gunes, Hatice", "title": "VITA: A Multi-Modal LLM-Based System for Longitudinal, Autonomous and Adaptive Robotic Mental Well-Being Coaching", "venue": "THRI", "doi": "10.1145/3712265", "url": "https://dl.acm.org/doi/10.1145/3712265", "abstract": "Recently, several works have explored if and how robotic coaches can promote and maintain mental well-being in different settings. However, findings from these studies revealed that these robotic coaches are not ready to be used and deployed in real-world settings due to several limitations that span from technological challenges to coaching success. To overcome these challenges, this article presents VITA, a novel multi-modal LLM-based system that allows robotic coaches to autonomously adapt to the coachee’s multi-modal behaviours (facial valence and speech duration) and deliver coaching exercises in order to promote mental well-being in adults. We identified five objectives that correspond to the challenges in the recent literature, and we show how the VITA system addresses these via experimental validations that include one in-lab pilot study (N = 4) that enabled us to test different robotic coach configurations (pre-scripted, generic and adaptive models) and inform its design for using it in the real world, and one real-world study (N = 17) conducted in a workplace over 4 weeks. Our results show that: (i) coachees perceived the VITA adaptive and generic configurations more positively than the pre-scripted one, and they felt understood and heard by the adaptive robotic coach, (ii) the VITA adaptive robotic coach kept learning successfully by personalising to each coachee over time and did not detect any interaction ruptures during the coaching and (iii) coachees had significant mental well-being improvements via the VITA-based robotic coach practice. The code for the VITA system is openly available via https://github.com/Cambridge-AFAR/VITA-system.", "morphology": "Humanoid", "studyMethod": "Demonstration,Interview,Questionnaire", "evaluation": "Subjective", "application": "Healthcare and Wellbeing" },
    { "id": 4, "year": 2025, "authors": "Hsu, Long-Jing; Swaminathan, Manasi; Khoo, Weslie; Amon, Kyrie Jig; Sato, Hiroki; Dobbala, Sathvika; Tsui, Kate; Crandall, David; Sabanovic, Selma", "title": "Bittersweet Snapshots of Life: Designing to Address Complex Emotions in a Reminiscence Interaction between Older Adults and a Robot", "venue": "CHI", "doi": "10.1145/3706598.3714256", "url": "https://dl.acm.org/doi/10.1145/3706598.3714256", "abstract": "Human-Computer Interaction and Human-Robot Interaction researchers have developed various reminiscence technologies for  older adults, but the focus of such work has mostly been on making  the technology usable and improving older adults’ memory recall.  Our study of a robot facilitating reminiscence through conversations about personal photographs with 20 older adults uncovered a  less discussed aspect of such interactions: reminiscence can evoke  both bitter and sweet emotions. Without adequate emotional sensitivity, the robot sometimes responded inappropriately, requiring  researchers to intervene in the interaction to address misunderstandings. To understand how to better address these challenges,  we conducted a follow-up co-design workshop with 7 older adults  to explore how the robot could better support managing bittersweet  emotions. Through reflexive thematic analysis of the two studies,  this paper identifies factors that trigger bittersweet emotions during  reminiscence with a robot and provides strategies for technology  to manage these emotions during such interactions. This research  highlights the importance of addressing emotional experiences in  the design of reminiscence technology. It also raises ethical concerns about the emotional vulnerability of deploying one-on-one  AI technologies for older adults.", "morphology": "Humanoid", "studyMethod": "Interview,Other Method", "evaluation": "Subjective", "application": "Healthcare and Wellbeing" },
    { "id": 5, "year": 2025, "authors": "Tao, Yiran; Yang, Jehan; Ding, Dan; Erickson, Zackory", "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation", "venue": "HRI", "doi": "10.5555/3721488.3721521", "url": "https://dl.acm.org/doi/10.5555/3721488.3721521", "abstract": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learningbased methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time. The project website with supplementary materials is at https: //lams-assistance.github.io/.", "morphology": "Functional", "studyMethod": "Demonstration,Interview,Questionnaire,Technical Evaluation", "evaluation": "Subjective", "application": "Other Domain" },
    { "id": 6, "year": 2025, "authors": "Banna, Tahsin Tariq; Rahman, Sejuti; Tareq, Mohammad", "title": "Beyond Words: Integrating Personality Traits and Context-Driven Gestures in Human-Robot Interactions", "venue": "AAMAS", "doi": "10.5555/3709347.3743537", "url": "https://dl.acm.org/doi/10.5555/3709347.3743537", "abstract": "As robots become increasingly integrated into human life, personalizing human-robot interactions (HRI) is crucial for improving user acceptance, engagement, and interaction quality. However, personalizing HRI poses a unique challenge due to the diversity of human personality traits. This paper proposes a method that leverages large language models (LLMs) to dynamically tailor robot conversations according to the Big Five (OCEAN) personality traits. Our novelty lies in using user personality traits to shape robots' verbal responses and implementing contextual action generation for gestures. This study addresses two primary research questions: (1) Does adapting robots' verbal responses based on user personality traits improve communication satisfaction? (2) How does the addition of context-appropriate gestures further enhance user satisfaction? We used Goldberg's personality trait measurement scale (1992) to assess 26 participants who engaged in conversations with an LLM-powered Pepper robot on various topics. The quality of these interactions was self-reported using a revised version of Hecht's (1978) conversation satisfaction scale. Three experimental conditions were conducted: (i) Baseline: Standard LLM conversation, (ii) Personality-congruent: LLM-adjusted dialogue based on personality of participants, and (iii) Enhanced interaction: Personality adaptation plus dynamic gestures. For the third condition, we implemented contextually appropriate pre-defined animations and generated novel gestures by computing joint angle values in real time. Statistical analysis using ANOVA revealed significant differences in communication satisfaction across the three conditions (F=13.41, p<.001). Post-hoc analyses using Šidák's multiple comparison test showed significant pairwise differences: Condition 2 vs. 1: ΔΔmean 4.42, p = 0.02; Condition 3 vs. 1: ΔΔmean 8.23, p < 0.01; Condition 3 vs. 2: ΔΔmean 3.80, p = 0.05. These results demonstrate that both personality-congruent interactions and non-verbal gestures significantly enhance communication satisfaction, with the combined approach yielding the highest satisfaction. This approach opens new possibilities for developing socially intelligent robots with applications in healthcare, education, and customer service.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Other Domain" },
    { "id": 7, "year": 2025, "authors": "Abbasi, Nida Itrat; Laban, Guy; Ford, Tamsin; Jones, Peter B.; Gunes, Hatice", "title": "A Longitudinal Study of Child Wellbeing Assessment via Online Interactions with a Social Robot", "venue": "THRI", "doi": "10.1145/3722123", "url": "https://dl.acm.org/doi/10.1145/3722123", "abstract": "Socially Assistive Robots are studied in different child–robot interaction settings. However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing. In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children. The children (N = 40, 20 girls and 20 boys; 8–13 years) interacted with the Nao robot (30–45 mins) over three sessions, at least a week apart. Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety toward the robot, and the robot’s abilities. We divided the participants into three wellbeing clusters (low, med, and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analyzed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants’ gender. Our primary findings suggest that (I) online-mediated interactions with robots can be effective in assessing children’s mental wellbeing over time, and (II) children’s overall perception of the robot either improved or remained consistent across time. Supplementary exploratory analyses have also revealed that the gender of the children affected their wellbeing assessments with interactions effectively distinguishing between varying levels of wellbeing for both boys and girls for the first session and only for boys during the second session. The analyses have also revealed that girls have a higher opinion of the robot as a confidante as compared with boys. Findings from this work affirm the potential of using online-mediated interactions with robots for the assessment of the mental wellbeing of children.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Questionnaire, Other Method", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 8, "year": 2024, "authors": "Wester, Joel; Moghe, Bhakti; Winkle, Katie; van Berkel, Niels", "title": "Facing LLMs: Robot Communication Styles in Mediating Health Information between Parents and Young Adults", "venue": "Proceedings of the ACM on Human-Computer Interaction", "doi": "10.1145/3687036", "url": "https://dl.acm.org/doi/10.1145/3687036", "abstract": "Young adults may feel embarrassed when disclosing sensitive information to their parents, while parents might similarly avoid sharing sensitive aspects of their lives with their children. How to design interactive interventions that are sensitive to the needs of both younger and older family members in mediating sensitive information remains an open question. In this paper, we explore the integration of large language models (LLMs) with social robots. Specifically, we use GPT-4 to adapt different Robot Communication Styles (RCS) for a social robot mediator designed to elicit self-disclosure and mediate health information between parents and young adults living apart. We design and compare four literature-informed RCS: three LLM-adapted (Humorous, Self-deprecating, and Persuasive) and one manually created (Human-scripted), and assess participant perceptions of Likeability, Usefulness, Helpfulness, Relatedness, and Interpersonal Closeness. Through an online experiment with 183 participants, we assess the RCS across two groups: adults with children (Parents) and young adults without children (Young Adults). Our results indicate that both Parents and Young Adults favoured the Human-scripted and Self-deprecating RCS as compared to the other two RCS. The Self-deprecating RCS furthermore led to increased relatedness as compared to the Humorous RCS. Our qualitative findings reveal challenges people have in disclosing health information to family members, and who normally assumes the role of family facilitator-two areas in which social robots can play a key role. The findings offer insights for integrating LLMs with social robots in health-mediation and other contexts involving the sharing of sensitive information.", "morphology": "Other Morphology", "studyMethod": "Demonstration, Questionnaire", "evaluation": "Subjective", "application": "Healthcare and Wellbeing" },
    { "id": 9, "year": 2025, "authors": "Pineda, Kaitlynn Taylor; Brown, Ethan; Huang, Chien-Ming", "title": "\"See You Later, Alligator\": Impacts of Robot Small Talk on Task, Rapport, and Interaction Dynamics in Human-Robot Collaboration", "venue": "HRI", "doi": "10.5555/3721488.3721589", "url": "https://dl.acm.org/doi/10.5555/3721488.3721589", "abstract": "Small talk can foster rapport building in human-human teamwork; yet how non-anthropomorphic robots, such as collaborative manipulators commonly used in industry, may capitalize on these social communications remains unclear. This work investigates how robot-initiated small talk influences task performance, rapport, and interaction dynamics in human-robot collaboration. We developed an autonomous robot system that assists a human in an assembly task while initiating and engaging in small talk. A user study (N = 58) was conducted in which participants worked with either a functional robot, which engaged in only task-oriented speech, or a social robot, which also initiated small talk. Our study found that participants in the social condition reported significantly higher levels of rapport with the robot. Moreover, all participants in the social condition responded to the robot's small talk attempts; 59% initiated questions to the robot, and 73% engaged in lingering conversations after requesting the final task item. Although active working times were similar across conditions, participants in the social condition recorded longer task durations than those in the functional condition. We discuss the design and implications of robot small talk in shaping human-robot collaboration.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Industrial manufacturing" },
    { "id": 10, "year": 2025, "authors": "Reimann, Merle M.; Hindriks, Koen V.; Kunneman, Florian A.; Oertel, Catharine; Skantze, Gabriel; Leite, Iolanda", "title": "What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations", "venue": "HRI", "doi": "10.5555/3721488.3721576", "url": "https://dl.acm.org/doi/10.5555/3721488.3721576", "abstract": "When encountering a robot in the wild, it is not inherently clear to human users what the robot's capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user's rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who had three speech-based interactions with a social robot in a restaurant setting. Our results suggest that users preferred the robot communicating its capabilities proactively and adjusted their behavior in those interactions, using a more conversational interaction style while also enjoying the interaction more.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Public space service" },
    { "id": 11, "year": 2025, "authors": "Kamelabad, Alireza M.; Inoue, Elin; Skantze, Gabriel", "title": "Comparing Monolingual and Bilingual Social Robots as Conversational Practice Companions in Language Learning", "venue": "HRI", "doi": "10.5555/3721488.3721590", "url": "https://dl.acm.org/doi/10.5555/3721488.3721590", "abstract": "This study explores the impact of monolingual and bilingual robots in Robot-Assisted Language Learning (RALL) for non-native Swedish learners. In a within-group design, 47 participants interacted with a social robot under two conditions: a monolingual robot that communicated exclusively in Swedish and a bilingual robot capable of switching between Swedish and English. Each participant engaged in multiple role-play scenarios designed to match their language proficiency levels, and their experiences were assessed through surveys and behavioral data. The results show that the bilingual robot was generally favored by participants, leading to a more relaxed, enjoyable experience. The perceived learning was improved at the end of the experiment regardless of the condition. These findings suggest that incorporating bilingual support in language-learning robots may enhance user engagement and effectiveness, particularly for lower-proficiency learners.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Subjective", "application": "Teaching and education" },
    { "id": 12, "year": 2025, "authors": "Ali, Safinah; Abodayeh, Ayat; Dhuliawala, Zahra; Breazeal, Cynthia; Park, Hae Won", "title": "Towards Inclusive Co-creative Child-robot Interaction: Can Social Robots Support Neurodivergent Children's Creativity?", "venue": "HRI", "doi": "10.5555/3721488.3721531", "url": "https://dl.acm.org/doi/10.5555/3721488.3721531", "abstract": "This research designs and applies inclusive child-robot interactions for collaborative creativity, where elementary school children and a social robot collaboratively create and publish picture stories. The robot offers creativity scaffolding during parts of the creative process of storytelling through social interactions such as feedback, question asking, divergent thinking, and positive reinforcement. The collaborative tasks and robot interactions are personalized for neurodivergent children's unique needs. Through a five-session user study with 32 children (ages 5--9) over 8 months, we investigate the impact of the social robot on children's exhibited creativity in storytelling over time, their creative interactions with the robot, and their perceptions of the robot as a creative collaborator. Our research revealed that inclusive design practices eliminated creative barriers for children with neurodevelopmental disorders. The robot's creativity scaffolding interactions positively influenced children's verbal creativity in storytelling, and had an influence on their storytelling creative process. After multiple sessions interacting with the robot, we observed the emergence of diverse creator styles among neurodivergent learners. We propose Inclusive Co-creative Child-robot Interaction (ICCRI) guidelines for fostering creativity in children, and accommodating diverse creator styles in complex, open-ended creative tasks.", "morphology": "Other Morphology", "studyMethod": "Laboratory Experiment, Field Deployment, Questionnaire", "evaluation": "Both", "application": "Teaching and education" },
    { "id": 13, "year": 2025, "authors": "Panicker, Aswati; Chung, Chia-Fang; Šabanović, Selma", "title": "Haru in the Kitchen: Investigating Family Members’ Perceptions Toward a Social Robot Mediator of Food Experiences", "venue": "DIS", "doi": "10.1145/3715336.3735818", "url": "https://dl.acm.org/doi/10.1145/3715336.3735818", "abstract": "When families live together, they often share meals, and food plays a central part in their everyday routines and rituals. When this changes and families are separated by distance, they may transition these practices to technology-mediated ones. Social robots have shown effectiveness in facilitating human-to-human interactions in various communication contexts. In this study, we explore the possibility of distant families interacting through a social robot mediator in the kitchen. We conducted 9 scenario-based interviews using the Haru social robot as a probe. Our findings highlight opportunities for food-related mediation and participants’ hesitations and concerns. We discuss how future research can address these issues, particularly in terms of how a social robot can be positioned in the family and food space, how the robot can be customized for the family’s values, and how the robot can serve as a mediator during opportune contexts (e.g., playfulness) and moments (e.g., culturally synchronous practices).", "morphology": "Other Morphology", "studyMethod": "Demonstration, Interview", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 14, "year": 2025, "authors": "Ito, Shunichiro; Kochigami, Kanae; Kanda, Takayuki", "title": "A Robot Dynamically Asking Questions in University Classes", "venue": "HRI", "doi": "10.5555/3721488.3721591", "url": "https://dl.acm.org/doi/10.5555/3721488.3721591", "abstract": "We developed a robot that dynamically asks questions after listening to a lecture in university group classes to enhance students' learning. Based on interview results with ten professors, we designed our robot to let professors choose when a robot asks questions, ask various kinds of questions by letting professors choose question types, and ask questions dynamically without sharing them with professors in advance. The prepared question types include such as clarification questions, which are fact-based questions about a lecture, and thought-provoking questions, which activate students' critical thinking. We expanded them into eight sub-categories of questions that the most common types that are likely to emerge in classrooms. Leveraging large language models (LLMs), based on lecture transcripts, we developed a robot system that generates questions in the eight sub-categories and yields 0.93 classification accuracy toward human coding results. We also conducted a case study with our developed robot in four university lectures. In these lectures that involved our robot, the professors often asked reasoning questions and criticism questions, and the students showed engagement with the robot-professor dialogues. Interviews with 20 students revealed that the robot's questions contributed to the classes by helping the students reflect on the lectures and gain new perspectives. The professors also recognized some benefits of the robot, perceiving its presence as a question facilitator, a mood maker, a communication tool as well as a motivator to prepare more diligently for their own lectures.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Interview, Technical Evaluation", "evaluation": "Both", "application": "Teaching and education" },
    { "id": 15, "year": 2024, "authors": "Karli, Ulas Berk; Chen, Juo-Tung; Antony, Victor Nikhil; Huang, Chien-Ming", "title": "Alchemist: LLM-Aided End-User Development of Robot Applications", "venue": "HRI", "doi": "10.1145/3610977.3634969", "url": "https://dl.acm.org/doi/10.1145/3610977.3634969", "abstract": "Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming---moving from the conventional process of user specifying programming logic to an iterative, collaborative process in which the user specifies desired program outcomes while LLM produces detailed specifications. We introduce a novel integrated development system, Alchemist, that leverages LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, aiming to reduce the required knowledge for developing robot applications. We present a detailed examination of our system design and provide an exploratory study involving true end-users to assess capabilities, usability, and limitations of our system. Through the design, development, and evaluation of our system, we derive a set of lessons learned from the use of LLMs in robot programming. We discuss how LLMs may be the next frontier for democratizing end-user development of robot applications.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Both", "application": "Industrial manufacturing" },
    { "id": 16, "year": 2025, "authors": "Wang, Chenyang; Tozadore, Daniel; Bruno, Barbara; Dillenbourg, Pierre", "title": "The Child-Robot Relational Norm Intervention to Promote Correct Handwriting Posture for Children", "venue": "HRI", "doi": "10.5555/3721488.3721534", "url": "https://dl.acm.org/doi/10.5555/3721488.3721534", "abstract": "Persuasive social robots have the ability to influence human behaviour through social interaction, which makes them a valuable technological solution for all applications aiming to support a person's behaviour change. The recently proposed Child-Robot Relational Norm Intervention (CRNI) model introduces a new approach for persuasive social robotics, leveraging children's reluctance to disturb robots to promote behaviour change. Unlike traditional methods that rely on direct feedback or reminders, CRNI encourages children to self-monitor and self-correct improper behaviour, by making the robot express mild distress whenever the child exhibits the incorrect behaviour. This paper proposes the first implementation of the CRNI approach in a real HRI context and evaluates its effectiveness in improving children's handwriting posture. The evaluation includes two user studies: (i) a multi-session study with five children investigating the long-term impact of the approach, (ii) a controlled study with 29 children comparing CRNI to direct robot reminders. The results indicate that the CRNI model leads to more sustained posture correction compared to direct interventions. More broadly, our findings suggest that relational norm-based approaches can offer an effective yet less intrusive method for fostering positive behaviours in children.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Interview, Questionnaire", "evaluation": "Both", "application": "Teaching and education" },
    { "id": 17, "year": 2025, "authors": "Ikeda, Bryce; Gramopadhye, Maitrey; Nekervis, LillyAnn; Szafir, Daniel", "title": "MARCER: Multimodal Augmented Reality for Composing and Executing Robot Tasks", "venue": "HRI", "doi": "10.5555/3721488.3721555", "url": "https://dl.acm.org/doi/10.5555/3721488.3721555", "abstract": "In this work, we combine the strengths of humans and robots by developing MARCER, a novel interactive and multimodal end-user robot programming system. MARCER utilizes a Large Language Model to translate users' natural language task descriptions and environmental context into Action Plans for robot execution, based on a trigger-action programming paradigm that facilitates authoring reactive robot behaviors. MARCER also affords interaction via augmented reality to help users parameterize and validate robot programs and provide real-time, visual previews and feedback directly in the context of the robot's operating environment. We present the design, implementation, and evaluation of MARCER to explore the usability of such systems and demonstrate how trigger-action programming, Large Language Models, and augmented reality hold deep-seated synergies that, when combined, empower users to program general-purpose robots to perform everyday tasks.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 18, "year": 2025, "authors": "Xu, Michael F.; Mutlu, Bilge", "title": "Exploring the Use of Robots for Diary Studies", "venue": "HRI", "doi": "10.5555/3721488.3721513", "url": "https://dl.acm.org/doi/10.5555/3721488.3721513", "abstract": "As interest in studying in-the-wild human-robot interaction grows, there is a need for methods to collect data over time and in naturalistic or potentially private environments. HRI researchers have increasingly used the diary method for these studies, asking study participants to self-administer a structured data collection instrument, i.e., a diary, over a period of time. Although the diary method offers a unique window into settings that researchers may not have access to, they also lack the interactivity and probing that interview-based methods offer. In this paper, we explore a novel data collection method in which a robot plays the role of an interactive diary. We developed the Diary Robot system and performed in-home deployments for a week to evaluate the feasibility and effectiveness of this approach. Using traditional text-based and audio-based diaries as benchmarks, we found that robots are able to effectively elicit the intended information. We reflect on our findings, and describe scenarios where the utilization of robots in diary studies as a data collection instrument may be especially applicable.", "morphology": "Other Morphology", "studyMethod": "Field Deployment, Interview, Questionnaire", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 19, "year": 2025, "authors": "Leusmann, Jan; Belardinelli, Anna; Haliburton, Luke; Hasler, Stephan; Schmidt, Albrecht; Mayer, Sven; Gienger, Michael; Wang, Chao", "title": "Investigating LLM-Driven Curiosity in Human-Robot Interaction", "venue": "CHI", "doi": "10.1145/3706598.3713923", "url": "https://dl.acm.org/doi/10.1145/3706598.3713923", "abstract": "Integrating curious behavior traits into robots is essential for them to learn and adapt to new tasks over their lifetime and to enhance human-robot interaction. However, the effects of robots expressing curiosity on user perception, user interaction, and user experience in collaborative tasks are unclear. In this work, we present a Multimodal Large Language Model-based system that equips a robot with non-verbal and verbal curiosity traits. We conducted a user study (N = 20) to investigate how these traits modulate the robot’s behavior and the users’ impressions of sociability and quality of interaction. Participants prepared cocktails or pizzas with a robot, which was either curious or non-curious. Our results show that we could create user-centric curiosity, which users perceived as more human-like, inquisitive, and autonomous while resulting in a longer interaction time. We contribute a set of design recommendations allowing system designers to take advantage of curiosity in collaborative tasks.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 20, "year": 2025, "authors": "Zhang, Alex Wuqi; Queiroz, Rafael; Sebo, Sarah", "title": "Balancing User Control and Perceived Robot Social Agency through the Design of End-User Robot Programming Interfaces", "venue": "HRI", "doi": "10.5555/3721488.3721598", "url": "https://dl.acm.org/doi/10.5555/3721488.3721598", "abstract": "Perceived social agency—the perception of a robot as an autonomous and intelligent social other—is important for fostering meaningful and engaging human-robot interactions. While end-user programming (EUP) enables users to customize robot behavior, enhancing usability and acceptance, it can also potentially undermine the robot's perceived social agency. This study explores the trade-offs between user control over robot behavior and preserving the robot's perceived social agency, and how these factors jointly impact user experience. We conducted a between-subjects study (N = 57) where participants customized the robot's behavior using either a High-Granularity Interface with detailed block-based programming, a Low-Granularity Interface with broader input-form customizations, or no EUP at all. Results show that while both EUP interfaces improved alignment with user preferences, the Low-Granularity Interface better preserved the robot's perceived social agency and led to a more engaging interaction. These findings highlight the need to balance user control with perceived social agency, suggesting that moderate customization without excessive granularity may enhance the overall satisfaction and acceptance of robot products.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire, Other Method", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 21, "year": 2025, "authors": "Antony, Victor Nikhil; Stiber, Maia; Huang, Chien-Ming", "title": "Xpress: A System For Dynamic, Context-Aware Robot Facial Expressions using Language Models", "venue": "HRI", "doi": "10.5555/3721488.3721605", "url": "https://dl.acm.org/doi/10.5555/3721488.3721605", "abstract": "Facial expressions are vital in human communication and significantly influence outcomes in human-robot interaction (HRI), such as likeability, trust, and companionship. However, current methods for generating robotic facial expressions are often labor-intensive, lack adaptability across contexts and platforms, and have limited expressive ranges—leading to repetitive behaviors that reduce interaction quality, particularly in long-term scenarios. We introduce Xpress, a system that leverages language models (LMs) to dynamically generate context-aware facial expressions for robots through a three-phase process: encoding temporal flow, conditioning expressions on context, and generating facial expression code. We demonstrated Xpress as a proof-of-concept through two user studies (n=15x2) and a case study with children and parents (n=13), in storytelling and conversational scenarios to assess the system's context-awareness, expressiveness, and dynamism. Results demonstrate Xpress's ability to dynamically produce expressive and contextually appropriate facial expressions, highlighting its versatility and potential in HRI applications.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Interview, Questionnaire, Technical Evaluation", "evaluation": "Subjective", "application": "Social and conversational systems" },
    { "id": 22, "year": 2024, "authors": "Stampf, Annika; Colley, Mark; Girst, Bettina; Rukzio, Enrico", "title": "Exploring Passenger-Automated Vehicle Negotiation Utilizing Large Language Models for Natural Interaction", "venue": "AutomotiveUI", "doi": "10.1145/3640792.3675725", "url": "https://dl.acm.org/doi/10.1145/3640792.3675725", "abstract": "As vehicle automation advances to SAE Levels 3 to 5, transitioning driving control from human to system, ensuring automated vehicles (AVs) align with user preferences becomes a challenge. Natural interaction emerges as a common goal, offering ways to convey user interests in a user-friendly manner. However, technical, legal, or design constraints may prevent fulfilling these preferences, leading to potential conflicts. Through an online survey (N=50), potential driver-passenger conflicts and their handling strategies were explored. Subsequently, in a Virtual Reality study (N=14), we applied identified strategies (ranging from distracting to motivating and adhering to social norms) to user-AV interactions using a state-of-the-art language model (GPT-4 Turbo) primed with the strategies to simulate realistic dialogues. Additionally, adaptive communication was compared to non-adaptive communication. Our findings reveal a preference for adaptive communication. Yet, despite using advanced modeling, accurately predicting user interactions remained challenging, with users often trying to outsmart the AI.", "morphology": "Other Morphology", "studyMethod": "Other Method", "evaluation": "Subjective", "application": "AR/VR-enabled Interaction" },
    { "id": 23, "year": 2025, "authors": "Pan, Ziqi; Zhang, Xiucheng; Li, Zisu; Peng, Zhenhui; Fan, Mingming; Ma, Xiaojuan", "title": "ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts", "venue": "CHI", "doi": "10.1145/3706598.3713791", "url": "https://dl.acm.org/doi/10.1145/3706598.3713791", "abstract": "Intelligent agents coexisting with humans often need to interact with human-shared objects in environments. Thus, agents should plan their interactions based on objects’ affordances and the current situation to achieve acceptable outcomes. How to support intelligent agents’ planning of affordance-based interactions compatible with human perception and values in real-world contexts remains under-explored. We conducted a formative study identifying the physical, intrapersonal, and interpersonal contexts that count to household human-agent interaction. We then proposed ACKnowledge, a computational framework integrating a dynamic knowledge graph, a large language model, and a vision language model for affordance-based interaction planning in dynamic human environments. In evaluations, ACKnowledge generated acceptable planning results with an understandable process. In real-world simulation tasks, ACKnowledge achieved a high execution success rate and overall acceptability, significantly enhancing usage-rights respectfulness and social appropriateness over baselines. The case study’s feedback demonstrated ACKnowledge’s negotiation and personalization capabilities toward an understandable planning process.", "morphology": "Other Morphology", "studyMethod": "Demonstration, Interview, Questionnaire, Technical Evaluation, Other Method", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 24, "year": 2024, "authors": "Arjmand, Mehdi; Nouraei, Farnaz; Steenstra, Ian; Bickmore, Timothy", "title": "Empathic Grounding: Explorations using Multimodal Interaction and Large Language Models with Conversational Agents", "venue": "IVA", "doi": "10.1145/3652988.3673949", "url": "https://dl.acm.org/doi/10.1145/3652988.3673949", "abstract": "We introduce the concept of “empathic grounding” in conversational agents as an extension of Clark’s conceptualization of grounding in conversation in which the grounding criterion includes listener empathy for the speaker’s affective state. Empathic grounding is generally required whenever the speaker’s emotions are foregrounded and can make the grounding process more efficient and reliable by communicating both propositional and affective understanding. Both speaker expressions of affect and listener empathic grounding can be multimodal, including facial expressions and other nonverbal displays. Thus, models of empathic grounding for embodied agents should be multimodal to facilitate natural and efficient communication. We describe a multimodal model that takes as input user speech and facial expression to generate multimodal grounding moves for a listening agent using a large language model. We also describe a testbed to evaluate approaches to empathic grounding, in which a humanoid robot interviews a user about a past episode of pain and then has the user rate their perception of the robot’s empathy. We compare our proposed model to one that only generates non-affective grounding cues in a between-subjects experiment. Findings demonstrate that empathic grounding increases user perceptions of empathy, understanding, emotional intelligence, and trust. Our work highlights the role of emotion awareness and multimodality in generating appropriate grounding moves for conversational agents.", "morphology": "Humanoid", "studyMethod": "Other Method", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 25, "year": 2024, "authors": "Pereira, Andre; Marcinek, Lubos; Miniota, Jura; Thunberg, Sofia; Lagerstedt, Erik; Gustafson, Joakim; Skantze, Gabriel; Irfan, Bahar", "title": "Multimodal User Enjoyment Detection in Human-Robot Conversation: The Power of Large Language Models", "venue": "ICMI", "doi": "10.1145/3678957.3685729", "url": "https://dl.acm.org/doi/10.1145/3678957.3685729", "abstract": "Enjoyment is a crucial yet complex indicator of positive user experience in Human-Robot Interaction (HRI). While manual enjoyment annotation is feasible, developing reliable automatic detection methods remains a challenge. This paper investigates a multimodal approach to automatic enjoyment annotation for HRI conversations, leveraging large language models (LLMs), visual, audio, and temporal cues. Our findings demonstrate that both text-only and multimodal LLMs with carefully designed prompts can achieve performance comparable to human annotators in detecting user enjoyment. Furthermore, results reveal a stronger alignment between LLM-based annotations and user self-reports of enjoyment compared to human annotators. While multimodal supervised learning techniques did not improve all of our performance metrics, they could successfully replicate human annotators and highlighted the importance of visual and audio cues in detecting subtle shifts in enjoyment. This research demonstrates the potential of LLMs for real-time enjoyment detection, paving the way for adaptive companion robots that can dynamically enhance user experiences.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 26, "year": 2025, "authors": "Hu, Yaxin; Zhu, Anjun; Toma, Catalina L.; Mutlu, Bilge", "title": "Designing Telepresence Robots to Support Place Attachment", "venue": "HRI", "doi": "10.5555/3721488.3721522", "url": "https://dl.acm.org/doi/10.5555/3721488.3721522", "abstract": "People feel attached to places that are meaningful to them, which psychological research calls 'place attachment.' Place attachment is associated with self-identity, self-continuity, and psychological well-being. Even small cues, including videos, images, sounds, and scents, can facilitate feelings of connection and belonging to a place. Telepresence robots that allow people to see, hear, and interact with a remote place have the potential to establish and maintain a connection with places and support place attachment. In this paper, we explore the design space of robotic telepresence to promote place attachment, including how users might be guided in a remote place and whether they experience the environment individually or with others. We prototyped a telepresence robot that allows one or more remote users to visit a place and be guided by a local human guide or a conversational agent. Participants were 38 university alumni who visited their alma mater via the telepresence robot. Our findings uncovered four distinct user personas in the remote experience and highlighted the need for social participation to enhance place attachment. We generated design implications for future telepresence robot design to support people's connections with places of personal significance.", "morphology": "Functional", "studyMethod": "Field Deployment, Other Method", "evaluation": "Both", "application": "Public space service" },
    { "id": 27, "year": 2025, "authors": "Goubard, Cedric; Demiris, Yiannis", "title": "Cognitive Modelling of Visual Attention Captures Trust Dynamics in Human-Robot Collaboration", "venue": "THRI", "doi": "10.1145/3732795", "url": "https://dl.acm.org/doi/10.1145/3732795", "abstract": "Understanding how humans perceive and interact with robots is crucial for collaborative scenarios. Trust, a pivotal factor in such interactions, is inherently volatile and subjective, posing significant challenges for robots. However, trust has also been shown to influence specific human bio-signals and behaviours, suggesting that it could be inferred from those indicators. One such indicator is visual attention, the cognitive process of focusing on distinct environmental elements, often manifested through eye gaze. Despite recent research connecting eye gaze and trust in Human-Robot Collaboration scenarios, this relationship remains largely unexplored. This paper presents a novel signal, the Attention Arbitration Ratio (AAR), which is shown to be a promising real-time predictor of subjective and objective trust measures. We obtain this signal using a visual attention modelling framework that explicitly emulates the Bottom-Up and Top-Down processes, two key cognitive components. We demonstrate the connection between the AAR and trust using Bayesian data analysis, and we analyse the sensitivity of that connection with different visual attention models. For evaluation purposes, we collected gaze data and trust questionnaires from 49 interactions where 29 participants engaged in a collaborative assistive cooking task with a robot, for a total duration of 24h53 of data collection.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Objective", "application": "Domestic and everyday use" },
    { "id": 28, "year": 2024, "authors": "Axelsson, Minja; Spitale, Micol; Gunes, Hatice", "title": "\"Oh, Sorry, I Think I Interrupted You\": Designing Repair Strategies for Robotic Longitudinal Well-being Coaching", "venue": "HRI", "doi": "10.1145/3610977.3634948", "url": "https://dl.acm.org/doi/10.1145/3610977.3634948", "abstract": "Robotic well-being coaches have been shown to successfully promote people's mental well-being. To provide successful coaching, a robotic coach should have the capability to repair the mistakes it makes. Past investigations of robot mistakes are limited to game or task-based, one-off and in-lab studies. This paper presents a 4-phase design process to design repair strategies for robotic longitudinal well-being coaching with the involvement of real-world stakeholders: 1) designing repair strategies with a professional well-being coach; 2) a longitudinal study with the involvement of experienced users (i.e., who had already interacted with a robotic coach) to investigate the repair strategies defined in (1); 3) a design workshop with users from the study in (2) to gather their perspectives on the robotic coach's repair strategies; 4) discussing the results obtained in (2) and (3) with the mental well-being professional to reflect on how to design repair strategies for robotic coaching. Our results show that users have different expectations for a robotic coach than a human coach, which influences how repair strategies should be designed. We show that different repair strategies (e.g., apologizing, explaining, or repairing empathically) are appropriate in different scenarios, and that preferences for repair strategies change during longitudinal interactions with the robotic coach.", "morphology": "Humanoid", "studyMethod": "Interview, Questionnaire, Other Method", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 29, "year": 2025, "authors": "Hsu, Long-Jing; Bays, Janice; Swaminathan, Manasi; Khoo, Weslie; Sato, Hiroki; Amon, Kyrie Jig; Dobbala, Sathvika; Thant, Min Min; Foster, Alex; Tsui, Kate; Stafford, Philip B.; Crandall, David; Sabanovic, Selma", "title": "Research as Care: A Reflection on Incorporating the Ethics of Care in Design Research with People Living with Dementia", "venue": "DIS", "doi": "10.1145/3715336.3735678", "url": "https://dl.acm.org/doi/10.1145/3715336.3735678", "abstract": "When computing researchers design technologies for vulnerable populations and engage with them over extended periods, researchers may incorporate 'care'—deliberate actions to build and maintain relationships with participants—to improve engagement and deepen their understanding of situated perspectives. However, when researchers choose to take actions involving care, these efforts are rarely made explicit. Reflecting on our three-year project of designing and testing a social robot with 31 participants living with dementia, we realized the benefit of intentional reflection on the ethics and practice of care during the research process. We offer 'research as care' guidelines into computing design research: 1) viewing participants as individuals, 2) being intentional in the ongoing and dynamic engagement, 3) acknowledging the reciprocity inherent in care, 4) reporting care practices transparently, 5) tailoring care to the specific context, and 6) making an informed choice to incorporate care. By incorporating research as care, computing design researchers can provide a more productive experience for participants and enhance their designs’ overall quality and validity.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Interview, Other Method", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 30, "year": 2023, "authors": "Cho, Hyungjun; Lee, Jiyeon; Ku, Bonhee; Jeong, Yunwoo; Yadgarova, Shakhnozakhon; Nam, Tek-Jin", "title": "ARECA: A Design Speculation on Everyday Products Having Minds", "venue": "DIS", "doi": "10.1145/3563657.3596002", "url": "https://dl.acm.org/doi/10.1145/3563657.3596002", "abstract": "An increasing number of everyday products are being designed to possess qualities such as intelligence, consciousness, and emotion. However, there is a need for more understanding on how to design for these properties of mind. To address this, we present the design of Areca, an air purifier that keeps a diary. This paper outlines our design process, focusing on how the diary generation process gives Areca properties of mind and how its appearance and interaction design support this concept. Through exhibiting Areca in a design exhibition, we gathered people's initial reactions and perceptions to further evaluate the effectiveness of our design intentions. Finally, based on these experiences, we engage in discussions on the design of products having minds.", "morphology": "Functional", "studyMethod": "Other Method", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 31, "year": 2023, "authors": "Axelsson, Agnes; Skantze, Gabriel", "title": "Do you follow? A fully automated system for adaptive robot presenters", "venue": "HRI", "doi": "10.1145/3568162.3576958", "url": "https://dl.acm.org/doi/10.1145/3568162.3576958", "abstract": "An interesting application for social robots is to act as a presenter, for example as a museum guide. In this paper, we present a fully automated system architecture for building adaptive presentations for embodied agents. The presentation is generated from a knowledge graph, which is also used to track the grounding state of information, based on multimodal feedback from the user. We introduce a novel way to use large-scale language models (GPT-3 in our case) to lexicalise arbitrary knowledge graph triples, greatly simplifying the design of this aspect of the system. We also present an evaluation where 43 participants interacted with the system. The results show that users prefer the adaptive system and consider it more human-like and flexible than a static version of the same system, but only partial results are seen in their learning of the facts presented by the robot.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Technical Evaluation", "evaluation": "Subjective", "application": "Teaching and education" },
    { "id": 32, "year": 2025, "authors": "Ferrini, Lorenzo; Andriella, Antonio; Ros, Raquel; Lemaignan, Séverin", "title": "From Percepts to Semantics: A Multi-modal Saliency Map to Support Social Robots’ Attention", "venue": "THRI", "doi": "10.1145/3737891", "url": "https://dl.acm.org/doi/10.1145/3737891", "abstract": "In social robots, visual attention expresses awareness of the scenario components and dynamics. As in humans, their attention should be driven by a combination of different attention mechanisms. In this article, we introduce multi-modal saliency maps, i.e., spatial representations of saliency that dynamically integrate multiple attention sources depending on the context. We provide the mathematical formulation of the model and an open source software implementation. Finally, we present an initial exploration of its potential in social interaction scenarios with humans and evaluate its implementation.", "morphology": "Other Morphology", "studyMethod": "Laboratory Experiment, Other Method", "evaluation": "Objective", "application": "Social and conversational systems" },
    { "id": 33, "year": 2025, "authors": "Mahadevan, Karthik; Lewis, Blaine; Li, Jiannan; Mutlu, Bilge; Tang, Anthony; Grossman, Tovi", "title": "ImageInThat: Manipulating Images to Convey User Instructions to Robots", "venue": "HRI", "doi": "10.5555/3721488.3721582", "url": "https://dl.acm.org/doi/10.5555/3721488.3721582", "abstract": "Foundation models are rapidly improving the capability of robots in performing everyday tasks autonomously such as meal preparation, yet robots will still need to be instructed by humans due to model performance, the difficulty of capturing user preferences, and the need for user agency. Robots can be instructed using various methods---natural language conveys immediate instructions but can be abstract or ambiguous, whereas end-user programming supports longer-horizon tasks but interfaces face difficulties in capturing user intent. In this work, we propose using direct manipulation of images as an alternative paradigm to instruct robots, and introduce a specific instantiation called ImageInThat which allows users to perform direct manipulation on images in a timeline-style interface to generate robot instructions. Through a user study, we demonstrate the efficacy of ImageInThat to instruct robots in kitchen manipulation tasks, comparing it to a text-based natural language instruction method. The results show that participants were faster with ImageInThat and preferred to use it over the text-based method. Supplementary material including code can be found at: https://image-in-that.github.io/.", "morphology": "Functional", "studyMethod": "Interview, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 34, "year": 2025, "authors": "Kontogiorgos, Dimosthenis; Shah, Julie", "title": "Questioning the Robot: Using Human Non-verbal Cues to Estimate the Need for Explanations", "venue": "HRI", "doi": "10.5555/3721488.3721577", "url": "https://dl.acm.org/doi/10.5555/3721488.3721577", "abstract": "As black-box AI systems become increasingly complex, understanding when and how to provide explanations to users is crucial. Multimodal signals, such as facial expressions, offer novel insights into how frequently explanations should be given. This paper explores whether users' facial features can help estimate the need for explanations in a collaborative robot task. We applied three state-of-the-art eXplainable AI (XAI) methods, addressing how, why, and what-if questions, explaining the robot's failure detection model. Each explanation type conveyed information differently: how-explanations described how the model functions, why-explanations provided personalised insights into input-feature-related cues, and what-if-explanations explored alternative scenarios. In a mixed-design study (N = 33), participants performed a robot-assisted pick-and-place task, receiving different explanation types. Our results show that users responded differently to these explanations, with why-explanations being the most preferred and prompting closer alignment in facial expressions with the robot. Contrary to expectations, what-if explanations led to the least alignment and required greater vocal effort. These findings demonstrate how non-verbal cues can guide the frequency and type of explanations (personalised or general) and further highlight the importance of model transparency in human-robot collaboration.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Subjective", "application": "Industrial manufacturing" },
    { "id": 35, "year": 2024, "authors": "Lee, Christine P; Praveena, Pragathi; Mutlu, Bilge", "title": "REX: Designing User-centered Repair and Explanations to Address Robot Failures", "venue": "DIS", "doi": "10.1145/3643834.3661559", "url": "https://dl.acm.org/doi/10.1145/3643834.3661559", "abstract": "Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors—safety, privacy, and complexity—that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.", "morphology": "Functional", "studyMethod": "Demonstration, Interview", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 36, "year": 2024, "authors": "Verma, Mudit; Bhambri, Siddhant; Kambhampati, Subbarao", "title": "Theory of Mind Abilities of Large Language Models in Human-Robot Interaction: An Illusion?", "venue": "HRI", "doi": "10.1145/3610978.3640767", "url": "https://dl.acm.org/doi/10.1145/3610978.3640767", "abstract": "Large Language Models (LLMs) have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of LLMs especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs an LLM to assess the robot’s generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot’s behavior X, would the human observer fnd it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across fve domains. A frst analysis of the belief test yields extremely positive results infating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. The high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack. We report our results on GPT-4 and GPT-3.5-turbo.", "morphology": "Functional", "studyMethod": "Questionnaire, Other Method", "evaluation": "Objective", "application": "Social and conversational systems" },
    { "id": 37, "year": 2024, "authors": "Mahadevan, Karthik; Chien, Jonathan; Brown, Noah; Xu, Zhuo; Parada, Carolina; Xia, Fei; Zeng, Andy; Takayama, Leila; Sadigh, Dorsa", "title": "Generative Expressive Robot Behaviors using Large Language Models", "venue": "HRI", "doi": "10.1145/3610977.3634999", "url": "https://dl.acm.org/doi/10.1145/3610977.3634999", "abstract": "People employ expressive behaviors to efectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying “excuse me” to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot’s available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.", "morphology": "Functional", "studyMethod": "Demonstration, Interview, Questionnaire", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 38, "year": 2025, "authors": "Skantze, Gabriel; Irfan, Bahar", "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction", "venue": "HRI", "doi": "10.5555/3721488.3721593", "url": "https://dl.acm.org/doi/10.5555/3721488.3721593", "abstract": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a withinsubject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.", "morphology": "Humanoid", "studyMethod": "Demonstration, Interview, Questionnaire", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 39, "year": 2024, "authors": "Mannava, Vivek; Mitrevski, Alex; Plöger, Paul G.", "title": "Exploring the suitability of conversational AI for child-robot interaction", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731435", "url": "https://ieeexplore.ieee.org/document/10731435", "abstract": "Current approaches in education, while aiming to be universally effective, often struggle to fully adapt to the unique needs and communication styles of individual children; this disparity can limit the children’s engagement and hinder their learning progress. Similarly, parents or guardians, despite their good intentions, may also be unable to provide consistent and personalized support to each child. In this work, we investigate the use of conversational systems for socially assistive robots (SARs) as a potential solution to this problem, as such systems have the potential to allow children to interact and learn at their own pace, in a way that aligns with their communication preferences. To ensure that the robot’s language is suitable for children, we present a system that leverages a combination of natural language processing (NLP) techniques, including dialog management, child-friendly language generation, and context-aware response adaptation; to achieve this, our system combines Rasa for dialog management, GPT-3.5 for language generation, and textstat for language complexity evaluation. We evaluate the suitability of the generated language for a young audience through two user studies with adult participants, one in which the conversational system was embodied in a robot and involved direct interaction between a human and a robot, and another where participants evaluated conversational transcripts from the first study. Our results suggest that the system has the potential to maintain engaging and safe conversations, and adapt its language to individual needs.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Subjective", "application": "Teaching and education" },
    { "id": 40, "year": 2024, "authors": "Sievers, Thomas; Russwinkel, Nele", "title": "Introducing a note of levity to human-robot interaction with dialogs containing irony, sarcasm and jocularity", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731234", "url": "https://ieeexplore.ieee.org/document/10731234", "abstract": "Dialogs between humans and social robots are often fact-oriented and not very mood-dependent, at least on the part of the robot. This raises the question of what influence the tonality of the utterances has on the perceived personality of the robot and how this influences the user’s further expectations. One element that is not necessarily expected from robots in conversation is humor, especially the use of irony and sarcasm. We examined the effect of a conversation with cheerfully ironic or sullenly sarcastic remarks by the robot in contrast to utterances in a more seriously neutral tone. To measure these effects in terms of perceived warmth, competence and possible discomfort in conversation we used the Robotic Social Attributes Scale (RoSAS) on 20 participants with an age ranging from 26 to 58. In addition, we asked the participants for their assessment of the suitability of the robot’s dialog style for different areas of application and which dialog style they personally liked best. The dialog parts of the robot were generated with ChatGPT using suitable prompts. Our results showed that the perceived warmth gained highest rates when when the robot hat a cheerfully ironic tone. Discomfort was rated significantly higher when a sullenly sarcastic tone was used by the robot. Perceived competence seemed to be slightly negatively influenced in the sarcastic condition. The results proved that tonality appears to be a very relevant design element for a successful interaction between humans and robots, but must be used carefully to achieve the desired goal.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Subjective", "application": "Social and conversational systems" },
    { "id": 41, "year": 2024, "authors": "Sievers, Thomas; Russwinkel, Nele", "title": "Interacting with a sentimental robot – making human emotions tangible for a social robot via ChatGPT", "venue": "ARSO", "doi": "10.1109/ARSO60199.2024.10557749", "url": "https://ieeexplore.ieee.org/document/10557749", "abstract": "Recognizing and correctly assessing the emotions of a human conversation partner is – if successful – a milestone in social interaction between humans and social robots. The robot should recognize human emotions and take them into account in its reactions. It is also important that humans and robots assess the content of the dialog from an emotional perspective in the same way. But how can the emotional state of a person in a dialog with a robot be accessed? We examined this question more closely in our study. A Large Language Model (LLM) from OpenAI (ChatGPT) was used for conversations with a Pepper robot. We had the course of the dialog assessed once by the human interlocutor and once by the GPT model itself using sentiment analysis. In addition, the predominant emotion was named by both conversation partners after the dialog. A comparison of these evaluations provided an assessment of whether the human and the social robot arrived at the same results. We were also investigating whether the transmission of emotion recognition data had a noticeable influence on the tonality of the conversation. To do this, we used the robot’s emotion recognition capabilities to send cues to the GPT model about the current emotional state of the human at each turn of the conversation, so that the LLM could take this into account in the generation of the robot’s utterances. It was found that the predominant emotion of the human and the general mood of a conversation were interpreted by humans and the GPT model in largely the same way, whereby an existing emotion recognition made the robot’s assessment of the general mood deviate noticeably.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 42, "year": 2023, "authors": "Cui, Yuchen; Karamcheti, Siddharth; Palleti, Raj; Shivakumar, Nidhya; Liang, Percy; Sadigh, Dorsa", "title": "No, to the Right: Online Language Corrections for Robotic Manipulation via Shared Autonomy", "venue": "HRI", "doi": "10.1145/3568162.3578623", "url": "https://dl.acm.org/doi/10.1145/3568162.3578623", "abstract": "Systems for language-guided human-robot interaction must satisfy two key desiderata for broad adoption: adaptivity and learning efficiency. Unfortunately, existing instruction-following agents cannot adapt, lacking the ability to incorporate online natural language supervision, and even if they could, require hundreds of demonstrations to learn even simple policies. In this work, we address these problems by presenting Language-Informed Latent Actions with Corrections (LILAC), a framework for incorporating and adapting to natural language corrections \"to the right\", or \"no, towards the book\" - online, during execution. We explore rich manipulation domains within a shared autonomy paradigm. Instead of discrete turn-taking between a human and robot, LILAC splits agency between the human and robot: language is an input to a learned model that produces a meaningful, low-dimensional control space that the human can use to guide the robot. Each real-time correction refines the human's control space, enabling precise, extended behaviors - with the added benefit of requiring only a handful of demonstrations to learn. We evaluate our approach via a user study where users work with a Franka Emika Panda manipulator to complete complex manipulation tasks. Compared to existing learned baselines covering both open-loop instruction following and single-turn shared autonomy, we show that our corrections-aware approach obtains higher task completion rates, and is subjectively preferred by users because of its reliability, precision, and ease of use.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Both", "application": "Other Domain" },
    { "id": 43, "year": 2025, "authors": "Wanniarachchi, Dhanuja; Misra, Archan", "title": "MIMIC: AI and AR-enhanced Multi-Modal, Immersive, Relative Instruction Comprehension", "venue": "IMWUT", "doi": "10.1145/3712268", "url": "https://dl.acm.org/doi/10.1145/3712268", "abstract": "We present a multimodal instruction comprehension framework, called MImIC, that utilizes visual sensing (including LIDAR and 2D RGB sensing) & AI spatial reasoning capabilities to support more seamless and immersive interaction between humans and AI-driven situated assistive agents. MImIC's key new capability is to support disambiguation of a wider set of relative spatial references that users naturally employ while issuing spatially-situated instructions. To support enhanced visual grounding via a combination of both fully-qualified and relative attribute references, MImIC uses (a) a fine-tuned transformer-based language translation DNN to accurately convert natural verbal commands into a structured set of machine understandable constraints (BLEU score=92.5), (b) a set of modules that use RGB+LIDAR sensing data to convert any relative attribute preferences to fully-qualified attribute constraints (median height/width estimation errors <=2cm), and (c) an enhanced image segmentation DNN, augmented with gesture+verbal cues, to extract target objects of interest (top-1 accuracy= ~85%). To demonstrate the viability and superiority of MImIC, we implement an exemplar AR-augmented, immersive furniture shopping application, called AIRFurn. AIRFurn allows users to browse for, select and overlay furniture items of interest using natural multi-modal and relative cues. experimental studies, using 34 & 11 users over 8 different layouts in a lab setting and 11 users in 6 different real-world home setups, show that AIRFurn offers superior performance, with significantly (~3x) lower task completion times, much higher task (17%+) accuracy and greater user satisfaction (SUS score= 78.8) compared to baselines where users perform selection using only fully-qualified verbal commands or manipulation of AR interfaces.", "morphology": "Other Morphology", "studyMethod": "Laboratory Experiment, Field Deployment, Questionnaire", "evaluation": "Both", "application": "AR/VR-enabled Interaction" },
    { "id": 44, "year": 2024, "authors": "Su, Zhidong; Sheng, Weihua", "title": "ChatAdp: ChatGPT-powered adaptation system for human-robot interaction", "venue": "ICRA", "doi": "10.1109/ICRA57147.2024.10611520", "url": "https://ieeexplore.ieee.org/document/10611520", "abstract": "Different people have different preferences when it comes to human-robot interaction. Therefore, it is desirable for the robot to adapt its actions to fit users’ preferences. Human feedback is essential to facilitating robot adaptation. However, when the task is complex or the robot action space is large, it requires a large amount of user feedback. ChatGPT is a powerful generative AI tool based on large language models (LLMs), which possesses a significant corpus of information obtained from human society, and exhibits robust proficiency in the comprehension and acquisition of natural language. Therefore, in this paper, we proposed a ChatGPT-powered adaptation system (ChatAdp) for human-robot interaction which requires less user feedback to achieve a good adaptation result. In the proposed ChatAdp, we use ChatGPT as a user simulator to provide feedback. We evaluated ChatAdp in a case study for context-aware conversation adaptation. The results are very promising. Our proposed method can achieve a mean success rate of 92% on the user’s natural language-described preferences after receiving 33 rounds of feedback from a user on average, which is only 2% of the number of states covered by the user preferences and outperforms the two baseline methods.", "morphology": "Other Morphology", "studyMethod": "Demonstration, Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 45, "year": 2023, "authors": "Zhang, Bowen; Soh, Harold", "title": "Large language models as zero-shot human models for human-robot interaction", "venue": "IROS", "doi": "10.1109/IROS55552.2023.10341488", "url": "https://ieeexplore.ieee.org/document/10341488", "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios focused on the important element of trust. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n=65) where preliminary results show that planning with an LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 46, "year": 2024, "authors": "Latif, Ehsan; Parasuraman, Ramviyas; Zhai, Xiaoming", "title": "PhysicsAssistant: An LLM-powered interactive learning robot for physics lab investigations", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731312", "url": "https://ieeexplore.ieee.org/document/10731312", "abstract": "Robot systems in education can leverage Large language models’ (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students’ physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants’ responses to student queries on a 0-4 scale based on Bloom’s taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers’ labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.", "morphology": "Other Morphology", "studyMethod": "Demonstration, Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 47, "year": 2024, "authors": "Yu, Hongqi; Tang, Fei; Zhang, Lei; Gomez, Randy; Nichols, Eric; Li, Guangliang", "title": "Improving perceived emotional intelligence of embodied chatbot haru via multi-modal interaction", "venue": "ROBIO", "doi": "10.1109/ROBIO64047.2024.10907584", "url": "https://ieeexplore.ieee.org/document/10907584", "abstract": "To realize natural and friendly interaction, this paper proposed an emotion-aware framework to enable embodied chatbot Haru to mimic human users' emotions via multi-modal interaction during conversation. The proposed framework consists of ASR for speech recognition, ChatGPT for dialogue generation and Text-to-Speech for text to speech conversion. In addition, the pitch, rate and emotions of human speech detected with deep learning models are used to adjust the pitch and rate of Haru's speech. In addition, Haru can mimic the emotional state of human users by expressing emotive routine behaviors consisting of base rotation, eye animation, mouth movement etc., based on the detected emotion of human speech. Results of a user study with 20 subjects show that our embodied chatbot Haru can successfully mimic the intonation and emotion of human speech via vocal and visual interactions. Moreover, participants reported to have a better conversing experience with our embodied chatbot Haru compared to a neutral one with only lip synchronization.", "morphology": "Functional", "studyMethod": "Interview, Questionnaire", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 48, "year": 2024, "authors": "Pinto, Maria J.; Belpaeme, Tony", "title": "Predictive turn-taking: Leveraging language models to anticipate turn transitions in human-robot dialogue", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731379", "url": "https://ieeexplore.ieee.org/document/10731379", "abstract": "Natural and engaging spoken dialogue systems require seamless turn-taking coordination to avoid awkward interruptions and unnatural pauses. Traditional systems often rely on simplistic silence thresholds, relinquishing the turn after a predetermined period of silence, which invariably leads to a suboptimal interaction experience. This work explores the potential of Large Language Models (LLMs) for improved turn-taking prediction. Building upon research that uses linguistic cues, we investigate how LLMs, with their rich contextual knowledge and semantic encoding of language, can be used for this task. We hypothesize that by analysing dialogue context, syntactic structure, and pragmatic cues within the user’s utterance, LLMs can offer more accurate turn-completion predictions. This research evaluates the capabilities of recent LLMs such as Gemini, OpenAI’s API, Anthropic’s Claude2, and Meta AI’s Llama 2 to predict turn-ending points solely based on textual information, and demonstrates how the conversation between elderly users and companion robots can be enhanced by LLM-powered end-of-turn prediction.", "morphology": "Humanoid", "studyMethod": "Field Deployment", "evaluation": "Objective", "application": "Social and conversational systems" },
    { "id": 49, "year": 2025, "authors": "Bastin, Brieuc; Hasegawa, Shoichi; Solis, Jorge; Ronsse, Renaud; Macq, Benoit; El Hafi, Lotfi; Ricardez, Gustavo Alfonso Garcia; Taniguchi, Tadahiro", "title": "GPTAlly: a safety-oriented system for human-robot collaboration based on foundation models", "venue": "SII", "doi": "10.1109/SII59315.2025.10870936", "url": "https://ieeexplore.ieee.org/document/10870936", "abstract": "As robots increasingly integrate into the workplace, Human-Robot Collaboration (HRC) has become increasingly important. However, most HRC solutions are based on pre-programmed tasks and use fixed safety parameters, which keeps humans out of the loop. To overcome this, HRC solutions that can easily adapt to human preferences during the operation as well as their safety precautions considering the familiarity with robots are necessary. In this paper, we introduce GPTAlly, a novel safety-oriented system for HRC that leverages the emerging capabilities of Large Language Models (LLMs). GPTAlly uses LLMs to 1) infer users’ subjective safety perceptions to modify the parameters of a Safety Index algorithm; 2) decide on subsequent actions when the robot stops to prevent unwanted collisions; and 3) re-shape the robot arm trajectories based on user instructions. We subjectively evaluate the robot’s behavior by comparing the safety perception of GPT-4 to the participants. We also evaluate the accuracy of natural language-based robot programming of decision-making requests. The results show that GPTAlly infers safety perception similarly to humans, and achieves an average of 80% of accuracy in decision-making, with few instances under 50%. Code available at: https://axtiop.github.io/GPTAlly", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Industrial manufacturing" },
    { "id": 50, "year": 2024, "authors": "Yano, Yuga; Mizutani, Akinobu; Fukuda, Yukiya; Kanaoka, Daiju; Ono, Tomohiro; Tamukoh, Hakaru", "title": "Unified understanding of environment, task, and human for human-robot interaction in real-world environments", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731235", "url": "https://ieeexplore.ieee.org/document/10731235", "abstract": "To facilitate human--robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.", "morphology": "Functional", "studyMethod": "Field Deployment, Questionnaire", "evaluation": "Both", "application": "Public space service" },
    { "id": 51, "year": 2025, "authors": "Zhang, Tianyi; Au Yeung, Colin; Aurelia, Emily; Onishi, Yuki; Chulpongsatorn, Neil; Li, Jiannan; Tang, Anthony", "title": "Prompting an embodied AI agent: How embodiment and multimodal signaling affects prompting behaviour", "venue": "CHI", "doi": "10.1145/3706598.3713110", "url": "https://doi.org/10.1145/3706598.3713110", "abstract": "Current voice agents wait for a user to complete their verbal instruction before responding; yet, this is misaligned with how humans engage in everyday conversational interaction, where interlocutors use multimodal signaling (e.g. nodding, grunting, or looking at referred to objects) to ensure conversational grounding. We designed an embodied VR agent that exhibits multimodal signaling behaviors in response to situated prompts, by turning its head, or by visually highlighting objects being discussed or referred to. We explore how people prompt this agent to design and manipulate the objects in a VR scene. Through a Wizard of Oz study, we found that participants interacting with an agent that indicated its understanding of spatial and action references were able to prevent errors 30% of the time, and were more satisfied and confident in the agent’s abilities. These findings underscore the importance of designing multimodal signaling communication techniques for future embodied agents.", "morphology": "Other Morphology", "studyMethod": "Laboratory Experiment, Interview, Questionnaire, Other Method", "evaluation": "Objective", "application": "AR/VR-enabled Interaction" },
    { "id": 52, "year": 2025, "authors": "Shirado, Hirokazu; Shimizu, Kye; Christakis, Nicholas A; Kasahara, Shunichi", "title": "Realism drives interpersonal reciprocity but yields to AI-assisted egocentrism in a coordination experiment", "venue": "CHI", "doi": "10.1145/3706598.3713371", "url": "https://doi.org/10.1145/3706598.3713371", "abstract": "Virtual reality technologies that enhance realism and artificial intelligence (AI) systems that assist human behavior are increasingly interwoven in social applications. However, how these technologies might jointly influence interpersonal coordination remains unclear. We conducted an experiment with 240 participants in 120 pairs who interacted through remote-controlled robot cars in a physical space or virtual cars in a digital space, with or without autosteering assistance, using the chicken game, an established model of interpersonal coordination. We find that both realism and AI assistance help improve user performance but through opposing mechanisms. Real-world contexts enhanced communication, fostering reciprocal actions and collective benefits. In contrast, autosteering assistance diminished the need for interpersonal coordination, shifting participants’ focus towards self-interest. Notably, when combined, the egocentric effects of autosteering assistance outweighed the prosocial effects of realism. The design of HCI systems that involve social coordination will, we believe, need to take such effects into account.", "morphology": "Other Morphology", "studyMethod": "Technical Evaluation", "evaluation": "Subjective", "application": "AR/VR-enabled Interaction" },
    { "id": 53, "year": 2024, "authors": "Elfleet, Morad; Chollet, Mathieu", "title": "Investigating the impact of multimodal feedback on user-perceived latency and immersion with LLM-powered embodied conversational agents in virtual reality", "venue": "IVA", "doi": "10.1145/3652988.3673965", "url": "https://doi.org/10.1145/3652988.3673965", "abstract": "Our research investigates the impact of latency on presence and immersion in virtual reality (VR) environments, focusing on interactions with LLM-powered Embodied Conversational Agents (ECAs). We explore the effectiveness of multimodal feedback strategies—including filled pauses, nonverbal turn-taking behaviours, and visual feedback—in mitigating perceived latency. Eighteen participants were subjected to both a baseline condition, without feedback interventions, and a feedback-enhanced condition. Our findings indicate that the feedback condition significantly improved the sense of presence and immersion. We also found that perceived response time and users’ impressions of the agents improved, thereby increasing willingness for future interactions. Additionally, chatbot experience positively correlated with agent likeability, whereas VR experience showed no significant correlation. These results highlight the effectiveness of feedback modalities in enhancing spatial presence and overall immersion, despite latency issues in VR interactions with LLM-powered agents.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Other Method", "evaluation": "Both", "application": "AR/VR-enabled Interaction" },
    { "id": 54, "year": 2024, "authors": "van Rijn, Pol; Mertes, Silvan; Janowski, Kathrin; Weitz, Katharina; Jacoby, Nori; André, Elisabeth", "title": "Giving robots a voice: Human-in-the-loop voice creation and open-ended labeling", "venue": "CHI", "doi": "10.1145/3613904.3642038", "url": "https://doi.org/10.1145/3613904.3642038", "abstract": "Speech is a natural interface for humans to interact with robots. Yet, aligning a robot’s voice to its appearance is challenging due to the rich vocabulary of both modalities. Previous research has explored a few labels to describe robots and tested them on a limited number of robots and existing voices. Here, we develop a robot-voice creation tool followed by large-scale behavioral human experiments (N=2,505). First, participants collectively tune robotic voices to match 175 robot images using an adaptive human-in-the-loop pipeline. Then, participants describe their impression of the robot or their matched voice using another human-in-the-loop paradigm for open-ended labeling. The elicited taxonomy is then used to rate robot attributes and to predict the best voice for an unseen robot. We offer a web interface to aid engineers in customizing robot voices, demonstrating the synergy between cognitive science and machine learning for engineering tools.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 55, "year": 2022, "authors": "Elgarf, Maha; Zojaji, Sahba; Skantze, Gabriel; Peters, Christopher", "title": "CreativeBot: a Creative Storyteller robot to stimulate creativity in children", "venue": "ICMI", "doi": "10.1145/3536221.3556578", "url": "https://doi.org/10.1145/3536221.3556578", "abstract": "We present the design and evaluation of a storytelling activity between children and an autonomous robot aiming at nurturing children’s creativity. We assessed whether a robot displaying creative behavior will positively impact children’s creativity skills in a storytelling context. We developed two models for the robot to engage in the storytelling activity: creative model, where the robot generates creative story ideas, and the non-creative model, where the robot generates non-creative story ideas. We also investigated whether the type of the storytelling interaction will have an impact on children’s creativity skills. We used two types of interaction: 1) Collaborative, where the child and the robot collaborate together by taking turns to tell a story. 2) Non-collaborative: where the robot first tells a story to the child and then asks the child to tell it another story. We conducted a between-subjects study with 103 children in four different conditions: Creative collaborative, Non-creative collaborative, Creative non-collaborative and Non-Creative non-collaborative. The children’s stories were evaluated according to the four standard creativity variables: fluency, flexibility, elaboration and originality. Results emphasized that children who interacted with a creative robot showed higher creativity during the interaction than children who interacted with a non-creative robot. Nevertheless, no significant effect of the type of the interaction was found on children’s creativity skills. Our findings are significant to the Child-Robot interaction (cHRI) community since they enrich the scientific understanding of the development of child-robot encounters for educational applications.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Other Method", "evaluation": "Subjective", "application": "Teaching and education" },
    { "id": 56, "year": 2024, "authors": "Kim, Callie Y.; Lee, Christine P.; Mutlu, Bilge", "title": "Understanding large-language model (LLM)-powered human-robot interaction", "venue": "HRI", "doi": "10.1145/3610977.3634966", "url": "https://doi.org/10.1145/3610977.3634966", "abstract": "Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.", "morphology": "Humanoid", "studyMethod": "Demonstration, Laboratory Experiment, Interview", "evaluation": "Subjective", "application": "Other Domain" },
    { "id": 57, "year": 2025, "authors": "Cho, Hyungjun; Nam, Tek-Jin", "title": "Living alongside areca: Exploring human experiences with things expressing thoughts and emotions", "venue": "CHI", "doi": "10.1145/3706598.3713228", "url": "https://doi.org/10.1145/3706598.3713228", "abstract": "Technological advancements such as LLMs have enabled everyday things to use language, fostering increased anthropomorphism during interactions. This study employs material speculation to investigate how people experience things that express their thoughts, emotions, and intentions. We utilized Areca, an air purifier capable of keeping a diary, and placed it in the everyday spaces of eight participants over three weeks. Weekly interviews were conducted to capture participants’ evolving interactions with Areca, concluding with a session collaboratively speculating on the future of everyday things. Our findings indicate that things expressing thoughts, emotions, and intentions can be perceived as possessing agency beyond mere functionality. While some participants exhibited emotional engagement with Areca over time, responses varied, including moments of detachment. We conclude with design implications for HCI designers, offering insights into how emerging technologies may shape human-thing relationships in complex ways.", "morphology": "Other Morphology", "studyMethod": "Field Deployment, Interview", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 58, "year": 2025, "authors": "Wang, Mengyang; Yu, Keye; Zhang, Yukai; Fan, Mingming", "title": "Challenges in adopting companion robots: An exploratory study of robotic companionship conducted with Chinese retirees", "venue": "Proceedings of the ACM on Human-Computer Interaction", "doi": "10.1145/3710943", "url": "https://doi.org/10.1145/3710943", "abstract": "Companion robots hold immense potential in providing emotional support to older adults in the rapidly aging world. However, questions have been raised regarding whether healthy older adults benefit from having a robotic companion, how they perceive the value of companion robots, and what their relationship with companion robots would be like. To understand healthy older adults' perceptions, attitudes, and relationships toward companion robots, we conducted multiple focus groups with eighteen retirees. Our findings reveal the social context encountered by older adults in China and the mismatch between the current value proposition of companion robots and healthy older adults' needs. We further identify factors that may influence the adoption of robotic companionship, which include individuals' self-disclosure tendencies, quality of companionship, differentiated value, and seamless collaboration with aging-in-community infrastructure and services.", "morphology": "Functional", "studyMethod": "Demonstration, Interview, Questionnaire", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 59, "year": 2024, "authors": "Padmanabha, Akhil; Yuan, Jessie; Gupta, Janavi; Karachiwalla, Zulekha; Majidi, Carmel; Admoni, Henny; Erickson, Zackory", "title": "VoicePilot: Harnessing LLMs as speech interfaces for physically assistive robots", "venue": "UIST", "doi": "10.1145/3654777.3676401", "url": "https://doi.org/10.1145/3654777.3676401", "abstract": "Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos, code, and supporting files are located on our project website.", "morphology": "Functional", "studyMethod": "Demonstration, Interview, Questionnaire", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 60, "year": 2024, "authors": "Blanco, Antonio; Pérez, Gerardo; Condón, Alicia; Rodríguez, Trinidad; Núñez, Pedro", "title": "AI-enhanced social robots for older adults care: Evaluating the efficacy of ChatGPT-powered storytelling in the EBO platform", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731292", "url": "https://ieeexplore.ieee.org/document/10731292", "abstract": "The population of older adults is growing worldwide, and with it, there is a need for appropriate and effective cognitive therapies. Social robots have shown potential as therapeutic tools for older adults, providing companionship, entertainment, and therapeutic support. In particular, storytelling activities promote cognitive stimulation, socio-emotional skills, and simple entertainment. However, conventional storytelling methodologies concerning engagement, personalization, and interactivity exhibit a potential for greater diversity. These traditional approaches often require extensive preparation time, which can limit their feasibility and adaptability in diverse settings. Social robots can help overcome these limitations by providing a more interactive and engaging storytelling experience. In this paper, we present an approach to enhance the storytelling capabilities of the EBO robot, a social robot designed for interaction with older adult people. As a novelty, in addition to integrating the proposed system in the CORTEX cognitive architecture, we offer to include the therapist in the loop and integrate Artificial Intelligence techniques, including a Large Language Model, ChatGPT, to enable the robot to generate natural-sounding and engaging narratives, adjust its interactions based on the cognitive impairments, and align with the interests and preferences of the user. We demonstrate the effectiveness of our approach through a case study involving older adult participants and therapists. Our preliminary results show that the EBO robot, integrating AI techniques, can provide an interactive, customizable, and socially aware storytelling experience that promotes cognitive stimulation, socio-emotional skills, and simple entertainment that is engaging and enjoyable for older adults. Our approach could contribute to developing effective and engaging therapies for older adults using social robots as therapeutic tools.", "morphology": "Other Morphology", "studyMethod": "Laboratory Experiment, Field Deployment, Questionnaire", "evaluation": "Subjective", "application": "Healthcare and Wellbeing" },
    { "id": 61, "year": 2024, "authors": "Salem, Ahmed; Sumi, Kaoru", "title": "A comparative human-robot interaction study between face-display and an advanced social robot", "venue": "COMPSAC", "doi": "10.1109/COMPSAC61105.2024.00090", "url": "http://ieeexplore.ieee.org/document/10633431", "abstract": "Social robots are taking major roles in our lives through many sectors, ranging from eHealth to industry. However, social robots that can interact with people are expensive. In this study, we evaluate what makes an inexpensive retro-projected Face-Display/robot different from the advanced social robot “Furhat”. How will a retro-projected robot be perceived compared to an advanced social robot? We built a retro-projected robot and compared it with Furhat. Seventy-two participants interacted with both robots in a between-subjects study design. We compare the two robots using different HRI questionnaires. Furthermore, we investigate which robot face is preferred to interact with, a human face (due to social agency) or an anime face (due to Japanese anime culture). In a retro-projected robot, a human face is preferable as it is rated higher in terms of anthropomorphization, trust, competence, and empathy. An anime face was rated higher in warmth, attractiveness, pleasantness, and comfort to see. In Furhat, few differences were noticed between the human and anime faces. A human face was perceived to be more emphatically responsive, and an anime face was rated highly in trustworthiness, warmth, competence, and empathic understanding. Furhat excelled significantly in the aspects of anthropomorphization, likeability, trustworthiness, warmth, competence, empathic responsiveness, attractiveness, and pleasantness to see. On the contrary, the retro-projected robot excelled in inducing social discomfort. Interestingly, both robots were perceived similarly in aspects of perceived intelligence and safety, empathic understanding, and comfort to see. An anime face on either robot is significantly more pleasant and is always perceived to be young in age. We deduce that retro-projected robots are inferior to Furhat in most HRI aspects and similar in a few of them. Thus, retro-projected robots can serve effectively in fields where the three aforementioned indifferent aspects are the main requirements in an application. Moreover, whenever aesthetics and visual pleasantness are needed, an anime face is preferred over a human one.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 62, "year": 2023, "authors": "Li, Jiannan; Sousa, Maurı́cio; Mahadevan, Karthik; Wang, Bryan; Aoyagui, Paula Akemi; Yu, Nicole; Yang, Angela; Balakrishnan, Ravin; Tang, Anthony; Grossman, Tovi", "title": "Stargazer: An interactive camera robot for capturing how-to videos based on subtle instructor cues", "venue": "CHI", "doi": "10.1145/3544548.3580896", "url": "https://doi.org/10.1145/3544548.3580896", "abstract": "Live and pre-recorded video tutorials are an effective means for teaching physical skills such as cooking or prototyping electronics. A dedicated cameraperson following an instructor’s activities can improve production quality. However, instructors who do not have access to a cameraperson’s help often have to work within the constraints of static cameras. We present Stargazer, a novel approach for assisting with tutorial content creation with a camera robot that autonomously tracks regions of interest based on instructor actions to capture dynamic shots. Instructors can adjust the camera behaviors of Stargazer with subtle cues, including gestures and speech, allowing them to fluidly integrate camera control commands into instructional activities. Our user study with six instructors, each teaching a distinct skill, showed that participants could create dynamic tutorial videos with a diverse range of subjects, camera framing, and camera angle combinations using Stargazer.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Interview, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Teaching and education" },
    { "id": 63, "year": 2024, "authors": "Wang, Yufei; Zeng, Wenting; Liu, Changzhen; Ye, Zhuohan; Sun, Jiawei; Ji, Junxiang; Jiang, Zhihan; Yan, Xianyi; Wu, Yongyi; Wang, Yigao; Yang, Dingqi; Wang, Leye; Zhang, Daqing; Wang, Cheng; Chen, Longbiao", "title": "CrowdBot: An open-environment robot management system for on-campus services", "venue": "IMWUT", "doi": "10.1145/3659601", "url": "https://doi.org/10.1145/3659601", "abstract": "In contemporary campus environments, the provision of timely and efficient services is increasingly challenging due to limitations in accessibility and the complexity and openness of the environment. Existing service robots, while operational, often struggle with adaptability and dynamic task management, leading to inefficiencies. To overcome these limitations, we introduce CrowdBot, a robot management system that enhances service in campus environments. Our system leverages a hierarchical reinforcement learning-based cloud-edge hybrid scheduling framework (REDIS), for efficient online streaming task assignment and dynamic action scheduling. To verify the REDIS framework, we have developed a digital twin simulation platform, which integrates large language models and hot-swapping technology. This facilitates seamless human-robot interaction, efficient task allocation, and cost-effective execution through the reuse of robot equipment. Our comprehensive simulations corroborate the system's remarkable efficacy, demonstrating significant improvements with a 24.46% reduction in task completion times, a 9.37% decrease in travel distances, and up to a 3% savings in power usage. Additionally, the system achieves a 7.95% increase in the number of tasks completed and a 9.49% reduction in response time. Real-world case studies further affirm CrowdBot's capability to adeptly execute tasks and judiciously recycle resources, thereby offering a smart and viable solution for the streamlined management of campus services.", "morphology": "Other Morphology", "studyMethod": "Field Deployment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Public space service" },
    { "id": 64, "year": 2024, "authors": "Wang, Chongyang; Zheng, Siqi; Zhong, Lingxiao; Yu, Chun; Liang, Chen; Wang, Yuntao; Gao, Yuan; Lam, Tin Lun; Shi, Yuanchun", "title": "PepperPose: Full-body pose estimation with a companion robot", "venue": "CHI", "doi": "10.1145/3613904.3642231", "url": "https://doi.org/10.1145/3613904.3642231", "abstract": "Accurate full-body pose estimation across diverse actions in a user-friendly and location-agnostic manner paves the way for interactive applications in realms like sports, fitness, and healthcare. This task becomes challenging in real-world scenarios due to factors like the user’s dynamic positioning, the diversity of actions, and the varying acceptability of the pose-capturing system. In this context, we present PepperPose, a novel companion robot system tailored for optimized pose estimation. Unlike traditional methods, PepperPose actively tracks the user and refines its viewpoint, facilitating enhanced pose accuracy across different locations and actions. This allows users to enjoy a seamless action-sensing experience. Our evaluation, involving 30 participants undertaking daily functioning and exercise actions in a home-like space, underscores the robot’s promising capabilities. Moreover, we demonstrate the opportunities that PepperPose presents for human-robot interaction, its current limitations, and future developments.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Interview, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 65, "year": 2025, "authors": "Ge, Yate; Li, Meiying; Huang, Xipeng; Hu, Yuanda; Wang, Qi; Sun, Xiaohua; Guo, Weiwei", "title": "GenComUI: Exploring generative visual aids as medium to support task-oriented human-robot communication", "venue": "CHI", "doi": "10.1145/3706598.3714238", "url": "https://doi.org/10.1145/3706598.3714238", "abstract": "This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models (LLMs) that dynamically generates contextual visual aids—such as map annotations, path indicators, and animations—to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Both", "application": "Public space service" },
    { "id": 66, "year": 2025, "authors": "Ho, Hui-Ru; Kargeti, Nitigya; Liu, Ziqi; Mutlu, Bilge", "title": "SET-PAiREd: Designing for parental involvement in learning with an AI-assisted educational robot", "venue": "CHI", "doi": "10.1145/3706598.3713330", "url": "https://doi.org/10.1145/3706598.3713330", "abstract": "AI-assisted learning companion robots are increasingly used in early education. Many parents express concerns about content appropriateness, while they also value how AI and robots could supplement their limited skill, time, and energy to support their children’s learning. We designed a card-based kit, SET, to systematically capture scenarios that have different extents of parental involvement. We developed a prototype interface, PAiREd, with a learning companion robot to deliver LLM-generated educational content that can be reviewed and revised by parents. Parents can flexibly adjust their involvement in the activity by determining what they want the robot to help with. We conducted an in-home field study involving 20 families with children aged 3–5. Our work contributes to an empirical understanding of the level of support parents with different expectations may need from AI and robots and a prototype that demonstrates an innovative interaction paradigm for flexibly including parents in supporting their children.", "morphology": "Other Morphology", "studyMethod": "Field Deployment, Questionnaire, Other Method", "evaluation": "Both", "application": "Teaching and education" },
    { "id": 67, "year": 2025, "authors": "Lai, Yuzhi; Yuan, Shenghai; Nassar, Youssef; Fan, Mingyu; Gopal, Atmaraaj; Yorita, Arihiro; Kubota, Naoyuki; Rätsch, Matthias", "title": "Natural Multimodal Fusion-Based Human–Robot Interaction: Application With Voice and Deictic Posture via Large Language Model", "venue": "IEEE Robotics & Automation Magazine", "doi": "10.1109/MRA.2025.3543957", "url": "https://ieeexplore.ieee.org/document/10910098/", "abstract": "Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing human‒robot interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly, due to difficulties with complex syntax or sign language. To address the challenge, this article introduces a multimodal interaction framework that combines voice and deictic posture information to create a more natural HRI system. Visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity, using a Universal Robots UR3e manipulator. Our method demonstrates significantly better HRI performance in terms of accuracy and robustness. To benefit the research community and the general public, we made our code and design open source.", "morphology": "Functional", "studyMethod": "Demonstration, Laboratory Experiment, Field Deployment, Technical Evaluation", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 68, "year": 2024, "authors": "Zu, Weiqin; Song, Wenbin; Chen, Ruiqing; Guo, Ze; Sun, Fanglei; Tian, Zheng; Pan, Wei; Wang, Jun", "title": "Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework", "venue": "ICRA", "doi": "10.1109/ICRA57147.2024.10611462", "url": "https://ieeexplore.ieee.org/document/10611462/", "abstract": "The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Detailed experiments are conducted in both simulation and the real world demonstrating that LIM2N has solid user needs understanding, alongside an enhanced interactive experience.", "morphology": "Functional", "studyMethod": "Laboratory Experiment", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 69, "year": 2024, "authors": "Farooq, Muhammad Umar; Kang, Geon; Seo, Jiwon; Bae, Jungchan; Kang, Seoyeon; Jang, Young Jae", "title": "DAIM-HRI: A new Human-Robot Integration Technology for Industries", "venue": "ARSO", "doi": "10.1109/ARSO60199.2024.10557811", "url": "https://ieeexplore.ieee.org/document/10557811/", "abstract": "Robots have become a crucial part of the automation landscape of Industry 4.0, where human-robot interaction plays a pivotal role in task completion on the shop floor. Despite their significance, the integration of robots in small and medium enterprises (SMEs) is hindered by exorbitant costs and technical requirements. New human-robot integration (HRI) technologies can facilitate robot integration and enable non-technical staff at SMEs to control and collaborate with the robots. This work introduces a DAIM-HRI system that leverages a Large Language Model (LLM) and Robot Operating System (ROS) to enable natural language command-based robot control. It is designed to allow non-technical users to operate the robots in their language without any pre-training. By bridging the gap between humans and robots, this technology provides prospects for enhanced collaboration between shop floor workers and robots ultimately improving the efficiency of the manufacturing process.", "morphology": "Functional", "studyMethod": "Demonstration, Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Industrial manufacturing" },
    { "id": 70, "year": 2024, "authors": "Grassi, Lucrezia; Hong, Zhouyang; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio", "title": "Grounding conversational robots on vision through dense captioning and large language models", "venue": "ICRA", "doi": "10.1109/ICRA57147.2024.10611232", "url": "https://ieeexplore.ieee.org/document/10611232", "abstract": "This work explores a novel approach to empowering robots with visual perception capabilities using textual descriptions. Our approach involves the integration of GPT-4 with dense captioning, enabling robots to perceive and interpret the visual world through detailed text-based descriptions. To assess both user experience and the technical feasibility of this approach, experiments were conducted with human participants interacting with a Pepper robot equipped with visual capabilities. The results affirm the viability of the proposed approach, allowing to perform vision-based conversations effectively, despite processing time limitations.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Public space service" },
    { "id": 71, "year": 2024, "authors": "Ali, Hassan; Allgeuer, Philipp; Mazzola, Carlo; Belgiovine, Giulia; Kaplan, Burak Can; Gajdošech, Lukáš; Wermter, Stefan", "title": "Robots can multitask too: Integrating a memory architecture and llms for enhanced cross-task robot action generation", "venue": "IEEE-RAS International Conference on Humanoid Robots", "doi": "10.1109/Humanoids58906.2024.10769803", "url": "https://ieeexplore.ieee.org/abstract/document/10769803", "abstract": "Large Language Models (LLMs) have been recently used in robot applications for grounding LLM commonsense reasoning with the robot’s perception and physical abilities. In humanoid robots, memory also plays a critical role in fostering real-world embodiment and facilitating long-term interactive capabilities, especially in multi-task setups where the robot must remember previous task states, environment states, and executed actions. In this paper, we address incorporating memory processes with LLMs for generating cross-task robot actions, while the robot effectively switches between tasks. Our proposed dual-layered architecture features two LLMs, utilizing their complementary skills of reasoning and following instructions, combined with a memory model inspired by human cognition. Our results show a significant improvement in performance over a baseline of five robotic tasks, demonstrating the potential of integrating memory with LLMs for combining the robot’s action and perception for adaptive task execution.", "morphology": "Humanoid", "studyMethod": "Technical Evaluation", "evaluation": "Subjective", "application": "Other Domain" },
    { "id": 72, "year": 2025, "authors": "Shen, Jocelyn; Lee, Audrey; Alghowinem, Sharifa; Adkins, River; Breazeal, Cynthia; Park, Hae Won", "title": "Social robots as social proxies for fostering connection and empathy towards humanity", "venue": "HRI", "doi": "10.5555/3721488.3721608", "url": "https://dl.acm.org/doi/10.5555/3721488.3721608", "abstract": "Despite living in an increasingly connected world, social isolation is a prevalent issue today. While social robots have been explored as tools to enhance social connection through companionship, their potential as asynchronous social platforms for fostering connection towards humanity has received less attention. In this work, we introduce the design of a social support companion that facilitates the exchange of emotionally relevant stories and scaffolds reflection to enhance feelings of connection via five design dimensions. We investigate how social robots can serve as 'social proxies' facilitating human stories, passing stories from other human narrators to the user. To this end, we conduct a real-world deployment of 40 robot stations in users' homes over the course of two weeks. Through thematic analysis of user interviews, we find that social proxy robots can foster connection towards other people's experiences via mechanisms such as identifying connections across stories or offering diverse perspectives. We present design guidelines from our study insights on the use of social robot systems that serve as social platforms to enhance human empathy and connection.", "morphology": "Other Morphology", "studyMethod": "Field Deployment, Questionnaire", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 73, "year": 2025, "authors": "Lo, Jia-Hsun; Huang, Han-Pang; Lo, Jie-Shih", "title": "LLM-based robot personality simulation and cognitive system", "venue": "Science", "doi": "10.1038/s41598-025-01528-8", "url": "https://www.nature.com/articles/s41598-025-01528-8", "abstract": "The inherence of personality in human-robot interaction enhances conversational dynamics and user experience. The deployment of Chat GPT-4 within a cognitive robot framework is designed by using state-space realization to emulate specific personality traits, incorporating elements of emotion, motivation, visual attention, and both short-term and long-term memory. The encoding and retrieval of long-term memory are facilitated through document embedding techniques, while emotions are generated based on predictions of future events. This framework processes textual and visual information, responding or initiating actions in accordance with the configured personality settings and cognitive processes. The constancy and effectiveness of the personality simulation have been compared to human baseline and validated via two personality assessments: the International Personality Item Pool – Neuroticism, Extraversion and Openness (IPIP-NEO) and the Big Five personality test. Our proposed personality model of cognitive robot is designed by using Kelly’s role construct repertory, Cattell’s 16 personality factors and preferences, which are analyzed by construct validity and compared to human subjects. Theory of mind is observed in personality simulation, which perform better second-order of belief compared to other agent on the improved theory of mind dataset (ToMi dataset). Based on the proposed methods, our designed robot, Mobi, is enable to chat based on its own personality, handle social conflicts and understand user’s intent. Such simulations can achieve a high degree of human likeness, characterized by conversations that are flexible and imbued with intention.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 74, "year": 2025, "authors": "Grassi, Lucrezia; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio", "title": "Strategies for Controlling the Conversation Dynamics in Multi-Party Human-Robot Interaction", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-025-01298-3", "url": "https://doi.org/10.1007/s12369-025-01298-3", "abstract": "This article tackles the research question of whether it is possible to control conversation dynamics in a multi-party scenario using easily implementable solutions on off-the-shelf robotic platforms. To this end, we expanded upon our previously developed cloud robotic architecture by incorporating policies aimed at managing conversation dynamics through selective addressing of individuals, with the ultimate goal of balancing or unbalancing users’ participation or making subgroups of participants interact. Specifically, we computed the dominance of each speaker as a weighted sum of their speaking time and the number of words spoken within a moving window and used the Louvain algorithm to partition speakers into a set of non-overlapping communities. We then implemented six control policies, which were applied by the robot. Two of them, named BH and BS, aim to reduce dominance error (i.e., the difference in dominance between the most and least dominant speakers—both policies give the floor to the less dominant speaker). Two other policies, UH and US, are designed to increase the dominance error (both give the floor to the most dominant speaker). Finally, CH and CS aim to reduce the community error (i.e., the difference between the actual number of detected subgroups among speakers and the ideal target of a single group to which all speakers belong). Policies BH, UH, and CH (with 'H' standing for 'hard') do not allow any exceptions to the policy rules, while BS, US, and CS (with 'S' for 'soft') permit exceptions. To test the impact of these policies, we conducted a between-subjects study (N = 300) involving middle school students engaging in dialogue with a humanoid robot acting as a moderator. The study compared five conditions: in four of them, the robot used information gathered during the conversation to decide which speaker to address, applying one of the control policies—BH, BS, CH, or CS. The policies UH and US were excluded, as having a robot consistently give the floor to the most dominant child may raise ethical concerns. In the fifth condition, a baseline neutral policy (N) was applied, in which the robot did not explicitly address any speaker. The results imply that a robot using the proper control policies can influence conversation dynamics to keep both dominance error and community error significantly lower than those of a robot using the baseline policy, leading to more balanced participation and a reduction in the number of subgroups. Indeed, statistically significant differences have been found between the five policies considered in the dominance and community errors. However, no statistically significant differences in user experience—as measured by three scales of the validated SASSI questionnaire—were found when the robot used one of the control policies, as compared to the baseline, suggesting that participants are not negatively impacted by the robot’s attempt to control the conversation.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Subjective", "application": "Social and conversational systems" },
    { "id": 75, "year": 2025, "authors": "Herath, Damith; Busby Grant, Janie; Rodriguez, Adrian; Davis, Jenny L.", "title": "First impressions of a humanoid social robot with natural language capabilities", "venue": "Science", "doi": "10.1038/s41598-025-04274-z", "url": "https://www.nature.com/articles/s41598-025-04274-z", "abstract": "Concurrent developments in robotic design and natural language processing (NLP) have enabled the production of humanoid chatbots that can operate in commercial and community settings. Though still novel, the presence of physically embodied social robots is growing and will soon be commonplace. Our study is set at this point of emergence, investigating people’s first impressions of a humanoid chatbot in a public venue. Specifically, we introduced 'Pepper' to attendees at an innovation festival. Pepper is a humanoid robot outfitted with ChatGPT. Attendees engaged with Pepper in a bounded interaction and provided feedback about their experience (n = 88). Qualitative analyses reveal participants’ mixed emotional resonance, reactions to Pepper’s embodied form and movements, expectations about interpersonal connection and rituals of interaction, and attentiveness to issues of diversity and social inclusion. Findings document live responses to a humanoid chatbot, highlight the affective, social, and material forces that shape human–robot interaction, and underscore the value of 'in the wild' studies, creating space and scope for user-publics to express their perspectives and concerns.", "morphology": "Humanoid", "studyMethod": "Field Deployment, Interview", "evaluation": "Subjective", "application": "Public space service" },
    { "id": 76, "year": 2024, "authors": "Dell’Anna, Davide; Jamshidnejad, Anahita", "title": "SONAR: An Adaptive Control Architecture for Social Norm Aware Robots", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-024-01172-8", "url": "https://doi.org/10.1007/s12369-024-01172-8", "abstract": "Recent advances in robotics and artificial intelligence have made it necessary or desired for humans to get involved in interactions with social robots. A key factor for the human acceptance of these robots is their awareness of environmental and social norms. In this paper, we introduce SONAR (for SOcial Norm Aware Robots), a novel robot-agnostic control architecture aimed at enabling social agents to autonomously recognize, act upon, and learn over time social norms during interactions with humans. SONAR integrates several state-of-the-art theories and technologies, including the belief-desire-intention (BDI) model of reasoning and decision making for rational agents, fuzzy logic theory, and large language models, to support adaptive and norm-aware autonomous decision making. We demonstrate the feasibility and applicability of SONAR via real-life experiments involving human-robot interactions (HRI) using a Nao robot for scenarios of casual conversations between the robot and each participant. The results of our experiments show that our SONAR implementation can effectively and efficiently be used in HRI to provide the robot with environmental and social and norm awareness. Compared to a robot with no explicit social and norm awareness, introducing social and norm awareness via SONAR results in interactions that are perceived as more positive and enjoyable by humans, as well as in higher perceived trust in the social robot. Moreover, we investigate, via computer-based simulations, the extent to which SONAR can be used to learn and adapt to the social norms of different societies. The results of these simulations illustrate that SONAR can successfully learn adequate behaviors in a society from a relatively small amount of data. We publicly release the source code of SONAR, along with data and experiments logs.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Questionnaire, Technical Evaluation", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 77, "year": 2025, "authors": "Nardelli, Alice; Maccagni, Giacomo; Minutoli, Federico; Sgorbissa, Antonio; Recchiuto, Carmine", "title": "Towards Intuitive Interaction: Cognitive Architecture for Artificial Personality, Emotional Intelligence, and Cognitive Capabilities", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-025-01260-3", "url": "https://doi.org/10.1007/s12369-025-01260-3", "abstract": "Robotic personality shows significant potential in enhancing Human-Robot Interaction. However, shortcomings at both theoretical and implementation levels lead to stereotypical and fragmented models of artificial personality. To overcome these limitations, this study aims to further validate the Conscientiousness, Extroversion, and Agreeableness (CEA) taxonomy for synthetic personalities based on the corresponding three traits of the Big Five model. In the proposed implementation, the robotic personality, directly inspired by human psychology, not only influences how actions are executed but also impacts the robot’s inner hedonic drives and, consequently, the action selection process. Our objective is to introduce a task- and platform-independent framework that drives agent behavior through the interplay between personality dynamics, emotional responses to others, memory encoding, anticipation of future actions, and associated hedonic experiences. To validate the effectiveness of the framework in generating perceivable personality traits and emotionally and cognitively intelligent behaviors, and to explore the impact of embodiment on social interaction, a dual experiment was conducted. This experiment involved participants engaging in dyadic conversations with either a digital human or a robot.", "morphology": "Humanoid", "studyMethod": "Demonstration, Questionnaire", "evaluation": "Subjective", "application": "Social and conversational systems" },
    { "id": 78, "year": 2025, "authors": "Bassiouny, Abdelrhman; Elsayed, Ahmed H.; Falomir, Zoe; del Pobil, Angel P.", "title": "UJI-Butler: A Symbolic/Non-symbolic Robotic System that Learns Through Multi-modal Interaction", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-025-01234-5", "url": "https://doi.org/10.1007/s12369-025-01234-5", "abstract": "This paper introduces UJI-Butler, an innovative multi-robot framework that blends symbolic and non-symbolic artificial intelligence methods. Unlike previous systems, UJI-Butler integrates large language models (LLMs) with a knowledge base akin to RAG-based systems, while imposing logical reasoning on LLM-generated results. It facilitates multi-modal interaction with human users through speech, sign language, and physical interaction, fostering a human-in-the-loop learning paradigm. By acquiring new knowledge through verbal communication and mastering manipulation skills via human-lead-through programming, UJI-Butler enhances transparency and trust by incorporating human feedback during operations. Experimental results demonstrate that UJI-Butler’s combination of symbolic and non-symbolic AI offers intuitive interaction and accelerates the learning process with experience. It adeptly stores and utilizes knowledge gained from verbal communication, recognizing hand gestures for requests. Additionally, UJI-Butler successfully performs user-taught physical skills and generalizes them to varying object sizes and locations. The explicit nature of acquired knowledge enables seamless transferability to other platforms and modification by human users. The code of the whole project is available on Github, in addition, video demonstrations of the UJI-Butler system are available online in a Youtube Playlist.", "morphology": "Functional", "studyMethod": "Demonstration, Laboratory Experiment, Technical Evaluation", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 79, "year": 2025, "authors": "Sakamoto, Yuki; Uchida, Takahisa; Ishiguro, Hiroshi", "title": "Effectiveness of Conversational Robots Capable of Estimating and Modeling User Values", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-025-01258-x", "url": "https://doi.org/10.1007/s12369-025-01258-x", "abstract": "Personalizing a dialogue system according to the user has been recognized to have various positive effects. Despite the significance of user values, concepts guiding choices and evaluations being recognized in communication, they have not been considered in personalized dialogue systems. Therefore, this study constructs a dialogue system that understands user values through conversation. Furthermore, the impact of understanding values on the interactions between dialogue systems and users is examined. The method is organized with a user model of preferences and values based on the established means-end chain model. We used a large language model (LLM) to estimate values based on the users’ preferences and the reasons they prefer them. Furthermore, an infinite relational model (IRM) estimates the relationships between multiple elements within the user model. The experiments show that the proposed method could estimate user values and enhance animacy and perceived intelligence in users’ impressions of an android robot, prompting new insights into users’ own values. The perception of the robot contributes to improved-quality interactions, and new insights into values facilitate a deeper self-understanding of users. This achievement, demonstrating the effects of using values for interaction, can provide valuable insights into human-robot interaction.", "morphology": "Humanoid", "studyMethod": "Questionnaire, Other Method", "evaluation": "Both", "application": "Social and conversational systems" },
    { "id": 80, "year": 2024, "authors": "Rosén, Julia; Lindblom, Jessica; Lamb, Maurice; Billing, Erik", "title": "Previous Experience Matters: An in-Person Investigation of Expectations in Human–Robot Interaction", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-024-01107-3", "url": "https://doi.org/10.1007/s12369-024-01107-3", "abstract": "The human–robot interaction (HRI) field goes beyond the mere technical aspects of developing robots, often investigating how humans perceive robots. Human perceptions and behavior are determined, in part, by expectations. Given the impact of expectations on behavior, it is important to understand what expectations individuals bring into HRI settings and how those expectations may affect their interactions with the robot over time. For many people, social robots are not a common part of their experiences, thus any expectations they have of social robots are likely shaped by other sources. As a result, individual expectations coming into HRI settings may be highly variable. Although there has been some recent interest in expectations within the field, there is an overall lack of empirical investigation into its impacts on HRI, especially in-person robot interactions. To this end, a within-subject in-person study (N=31) was performed where participants were instructed to engage in open conversation with the social robot Pepper during two 2.5 min sessions. The robot was equipped with a custom dialogue system based on the GPT-3 large language model, allowing autonomous responses to verbal input. Participants’ affective changes towards the robot were assessed using three questionnaires, NARS, RAS, commonly used in HRI studies, and Closeness, based on the IOS scale. In addition to the three standard questionnaires, a custom question was administered to capture participants’ views on robot capabilities. All measures were collected three times: before the interaction, after the first interaction, and after the second interaction. Results revealed that participants largely stayed with their initial expectations, and prior experience with robots significantly influenced their perceptions. The findings suggest that expectations are mostly formed prior to interaction and are resilient to change, highlighting the relevance of social psychology principles for HRI research.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Interview, Questionnaire", "evaluation": "Subjective", "application": "Domestic and everyday use" },
    { "id": 81, "year": 2025, "authors": "Kodur, Krishna; Zand, Manizheh; Tognotti, Matthew; Banerjee, Sean; Banerjee, Natasha Kholgade; Kyrarini, Maria", "title": "Exploring the Dynamics of Human-Robot Interaction: Robot Error, Sentiment Analysis, and Politeness", "venue": "International Journal of Social Robotics", "doi": "10.1007/s12369-025-01282-x", "url": "https://doi.org/10.1007/s12369-025-01282-x", "abstract": "As robots become more integrated into daily life, understanding natural human communication is essential for developing user-friendly human-robot interaction (HRI) systems. While prior research focuses on robot-initiated behaviors, little is known about how user-initiated politeness, sentiment, and communication patterns influence perceptions of robots in real-world tasks. This study analyzes interactions between 35 participants and Alex, a mobile manipulator that understands unscripted speech, during a collaborative cooking task. Four hypotheses were tested regarding the effects of robot errors and user behavior on communication and perceptions. Kendall’s Tau correlation revealed that robot errors negatively affected communication length and politeness, and sentiment analysis showed that slower speech led to more neutral sentiment and reduced positive sentiment. These findings highlight the influence of robot errors and user behaviors on HRI, emphasizing the need for adaptive robots that respond to variations in politeness and sentiment, enhancing collaboration and user satisfaction.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Both", "application": "Domestic and everyday use" },
    { "id": 82, "year": 2024, "authors": "Stark, Carson; Chun, Bohkyung; Charleston, Casey; Ravi, Varsha; Pabon, Luis; Sunkari, Surya; Mohan, Tarun; Stone, Peter; Hart, Justin", "title": "Dobby: A Conversational Service Robot Driven by GPT-4", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731375", "url": "https://ieeexplore.ieee.org/document/10731375", "abstract": "This work introduces a robotics platform which comprehensively integrates multi-step action execution, natural language understanding, and memory to interactively perform service tasks in accordance with variable needs and intentions of users. The architecture is built around an AI agent derived from GPT-4 embedded in an embodied system. Semantic matching, plan validation, and state messages ground the agent in the physical world, enabling a seamless merger between communication and behavior. An HRI study compared mobile robots with and without conversational AI in a free-form tour-guide scenario, measuring adaptability along five dimensions: flexible task planning, interactive exploration of information, emotional-friendliness, personalization, and overall user satisfaction. Results demonstrate the advantages of the integrated conversational AI for service tasks.", "morphology": "Functional", "studyMethod": "Laboratory Experiment, Questionnaire", "evaluation": "Both", "application": "Public space service" },
    { "id": 83, "year": 2025, "authors": "Long, Yonghao; Lin, Anran; Kwok, Derek Hang Chun; Zhang, Lin; Yang, Zhenya; Shi, Kejian; Song, Lei; Fu, Jiawei; Lin, Hongbin; Wei, Wang; Chen, Kai; Chu, Xiangyu; Hu, Yang; Yip, Hon Chi; Chiu, Philip Wai Yan; Kazanzides, Peter; Taylor, Russell H.; Liu, Yunhui; Chen, Zihan; Wang, Zerui; Samuel Kwok Wai Au; Dou, Qi", "title": "Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery", "venue": "Science", "doi": "10.1126/scirobotics.adt3093", "url": "https://www.science.org/doi/10.1126/scirobotics.adt3093", "abstract": "Surgical robots capable of autonomously performing various tasks could enhance efficiency and augment human productivity in clinical contexts. Current solutions automate specific actions but struggle to generalize across diverse surgical environments. This study introduces an open-source surgical embodied intelligence simulator for reinforcement learning methods for minimally invasive robots, enabling zero-shot transfer of simulation-trained policies to real-world scenarios. The method integrates visual parsing, a perceptual regressor, policy learning, and a visual servoing controller. Experiments demonstrated autonomy in seven skill training tasks on the da Vinci Research Kit, and automation of five surgical assistive tasks on ex vivo animal tissues. Policies were also validated in live-animal trials for three tasks in dynamic in vivo conditions. The open-source platform and general-purpose learning paradigm aim to inspire future research on embodied intelligence for autonomous surgical robots.", "morphology": "Functional", "studyMethod": "Technical Evaluation", "evaluation": "Both", "application": "Healthcare and Wellbeing" },
    { "id": 84, "year": 2024, "authors": "Grassi, Lucrezia; Recchiuto, Carmine Tommaso; Sgorbissa, Antonio", "title": "Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness", "venue": "ROMAN", "doi": "10.1109/RO-MAN60168.2024.10731381", "url": "https://ieeexplore.ieee.org/document/10731381", "abstract": "This paper presents a system for diversity-aware autonomous conversation leveraging large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. Conversation flow is guided by the structure of the system’s knowledge base, while LLMs generate diversity-aware responses using carefully crafted prompts that incorporate user information, conversation history, contextual details, and specific guidelines. Performance was assessed through controlled and real-world experiments, measuring a wide range of performance indicators.", "morphology": "Humanoid", "studyMethod": "Laboratory Experiment, Field Deployment, Technical Evaluation", "evaluation": "Subjective", "application": "Public space service" },
    { "id": 85, "year": 2023, "authors": "Wilcock, Graham; Jokinen, Kristiina", "title": "To Err Is Robotic; to Earn Trust, Divine: Comparing ChatGPT and Knowledge Graphs for HRI", "venue": "ROMAN", "doi": "10.1109/RO-MAN57019.2023.10309510", "url": "https://ieeexplore.ieee.org/document/10309510", "abstract": "The paper discusses two current approaches to conversational AI, using large language models and knowledge graphs, and compares types of errors that occur in humanrobot interactions based on these approaches. It provides example dialogues and describes solutions to several error types including false implications, ontological errors, theory of mind errors, and handling of speech recognition errors. The paper addresses issues of particular concern for earning user trust.", "morphology": "Humanoid", "studyMethod": "Demonstration", "evaluation": "Subjective", "application": "Public space service" },
    { "id": 86, "year": 2024, "authors": "Jin, Yixiang; Li, Dingzhe; A, Yong; Shi, Jun; Hao, Peng; Sun, Fuchun; Zhang, Jianwei; Fang, Bin", "title": "RobotGPT: Robot Manipulation Learning From ChatGPT", "venue": "IEEE Robotics and Automation Letters", "doi": "10.1109/LRA.2024.3357432", "url": "https://ieeexplore.ieee.org/document/10412086/", "abstract": "We present RobotGPT, an innovative decision framework for robotic manipulation that prioritizes stability and safety. The execution code generated by ChatGPT cannot guarantee the stability and safety of the system. ChatGPT may provide different answers for the same task, leading to unpredictability. This instability prevents the direct integration of ChatGPT into the robot manipulation loop. Although setting the temperature to 0 can generate more consistent outputs, it may cause ChatGPT to lose diversity and creativity. Our objective is to leverage ChatGPT’s problem-solving capabilities in robot manipulation and train a reliable agent. The framework includes an effective prompt structure and a robust learning model. Additionally, we introduce a metric for measuring task difficulty to evaluate ChatGPT’s performance in robot manipulation. Furthermore, we evaluate RobotGPT in both simulation and real-world environments. Compared to directly using ChatGPT to generate code, our framework significantly improves task success rates, with an average increase from 38.5% to 91.5%. Therefore, training a RobotGPT by utilizing ChatGPT as an expert is a more stable approach compared to directly using ChatGPT as a task planner.", "morphology": "Functional", "studyMethod": "Laboratory Experiment", "evaluation": "Objective", "application": "Social and conversational systems" }
];